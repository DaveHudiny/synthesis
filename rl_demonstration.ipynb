{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d65c24c",
   "metadata": {},
   "source": [
    "## Demonstration of RL implementation for PRISM models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cac9c",
   "metadata": {},
   "source": [
    "This demonstration should provide general idea of how to use this implementation for own experiments with RL training.\n",
    "\n",
    "Remark: This demonstration does not include the important part of the project of using verification with PAYNT/Storm and focuses solely on training RL agents on PRISM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737ee53",
   "metadata": {},
   "source": [
    "### Imports and Initial Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c4fe51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_agents # The RL library\n",
    "import tensorflow as tf\n",
    "\n",
    "from rl_src.agents.recurrent_ppo_agent import Recurrent_PPO_agent # The recurrent PPO agent\n",
    "from rl_src.tools.args_emulator import ArgsEmulator # The argument emulator (historically I supported command line arguments for all of them)\n",
    "from rl_src.environment.environment_wrapper_vec import EnvironmentWrapperVec # The vectorized environment wrapper\n",
    "\n",
    "from rl_src.tests.general_test_tools import *\n",
    "from rl_src.tools.evaluators import evaluate_policy_in_model\n",
    "from rl_src.interpreters.extracted_fsc.table_based_policy import TableBasedPolicy \n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d4975",
   "metadata": {},
   "source": [
    "First, we have to create parameters for training using ArgsEmulator. Note, that the used parameters are only the subset of all possible Arguments, but there are many more, which are perhaps more experimental. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb23bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "prism_path = \"./rl_src/models/intercept\" # Folder with the PRISM model, you can change this to your own path\n",
    "prism_template = os.path.join(prism_path, \"sketch.templ\") # File with the prism template\n",
    "prism_spec = os.path.join(prism_path, \"sketch.props\") # File with the task PTL specification\n",
    "\n",
    "args = ArgsEmulator(\n",
    "    prism_model=prism_template,\n",
    "    prism_properties=prism_spec,\n",
    "    constants=\"\", # If PRISM model has some unresolved constants (such as Size=N, you can set them as \"N=10\")\n",
    "    discount_factor=0.99,\n",
    "    learning_rate=1.6e-4,\n",
    "    trajectory_num_steps=32, # Number of steps in a single driver.run() call and the length of the trajectory (sub-episodes) for training the agent\n",
    "    num_environments=16, # Number of environments to run in parallel/vectorized (batch size currently corresponds to this), I usually use 256\n",
    "    batch_size=16,\n",
    "    max_steps=400 # Number of steps, that can be taken in the environment per episode (if more, the episode is truncated)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90002c",
   "metadata": {},
   "source": [
    "Now we can initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67245363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:environment.pomdp_builder:Construct POMDP representation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------- \n",
      "Model type: \tPOMDP (sparse)\n",
      "States: \t4705\n",
      "Transitions: \t18386\n",
      "Choices: \t11810\n",
      "Observations: \t2598\n",
      "Reward Models:  steps\n",
      "State Labels: \t5 labels\n",
      "   * deadlock -> 0 item(s)\n",
      "   * init -> 1 item(s)\n",
      "   * notbad -> 4607 item(s)\n",
      "   * goal -> 96 item(s)\n",
      "   * exits -> 98 item(s)\n",
      "Choice Labels: \t7 labels\n",
      "   * placement -> 1 item(s)\n",
      "   * adv -> 2256 item(s)\n",
      "   * east -> 2352 item(s)\n",
      "   * north -> 2401 item(s)\n",
      "   * south -> 2352 item(s)\n",
      "   * west -> 2352 item(s)\n",
      "   * finished -> 96 item(s)\n",
      "-------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stormpy_model = initialize_prism_model(args.prism_model, args.prism_properties, args.constants) # Initialize the PRISM model using stormpy\n",
    "# stormpy_model = any POMDP storm model, if you have some constructed model -- it is useful in PAYNT, since PAYNT constructs its own POMDPs and I just take them.\n",
    "print(stormpy_model) # Prints the description of the PRISM model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225584e4",
   "metadata": {},
   "source": [
    "Now we can show some basic work with the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9c4a8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:environment.vectorized_sim_initializer:Model intercept found in compiled_models_vec_storm. The model will be loaded\n",
      "ERROR:environment.environment_wrapper_vec:Grid-like renderer not possible to initialize.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulator initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:environment.environment_wrapper_vec:Resetting the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'environment.environment_wrapper_vec.EnvironmentWrapperVec'>\n",
      "Environment initialized\n",
      "Initial TimeStep: TimeStep(\n",
      "{'step_type': <tf.Tensor: shape=(16,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>,\n",
      " 'reward': <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>,\n",
      " 'discount': <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99,\n",
      "       0.99, 0.99, 0.99, 0.99, 0.99], dtype=float32)>,\n",
      " 'observation': {'observation': <tf.Tensor: shape=(16, 8), dtype=float32, numpy=\n",
      "array([[ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  0.,  0.]], dtype=float32)>,\n",
      "                 'mask': <tf.Tensor: shape=(16, 7), dtype=bool, numpy=\n",
      "array([[False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False],\n",
      "       [False, False, False, False,  True, False, False]])>,\n",
      "                 'integer': <tf.Tensor: shape=(16, 1), dtype=int32, numpy=\n",
      "array([[1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425],\n",
      "       [1425]], dtype=int32)>}})\n",
      "TimeStep after some arbitrary step TimeStep(\n",
      "{'step_type': <tf.Tensor: shape=(16,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>,\n",
      " 'reward': <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>,\n",
      " 'discount': <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99,\n",
      "       0.99, 0.99, 0.99, 0.99, 0.99], dtype=float32)>,\n",
      " 'observation': {'observation': <tf.Tensor: shape=(16, 8), dtype=float32, numpy=\n",
      "array([[ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  0.,  3.,  0., -1., -1.,  1.,  0.]], dtype=float32)>,\n",
      "                 'mask': <tf.Tensor: shape=(16, 7), dtype=bool, numpy=\n",
      "array([[ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False],\n",
      "       [ True, False, False, False, False, False, False]])>,\n",
      "                 'integer': <tf.Tensor: shape=(16, 1), dtype=int32, numpy=\n",
      "array([[38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38],\n",
      "       [38]], dtype=int32)>}})\n"
     ]
    }
   ],
   "source": [
    "env = EnvironmentWrapperVec(stormpy_model, args, num_envs=args.num_environments)\n",
    "tf_env = TFPyEnvironment(env)\n",
    "print(\"Environment initialized\")\n",
    "\n",
    "# Example of how to use the environment\n",
    "print(f\"Initial TimeStep: {tf_env.reset()}\")\n",
    "print(f\"TimeStep after some arbitrary step {tf_env.step(tf.zeros((args.num_environments,)))}\") # The number of actions is equal to the number of environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46177347",
   "metadata": {},
   "source": [
    "Important thing in this framework is the structure of TimeSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06186945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'observation': {'observation': TensorSpec(shape=(8,), dtype=tf.float32, name='observation'),\n",
      "                 'mask': TensorSpec(shape=(7,), dtype=tf.bool, name='mask'),\n",
      "                 'integer': TensorSpec(shape=(1,), dtype=tf.int32, name='integer_information')}})\n"
     ]
    }
   ],
   "source": [
    "print(tf_env.time_step_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4856f0",
   "metadata": {},
   "source": [
    "As you can see above, the TimeStep consists of four parts:\n",
    " * step_type: information about the state of the simulation (i.e. simulation just started as 0, simulation is in the middle of run as 1 and simulation ended in 2). However, it is important to note, that since I work with vectorized environment, if you play some arbitrary action when the simulation ended, the simulation restarts to initial state.\n",
    " * reward: information about the reward that the agent receives. Note, that this is the value that the agent is trained on, but in the training output, I usually provide also real reward (the reward of the model without some training-related reward stuff).\n",
    " * discount: discount :)\n",
    " * observation: dictionary of three important vectors:\n",
    "   * observation: valuations of features of provided observation\n",
    "   * integer: index of observation in StormPy POMDP model\n",
    "   * mask: vector of booleans describing which actions are currently legal to play (if the action is illegal, simulator plays selects random legal action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07126bad",
   "metadata": {},
   "source": [
    "In the next cell, we initialize the Recurrent PPO Agent. Note, that I provide both env and tv_env (original wrapper and TensorFlow version of the environment). I do it, because my environment provides some other relevant information about the simulation, that is used during the evaluation. However, the training is runned on the tf_env.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60ef8745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Agent initialized\n",
      "INFO:root:Replay buffer initialized\n",
      "INFO:root:Collector driver initialized\n",
      "INFO:root:Evaluation driver initialized\n",
      "INFO:environment.environment_wrapper_vec:Resetting the environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(16,), dtype=int32, numpy=array([3, 6, 1, 0, 1, 5, 6, 3, 1, 1, 1, 1, 3, 2, 3, 2], dtype=int32)>, state={'actor_network_state': [<tf.Tensor: shape=(16, 32), dtype=float32, numpy=\n",
       "array([[ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05],\n",
       "       [ 6.57125175e-01,  2.45846752e-02, -5.04158258e-01,\n",
       "         5.65134168e-01,  2.27224872e-01, -4.64851975e-01,\n",
       "         6.54574394e-01, -3.13785486e-02, -6.00984460e-03,\n",
       "         6.76215142e-02,  2.19584897e-01, -2.13127583e-01,\n",
       "         1.34125501e-01,  1.27421722e-01,  6.32455170e-01,\n",
       "        -5.68710901e-02, -3.32760900e-01,  1.14051085e-02,\n",
       "         1.01076784e-02,  4.59009269e-03,  7.01762974e-01,\n",
       "        -2.17506438e-01, -1.27916457e-02,  6.08660817e-01,\n",
       "         6.48001730e-02, -1.83698714e-01, -4.05178756e-01,\n",
       "        -4.63577598e-01,  1.43414155e-01, -2.41662916e-02,\n",
       "         9.13439319e-03, -8.58846834e-05]], dtype=float32)>, <tf.Tensor: shape=(16, 32), dtype=float32, numpy=\n",
       "array([[ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885],\n",
       "       [ 0.8025765 ,  0.74926436, -0.67766094,  0.7684256 ,  0.99257725,\n",
       "        -0.7672524 ,  0.999169  , -0.06145991, -0.02796159,  0.5399395 ,\n",
       "         0.38339385, -0.3790805 ,  0.2836381 ,  0.69187737,  0.7648337 ,\n",
       "        -0.34452543, -0.34747735,  0.14058131,  0.35452378,  0.15532586,\n",
       "         0.9269328 , -0.80608416, -0.01839953,  0.9823503 ,  0.9461775 ,\n",
       "        -0.41565675, -0.5580852 , -0.69119227,  0.18637717, -0.74850273,\n",
       "         0.01678865, -0.06225885]], dtype=float32)>]}, info=())"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Recurrent_PPO_agent(\n",
    "            env, tf_env, args)\n",
    "agent.action(tf_env.reset()) # Example of how to use the policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59549ae7",
   "metadata": {},
   "source": [
    "## Agent Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a58a2db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:agents.father_agent:Training agent with replay buffer option: 1\n",
      "INFO:agents.father_agent:Before training evaluation.\n",
      "INFO:environment.environment_wrapper_vec:Resetting the environment.\n",
      "INFO:agents.father_agent:Average Return = -77.77941131591797\n",
      "INFO:agents.father_agent:Average Virtual Goal Value = 36.02941131591797\n",
      "INFO:agents.father_agent:Goal Reach Probability = 0.7205882352941176\n",
      "INFO:agents.father_agent:Trap Reach Probability = 0.27941176470588236\n",
      "INFO:agents.father_agent:Variance of Return = 2574.877685546875\n",
      "INFO:agents.father_agent:Current Best Return = -77.77941131591797\n",
      "INFO:agents.father_agent:Current Best Reach Probability = 0.7205882352941176\n",
      "INFO:environment.environment_wrapper_vec:Resetting the environment.\n",
      "INFO:agents.father_agent:Training agent on-policy\n",
      "INFO:agents.father_agent:Step: 0, Training loss: 2.475553512573242\n",
      "INFO:environment.environment_wrapper_vec:Resetting the environment.\n",
      "INFO:agents.father_agent:Average Return = -80.984375\n",
      "INFO:agents.father_agent:Average Virtual Goal Value = 32.8125\n",
      "INFO:agents.father_agent:Goal Reach Probability = 0.65625\n",
      "INFO:agents.father_agent:Trap Reach Probability = 0.34375\n",
      "INFO:agents.father_agent:Variance of Return = 2712.26513671875\n",
      "INFO:agents.father_agent:Current Best Return = -77.77941131591797\n",
      "INFO:agents.father_agent:Current Best Reach Probability = 0.7205882352941176\n",
      "INFO:environment.environment_wrapper_vec:Resetting the environment.\n",
      "INFO:agents.father_agent:Step: 5, Training loss: 3.47723650932312\n",
      "INFO:agents.father_agent:Step: 10, Training loss: 2.705836534500122\n",
      "INFO:agents.father_agent:Step: 15, Training loss: 3.35284161567688\n",
      "INFO:agents.father_agent:Step: 20, Training loss: 3.079991340637207\n",
      "INFO:agents.father_agent:Step: 25, Training loss: 4.742922782897949\n",
      "INFO:agents.father_agent:Step: 30, Training loss: 2.9121296405792236\n",
      "INFO:agents.father_agent:Step: 35, Training loss: 5.471159934997559\n",
      "INFO:agents.father_agent:Step: 40, Training loss: 3.1855030059814453\n",
      "INFO:agents.father_agent:Step: 45, Training loss: 1.9160245656967163\n",
      "INFO:agents.father_agent:Step: 50, Training loss: 1.0174129009246826\n",
      "INFO:agents.father_agent:Step: 55, Training loss: 1.0372626781463623\n",
      "INFO:agents.father_agent:Step: 60, Training loss: 2.0845539569854736\n",
      "INFO:agents.father_agent:Step: 65, Training loss: 1.3713710308074951\n",
      "INFO:agents.father_agent:Step: 70, Training loss: 1.2979695796966553\n",
      "INFO:agents.father_agent:Step: 75, Training loss: 2.4206671714782715\n",
      "INFO:agents.father_agent:Step: 80, Training loss: 1.8756294250488281\n",
      "INFO:agents.father_agent:Step: 85, Training loss: 1.39335036277771\n",
      "INFO:agents.father_agent:Step: 90, Training loss: 2.0658934116363525\n",
      "INFO:agents.father_agent:Step: 95, Training loss: 1.5436429977416992\n",
      "INFO:agents.father_agent:Training finished.\n",
      "INFO:environment.environment_wrapper_vec:Resetting the environment.\n",
      "INFO:agents.father_agent:Evaluating agent with greedy masked policy.\n",
      "INFO:agents.father_agent:Average Return = -78.19117736816406\n",
      "INFO:agents.father_agent:Average Virtual Goal Value = 24.264705657958984\n",
      "INFO:agents.father_agent:Goal Reach Probability = 0.4852941176470588\n",
      "INFO:agents.father_agent:Trap Reach Probability = 0.5147058823529411\n",
      "INFO:agents.father_agent:Variance of Return = 3061.095703125\n",
      "INFO:agents.father_agent:Current Best Return = -77.77941131591797\n",
      "INFO:agents.father_agent:Current Best Reach Probability = 0.7205882352941176\n"
     ]
    }
   ],
   "source": [
    "agent.train_agent(iterations = 100) # Train the agent for nr runs\n",
    "# I recommend to use at least 1000 iterations in general, but it depends on the task and the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db43dd",
   "metadata": {},
   "source": [
    "## Basic inference loop, where you can collect whatever you want. \n",
    "\n",
    "In TF Agents, there are some other options how to run policy (primarily drivers) and how to store data (replay buffers), but for simplicity, I show only the simplest running loop possible.\n",
    "\n",
    "Note, that I use agent.policy(), since there are multiple options how to work with the agent, because beside the original policy I also use wrapper masked policy, that can provide functionality with masking, reward machines etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93a3675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:environment.environment_wrapper_vec:Resetting the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy step 0: [4 2 1 1 5 6 6 1 4 6 1 3 0 1 3 1]\n",
      "Policy step 1: [2 5 2 6 2 1 6 4 4 6 2 2 2 6 4 2]\n",
      "Policy step 2: [3 2 3 6 5 5 4 5 1 1 5 1 2 0 5 0]\n",
      "Policy step 3: [1 2 4 0 1 2 2 3 1 1 1 2 5 1 1 2]\n",
      "Policy step 4: [1 5 1 5 0 1 5 0 4 5 4 1 5 5 0 0]\n",
      "Policy step 5: [3 5 1 3 0 0 4 6 4 6 5 4 3 1 0 3]\n",
      "Policy step 6: [1 1 3 2 1 5 1 2 1 2 1 1 5 4 0 3]\n",
      "Policy step 7: [3 2 0 5 3 4 2 1 6 4 3 0 0 6 4 1]\n",
      "Policy step 8: [0 0 5 2 5 0 5 0 1 5 0 0 3 1 1 1]\n",
      "Policy step 9: [6 4 5 1 4 4 1 4 5 3 6 2 1 0 4 4]\n"
     ]
    }
   ],
   "source": [
    "time_step = tf_env.reset() # Reset the environment before evaluation\n",
    "policy_state = agent.policy().get_initial_state(tf_env.batch_size) # Get the initial state of the policy\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    policy_step = agent.policy().action(time_step, policy_state) # Get the action from the policy\n",
    "    print(f\"Policy step {i}: {policy_step.action}\") # Print the action\n",
    "    policy_state = policy_step.state # Update the policy state\n",
    "    time_step = tf_env.step(policy_step.action) # Step the environment with the action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e14b4",
   "metadata": {},
   "source": [
    "## FSC Evaluation\n",
    "If you extracted FSC, you can evaluate it in the simulator.\n",
    "\n",
    "The action and update function have shapes [memory_size, nr_observations]. Action function provides played actions of Mealy machine for some memory and seen observation (integer index), update function tells us to which memory update to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ab00427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:environment.environment_wrapper_vec:Resetting the environment.\n",
      "INFO:tools.evaluation_results_class:Average Return = -80.776123046875\n",
      "INFO:tools.evaluation_results_class:Average Virtual Goal Value = 31.34328269958496\n",
      "INFO:tools.evaluation_results_class:Goal Reach Probability = 0.6268656716417911\n",
      "INFO:tools.evaluation_results_class:Trap Reach Probability = 0.373134328358209\n",
      "INFO:tools.evaluation_results_class:Variance of Return = 2518.6220703125\n",
      "INFO:tools.evaluation_results_class:Current Best Return = -80.776123046875\n",
      "INFO:tools.evaluation_results_class:Current Best Reach Probability = 0.6268656716417911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tools.evaluation_results_class.EvaluationResults at 0x708693f39840>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_size = 3 # That means that the result is 3-FSC\n",
    "action_function = tf.zeros((memory_size, stormpy_model.nr_observations), dtype=tf.int32) # Example of how to use the dummy action function\n",
    "update_function = tf.zeros((memory_size, stormpy_model.nr_observations), dtype=tf.int32) # Example of how to use the dummy update function\n",
    "action_labels = env.action_keywords # The action labels, that are used in the environment, i.e. if the first action is \"up\", the action 0 is \"up\"\n",
    "  # Currently, this is not used in the proposed evaluate_policy_in_model function, but it is useful for PAYNT\n",
    "  # TODO: Remapping action and update functions to the order of action labels\n",
    "\n",
    "fsc_policy = TableBasedPolicy(\n",
    "    original_policy=agent.policy(), # The policy of the agent, I use it because it have complete information about the environment specification\n",
    "    external_observation_to_action_table=action_function,\n",
    "    external_observation_to_update_table=update_function,\n",
    "    action_keywords=action_labels,\n",
    "    initial_memory = 0, # The initial memory of the policy, I use 0 because that is how it works in PAYNT, but you can set it to any value\n",
    ")\n",
    "\n",
    "evaluate_policy_in_model(\n",
    "    fsc_policy,\n",
    "    args,\n",
    "    env,\n",
    "    tf_env,\n",
    ")\n",
    "# NOTE: The evaluate_policy_in_model currently evaluates the policy only in 16 environments, that may not be enough for statistical significance.\n",
    "#       If you want more environments, change the num_envs parameter in the ArgsEmulator class.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c01fd5",
   "metadata": {},
   "source": [
    "### Useful tools\n",
    "\n",
    "One of the useful tools for FSC extraction is artificial synthesis of environment TimeSteps, since you can generate observations features valuation for all possible observations and then evaluate it on some policy. The observation integers are the same numbers as in StormPy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be76b59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'step_type': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 1, 1, 1], dtype=int32)>,\n",
      " 'reward': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>,\n",
      " 'discount': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>,\n",
      " 'observation': {'observation': <tf.Tensor: shape=(4, 8), dtype=float32, numpy=\n",
      "array([[ 0.,  0.,  6.,  0., -1., -1.,  1.,  0.],\n",
      "       [ 0.,  2.,  4.,  0.,  4.,  4.,  1.,  1.],\n",
      "       [ 0.,  2.,  4.,  0., -1., -1.,  1.,  1.],\n",
      "       [ 0.,  5.,  4.,  0., -1., -1.,  1.,  0.]], dtype=float32)>,\n",
      "                 'mask': <tf.Tensor: shape=(4, 7), dtype=bool, numpy=\n",
      "array([[ True, False, False, False, False, False, False],\n",
      "       [False,  True, False,  True, False,  True,  True],\n",
      "       [False,  True, False,  True, False,  True,  True],\n",
      "       [ True, False, False, False, False, False, False]])>,\n",
      "                 'integer': <tf.Tensor: shape=(4, 1), dtype=int32, numpy=\n",
      "array([[4],\n",
      "       [3],\n",
      "       [1],\n",
      "       [2]], dtype=int32)>}})\n",
      "\n",
      "PolicyStep(action=<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>, state=<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\n",
      "array([[0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0]], dtype=int32)>, info=())\n"
     ]
    }
   ],
   "source": [
    "fake_time_step = env.create_fake_timestep_from_observation_integer([4, 3, 1, 2])\n",
    "print(fake_time_step) # Example of how to use the fake time step\n",
    "initial_memory = fsc_policy.get_initial_state(4)\n",
    "print()\n",
    "print(fsc_policy.action(fake_time_step, initial_memory))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
