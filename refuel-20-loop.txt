2025-03-24 15:10:27.645178: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-24 15:10:27.673377: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-24 15:10:27.673447: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-24 15:10:27.674795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-24 15:10:27.680325: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-24 15:10:28.405312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
0it [00:00, ?it/s]Exception in thread Thread-6 (interactive_control):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 326, in interactive_control
    self.store_storm_result(result)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 152, in store_storm_result
    self.latest_storm_fsc = self.belief_controller_to_fsc(result, self.latest_paynt_result_fsc)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 751, in belief_controller_to_fsc
    paynt_fsc.make_stochastic()
  File "/home/davidh/Plocha/synthesis/paynt/quotient/fsc.py", line 90, in make_stochastic
    self.action_function[node][obs] = {self.action_function[node][obs] : 1.0}
  File "/home/davidh/Plocha/synthesis/prerequisites/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 352, in __hash__
    raise TypeError("Tensor is unhashable. "
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
0it [00:00, ?it/s]0it [28:16, ?it/s]
Exception in thread Thread-7 (interactive_control):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 326, in interactive_control
    self.store_storm_result(result)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 152, in store_storm_result
    self.latest_storm_fsc = self.belief_controller_to_fsc(result, self.latest_paynt_result_fsc)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 751, in belief_controller_to_fsc
    paynt_fsc.make_stochastic()
  File "/home/davidh/Plocha/synthesis/paynt/quotient/fsc.py", line 90, in make_stochastic
    self.action_function[node][obs] = {self.action_function[node][obs] : 1.0}
  File "/home/davidh/Plocha/synthesis/prerequisites/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 352, in __hash__
    raise TypeError("Tensor is unhashable. "
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
0it [00:00, ?it/s]0it [27:01, ?it/s]
Exception in thread Thread-8 (interactive_control):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 326, in interactive_control
    self.store_storm_result(result)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 152, in store_storm_result
    self.latest_storm_fsc = self.belief_controller_to_fsc(result, self.latest_paynt_result_fsc)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 751, in belief_controller_to_fsc
    paynt_fsc.make_stochastic()
  File "/home/davidh/Plocha/synthesis/paynt/quotient/fsc.py", line 90, in make_stochastic
    self.action_function[node][obs] = {self.action_function[node][obs] : 1.0}
  File "/home/davidh/Plocha/synthesis/prerequisites/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 352, in __hash__
    raise TypeError("Tensor is unhashable. "
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
0it [00:00, ?it/s]0it [27:58, ?it/s]
Exception in thread Thread-9 (interactive_control):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 326, in interactive_control
    self.store_storm_result(result)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 152, in store_storm_result
    self.latest_storm_fsc = self.belief_controller_to_fsc(result, self.latest_paynt_result_fsc)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 751, in belief_controller_to_fsc
    paynt_fsc.make_stochastic()
  File "/home/davidh/Plocha/synthesis/paynt/quotient/fsc.py", line 90, in make_stochastic
    self.action_function[node][obs] = {self.action_function[node][obs] : 1.0}
  File "/home/davidh/Plocha/synthesis/prerequisites/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 352, in __hash__
    raise TypeError("Tensor is unhashable. "
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
0it [00:00, ?it/s]0it [27:01, ?it/s]
Exception in thread Thread-10 (interactive_control):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 326, in interactive_control
    self.store_storm_result(result)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 152, in store_storm_result
    self.latest_storm_fsc = self.belief_controller_to_fsc(result, self.latest_paynt_result_fsc)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 751, in belief_controller_to_fsc
    paynt_fsc.make_stochastic()
  File "/home/davidh/Plocha/synthesis/paynt/quotient/fsc.py", line 90, in make_stochastic
    self.action_function[node][obs] = {self.action_function[node][obs] : 1.0}
  File "/home/davidh/Plocha/synthesis/prerequisites/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 352, in __hash__
    raise TypeError("Tensor is unhashable. "
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
0it [00:00, ?it/s]0it [30:13, ?it/s]
Exception in thread Thread-11 (interactive_control):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 326, in interactive_control
    self.store_storm_result(result)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 152, in store_storm_result
    self.latest_storm_fsc = self.belief_controller_to_fsc(result, self.latest_paynt_result_fsc)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 751, in belief_controller_to_fsc
    paynt_fsc.make_stochastic()
  File "/home/davidh/Plocha/synthesis/paynt/quotient/fsc.py", line 90, in make_stochastic
    self.action_function[node][obs] = {self.action_function[node][obs] : 1.0}
  File "/home/davidh/Plocha/synthesis/prerequisites/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 352, in __hash__
    raise TypeError("Tensor is unhashable. "
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
0it [00:00, ?it/s]0it [30:00, ?it/s]
Exception in thread Thread-12 (interactive_control):
Traceback (most recent call last):
  File "/usr/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 326, in interactive_control
    self.store_storm_result(result)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 152, in store_storm_result
    self.latest_storm_fsc = self.belief_controller_to_fsc(result, self.latest_paynt_result_fsc)
  File "/home/davidh/Plocha/synthesis/paynt/quotient/storm_pomdp_control.py", line 751, in belief_controller_to_fsc
    paynt_fsc.make_stochastic()
  File "/home/davidh/Plocha/synthesis/paynt/quotient/fsc.py", line 90, in make_stochastic
    self.action_function[node][obs] = {self.action_function[node][obs] : 1.0}
  File "/home/davidh/Plocha/synthesis/prerequisites/venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 352, in __hash__
    raise TypeError("Tensor is unhashable. "
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
0it [00:00, ?it/s]0it [27:27, ?it/s]
rth, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=north, A([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=west, A([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=north, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=north, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=north, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=north, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=west, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=north, A([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=west, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=west, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=north, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=west, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=west, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=15	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east
2025-03-24 15:11:53,037 - synthesizer.py:198 - double-checking specification satisfiability:  : 0.0005976443916715923
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: AR, synthesis time: 30.06 s
number of holes: 117, family size: 1e56, quotient: 6834 states / 24802 actions
explored: 0 %

optimum: 0.000598
--------------------
2025-03-24 15:11:53,076 - synthesizer_pomdp.py:90 - Added memory nodes to match Storm data
2025-03-24 15:11:53,077 - pomdp.py:349 - unfolding 2-FSC template into POMDP...
2025-03-24 15:11:53,086 - pomdp.py:355 - constructed quotient MDP having 7364 states and 28756 actions.
2025-03-24 15:11:53,100 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e61 to 262144
2025-03-24 15:11:53,100 - statistic.py:67 - synthesis initiated, design space: 1e61
2025-03-24 15:11:53,100 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
-----------PAYNT-----------                     
Value = 0.004022774147200814 | Time elapsed = 82.2s | FSC size = 354

-----------PAYNT-----------                     
Value = 0.09338872437224116 | Time elapsed = 82.2s | FSC size = 354

-----------PAYNT-----------                     
Value = 0.16358523338844272 | Time elapsed = 82.2s | FSC size = 354

-----------PAYNT-----------                     
Value = 0.20023392954212083 | Time elapsed = 82.3s | FSC size = 354

2025-03-24 15:11:54,764 - synthesizer_ar_storm.py:170 - Main family synthesis done
2025-03-24 15:11:54,764 - synthesizer_ar_storm.py:171 - Subfamilies buffer contains: 23 families
-----------PAYNT-----------                     
Value = 0.21992850822800797 | Time elapsed = 84.2s | FSC size = 354

-----------PAYNT-----------                     
Value = 0.23426663667050218 | Time elapsed = 84.2s | FSC size = 354

2025-03-24 15:12:22,315 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 15:12:22,391 - agents_wrapper.py:252 - Creating pretrainer
2025-03-24 15:12:22,452 - recurrent_ppo_agent.py:86 - Agent initialized
2025-03-24 15:12:22,459 - recurrent_ppo_agent.py:88 - Replay buffer initialized
2025-03-24 15:12:22,463 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:12:31,878 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:12:31,879 - father_agent.py:564 - Average Virtual Goal Value = 0.0
2025-03-24 15:12:31,879 - father_agent.py:566 - Goal Reach Probability = 0.0
2025-03-24 15:12:31,879 - father_agent.py:568 - Trap Reach Probability = 1.0
2025-03-24 15:12:31,879 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:12:31,879 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:12:31,879 - father_agent.py:574 - Current Best Reach Probability = 0.0
2025-03-24 15:12:34,953 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:12:40,176 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:12:40,176 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 15:12:40,176 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 15:12:40,176 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 15:12:40,176 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:12:40,176 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:12:40,176 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 15:12:40,176 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 1.903428077697754
Epoch: 0, Critic Loss: 4.887155532836914
Epoch: 10, Actor Loss: 1.3650702238082886
Epoch: 10, Critic Loss: 6.4995012283325195
Epoch: 20, Actor Loss: 1.0109411478042603
Epoch: 20, Critic Loss: 11.357904434204102
Epoch: 30, Actor Loss: 0.7489292621612549
Epoch: 30, Critic Loss: 8.034049034118652
Epoch: 40, Actor Loss: 0.6687890887260437
Epoch: 40, Critic Loss: 12.266130447387695
2025-03-24 15:13:11,964 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:13:15,487 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:13:15,488 - evaluators.py:130 - Average Virtual Goal Value = 0.9617696404457092
2025-03-24 15:13:15,488 - evaluators.py:132 - Goal Reach Probability = 0.019235393123346958
2025-03-24 15:13:15,488 - evaluators.py:134 - Trap Reach Probability = 0.980764606876653
2025-03-24 15:13:15,488 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:13:15,488 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:13:15,488 - evaluators.py:140 - Current Best Reach Probability = 0.019235393123346958
2025-03-24 15:13:15,488 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.6187487244606018
Epoch: 50, Critic Loss: 11.331500053405762
Epoch: 60, Actor Loss: 0.5627760887145996
Epoch: 60, Critic Loss: 6.725028038024902
Epoch: 70, Actor Loss: 0.5699354410171509
Epoch: 70, Critic Loss: 9.627533912658691
Epoch: 80, Actor Loss: 0.5238721370697021
Epoch: 80, Critic Loss: 12.85625171661377
Epoch: 90, Actor Loss: 0.6327731609344482
Epoch: 90, Critic Loss: 11.433216094970703
2025-03-24 15:13:47,571 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:13:51,095 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:13:51,095 - evaluators.py:130 - Average Virtual Goal Value = 1.6867766380310059
2025-03-24 15:13:51,096 - evaluators.py:132 - Goal Reach Probability = 0.0337355331199212
2025-03-24 15:13:51,096 - evaluators.py:134 - Trap Reach Probability = 0.9662644668800788
2025-03-24 15:13:51,096 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:13:51,096 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:13:51,096 - evaluators.py:140 - Current Best Reach Probability = 0.0337355331199212
2025-03-24 15:13:51,096 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.6117913126945496
Epoch: 100, Critic Loss: 7.944236755371094
Epoch: 110, Actor Loss: 0.6027030944824219
Epoch: 110, Critic Loss: 11.789266586303711
Epoch: 120, Actor Loss: 0.5311352610588074
Epoch: 120, Critic Loss: 14.098999977111816
Epoch: 130, Actor Loss: 0.520559549331665
Epoch: 130, Critic Loss: 15.095361709594727
Epoch: 140, Actor Loss: 0.6153243184089661
Epoch: 140, Critic Loss: 8.7173433303833
2025-03-24 15:14:22,706 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:14:26,688 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:14:26,688 - evaluators.py:130 - Average Virtual Goal Value = 1.873892068862915
2025-03-24 15:14:26,688 - evaluators.py:132 - Goal Reach Probability = 0.037477842491770066
2025-03-24 15:14:26,688 - evaluators.py:134 - Trap Reach Probability = 0.9625221575082299
2025-03-24 15:14:26,688 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:14:26,688 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:14:26,688 - evaluators.py:140 - Current Best Reach Probability = 0.037477842491770066
2025-03-24 15:14:26,688 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.586382269859314
Epoch: 150, Critic Loss: 14.44991683959961
Epoch: 160, Actor Loss: 0.5532551407814026
Epoch: 160, Critic Loss: 11.443758010864258
Epoch: 170, Actor Loss: 0.53178870677948
Epoch: 170, Critic Loss: 10.958230972290039
Epoch: 180, Actor Loss: 0.5635442137718201
Epoch: 180, Critic Loss: 8.616531372070312
Epoch: 190, Actor Loss: 0.5000503063201904
Epoch: 190, Critic Loss: 21.176513671875
2025-03-24 15:14:58,832 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:15:02,266 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:15:02,266 - evaluators.py:130 - Average Virtual Goal Value = 1.6206029653549194
2025-03-24 15:15:02,266 - evaluators.py:132 - Goal Reach Probability = 0.03241206030150754
2025-03-24 15:15:02,266 - evaluators.py:134 - Trap Reach Probability = 0.9675879396984924
2025-03-24 15:15:02,266 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:15:02,266 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:15:02,266 - evaluators.py:140 - Current Best Reach Probability = 0.037477842491770066
2025-03-24 15:15:02,267 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.5382599830627441
Epoch: 200, Critic Loss: 20.93997573852539
Epoch: 210, Actor Loss: 0.5245003700256348
Epoch: 210, Critic Loss: 6.982837677001953
Epoch: 220, Actor Loss: 0.5110893845558167
Epoch: 220, Critic Loss: 8.578529357910156
Epoch: 230, Actor Loss: 0.5218607783317566
Epoch: 230, Critic Loss: 7.114869117736816
Epoch: 240, Actor Loss: 0.49616310000419617
Epoch: 240, Critic Loss: 12.071477890014648
2025-03-24 15:15:34,167 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:15:37,270 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:15:37,270 - evaluators.py:130 - Average Virtual Goal Value = 2.5583245754241943
2025-03-24 15:15:37,270 - evaluators.py:132 - Goal Reach Probability = 0.051166489925768825
2025-03-24 15:15:37,270 - evaluators.py:134 - Trap Reach Probability = 0.9488335100742312
2025-03-24 15:15:37,270 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:15:37,270 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:15:37,270 - evaluators.py:140 - Current Best Reach Probability = 0.051166489925768825
2025-03-24 15:15:37,270 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.44739899039268494
Epoch: 250, Critic Loss: 14.695989608764648
Epoch: 260, Actor Loss: 0.4475916922092438
Epoch: 260, Critic Loss: 6.725415229797363
Epoch: 270, Actor Loss: 0.41812989115715027
Epoch: 270, Critic Loss: 11.193375587463379
Epoch: 280, Actor Loss: 0.4274045526981354
Epoch: 280, Critic Loss: 11.50898551940918
Epoch: 290, Actor Loss: 0.4234321713447571
Epoch: 290, Critic Loss: 8.610095024108887
2025-03-24 15:16:08,312 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:16:11,693 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:16:11,693 - evaluators.py:130 - Average Virtual Goal Value = 4.080442905426025
2025-03-24 15:16:11,693 - evaluators.py:132 - Goal Reach Probability = 0.08160886039055669
2025-03-24 15:16:11,693 - evaluators.py:134 - Trap Reach Probability = 0.9183911396094433
2025-03-24 15:16:11,693 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:16:11,693 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:16:11,693 - evaluators.py:140 - Current Best Reach Probability = 0.08160886039055669
2025-03-24 15:16:11,693 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.3523264229297638
Epoch: 300, Critic Loss: 13.588678359985352
Epoch: 310, Actor Loss: 0.36519744992256165
Epoch: 310, Critic Loss: 12.566555976867676
Epoch: 320, Actor Loss: 0.3070809841156006
Epoch: 320, Critic Loss: 10.13097858428955
Epoch: 330, Actor Loss: 0.40182191133499146
Epoch: 330, Critic Loss: 14.972673416137695
Epoch: 340, Actor Loss: 0.3154427707195282
Epoch: 340, Critic Loss: 21.181934356689453
2025-03-24 15:16:43,021 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:16:47,082 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:16:47,082 - evaluators.py:130 - Average Virtual Goal Value = 4.799879550933838
2025-03-24 15:16:47,082 - evaluators.py:132 - Goal Reach Probability = 0.09599759253686428
2025-03-24 15:16:47,082 - evaluators.py:134 - Trap Reach Probability = 0.9040024074631358
2025-03-24 15:16:47,082 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:16:47,082 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:16:47,082 - evaluators.py:140 - Current Best Reach Probability = 0.09599759253686428
2025-03-24 15:16:47,082 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.32531192898750305
Epoch: 350, Critic Loss: 11.147601127624512
Epoch: 360, Actor Loss: 0.36872559785842896
Epoch: 360, Critic Loss: 13.076662063598633
Epoch: 370, Actor Loss: 0.36891186237335205
Epoch: 370, Critic Loss: 9.503158569335938
Epoch: 380, Actor Loss: 0.25788965821266174
Epoch: 380, Critic Loss: 9.661491394042969
Epoch: 390, Actor Loss: 0.27900394797325134
Epoch: 390, Critic Loss: 14.328460693359375
2025-03-24 15:17:18,960 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:17:21,957 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:17:21,957 - evaluators.py:130 - Average Virtual Goal Value = 5.794806957244873
2025-03-24 15:17:21,957 - evaluators.py:132 - Goal Reach Probability = 0.11589613679544016
2025-03-24 15:17:21,957 - evaluators.py:134 - Trap Reach Probability = 0.8841038632045598
2025-03-24 15:17:21,957 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:17:21,957 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:17:21,957 - evaluators.py:140 - Current Best Reach Probability = 0.11589613679544016
2025-03-24 15:17:21,957 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.33032798767089844
Epoch: 400, Critic Loss: 12.165704727172852
Epoch: 410, Actor Loss: 0.29576390981674194
Epoch: 410, Critic Loss: 7.991350173950195
Epoch: 420, Actor Loss: 0.39856240153312683
Epoch: 420, Critic Loss: 17.391441345214844
Epoch: 430, Actor Loss: 0.29162514209747314
Epoch: 430, Critic Loss: 12.420698165893555
Epoch: 440, Actor Loss: 0.3756689429283142
Epoch: 440, Critic Loss: 14.279187202453613
2025-03-24 15:17:53,503 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:17:56,548 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:17:56,548 - evaluators.py:130 - Average Virtual Goal Value = 8.007054328918457
2025-03-24 15:17:56,548 - evaluators.py:132 - Goal Reach Probability = 0.16014109347442682
2025-03-24 15:17:56,548 - evaluators.py:134 - Trap Reach Probability = 0.8398589065255732
2025-03-24 15:17:56,548 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:17:56,548 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:17:56,548 - evaluators.py:140 - Current Best Reach Probability = 0.16014109347442682
2025-03-24 15:17:56,548 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.31593096256256104
Epoch: 450, Critic Loss: 6.465372085571289
Epoch: 460, Actor Loss: 0.3428153097629547
Epoch: 460, Critic Loss: 11.321416854858398
Epoch: 470, Actor Loss: 0.29143035411834717
Epoch: 470, Critic Loss: 12.567886352539062
Epoch: 480, Actor Loss: 0.3262428343296051
Epoch: 480, Critic Loss: 9.211071014404297
Epoch: 490, Actor Loss: 0.32042765617370605
Epoch: 490, Critic Loss: 11.876197814941406
2025-03-24 15:18:28,150 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:18:31,520 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:18:31,520 - evaluators.py:130 - Average Virtual Goal Value = 8.411703109741211
2025-03-24 15:18:31,520 - evaluators.py:132 - Goal Reach Probability = 0.16823406478578892
2025-03-24 15:18:31,520 - evaluators.py:134 - Trap Reach Probability = 0.831765935214211
2025-03-24 15:18:31,520 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:18:31,520 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:18:31,520 - evaluators.py:140 - Current Best Reach Probability = 0.16823406478578892
2025-03-24 15:18:31,520 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.28122231364250183
Epoch: 500, Critic Loss: 18.141855239868164
Epoch: 510, Actor Loss: 0.3038357198238373
Epoch: 510, Critic Loss: 7.958436489105225
Epoch: 520, Actor Loss: 0.3334299921989441
Epoch: 520, Critic Loss: 6.439278602600098
Epoch: 530, Actor Loss: 0.2955029308795929
Epoch: 530, Critic Loss: 11.282266616821289
Epoch: 540, Actor Loss: 0.31017449498176575
Epoch: 540, Critic Loss: 9.820768356323242
2025-03-24 15:19:03,257 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:19:06,260 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:19:06,261 - evaluators.py:130 - Average Virtual Goal Value = 7.900136947631836
2025-03-24 15:19:06,261 - evaluators.py:132 - Goal Reach Probability = 0.15800273597811218
2025-03-24 15:19:06,261 - evaluators.py:134 - Trap Reach Probability = 0.8419972640218878
2025-03-24 15:19:06,261 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:19:06,261 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:19:06,261 - evaluators.py:140 - Current Best Reach Probability = 0.16823406478578892
2025-03-24 15:19:06,261 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.2805759310722351
Epoch: 550, Critic Loss: 15.83710765838623
Epoch: 560, Actor Loss: 0.23674960434436798
Epoch: 560, Critic Loss: 9.29035758972168
Epoch: 570, Actor Loss: 0.293608695268631
Epoch: 570, Critic Loss: 11.816461563110352
Epoch: 580, Actor Loss: 0.2995772659778595
Epoch: 580, Critic Loss: 18.00731658935547
Epoch: 590, Actor Loss: 0.2791225016117096
Epoch: 590, Critic Loss: 12.274508476257324
2025-03-24 15:19:38,420 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:19:41,723 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:19:41,724 - evaluators.py:130 - Average Virtual Goal Value = 9.876092910766602
2025-03-24 15:19:41,724 - evaluators.py:132 - Goal Reach Probability = 0.19752186588921283
2025-03-24 15:19:41,724 - evaluators.py:134 - Trap Reach Probability = 0.8024781341107872
2025-03-24 15:19:41,724 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:19:41,724 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:19:41,724 - evaluators.py:140 - Current Best Reach Probability = 0.19752186588921283
2025-03-24 15:19:41,724 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.3183605670928955
Epoch: 600, Critic Loss: 9.260660171508789
Epoch: 610, Actor Loss: 0.26862967014312744
Epoch: 610, Critic Loss: 17.845993041992188
Epoch: 620, Actor Loss: 0.27605047821998596
Epoch: 620, Critic Loss: 22.746244430541992
Epoch: 630, Actor Loss: 0.2501744329929352
Epoch: 630, Critic Loss: 6.541609764099121
Epoch: 640, Actor Loss: 0.25138187408447266
Epoch: 640, Critic Loss: 18.170913696289062
2025-03-24 15:20:15,325 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:20:18,985 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:20:18,985 - evaluators.py:130 - Average Virtual Goal Value = 9.250456809997559
2025-03-24 15:20:18,985 - evaluators.py:132 - Goal Reach Probability = 0.1850091407678245
2025-03-24 15:20:18,985 - evaluators.py:134 - Trap Reach Probability = 0.8149908592321755
2025-03-24 15:20:18,985 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:20:18,985 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:20:18,985 - evaluators.py:140 - Current Best Reach Probability = 0.19752186588921283
2025-03-24 15:20:18,985 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.24928823113441467
Epoch: 650, Critic Loss: 11.973121643066406
Epoch: 660, Actor Loss: 0.28954505920410156
Epoch: 660, Critic Loss: 15.974956512451172
Epoch: 670, Actor Loss: 0.29369059205055237
Epoch: 670, Critic Loss: 10.958863258361816
Epoch: 680, Actor Loss: 0.2766781151294708
Epoch: 680, Critic Loss: 7.940978050231934
Epoch: 690, Actor Loss: 0.29205426573753357
Epoch: 690, Critic Loss: 15.369735717773438
2025-03-24 15:20:50,771 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:20:54,203 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:20:54,203 - evaluators.py:130 - Average Virtual Goal Value = 8.411703109741211
2025-03-24 15:20:54,203 - evaluators.py:132 - Goal Reach Probability = 0.16823406478578892
2025-03-24 15:20:54,203 - evaluators.py:134 - Trap Reach Probability = 0.831765935214211
2025-03-24 15:20:54,203 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:20:54,203 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:20:54,203 - evaluators.py:140 - Current Best Reach Probability = 0.19752186588921283
2025-03-24 15:20:54,203 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.3576195240020752
Epoch: 700, Critic Loss: 16.746246337890625
2025-03-24 15:20:54,205 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 15:20:54,206 - father_agent.py:449 - Before training evaluation.
2025-03-24 15:20:54,206 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:21:01,278 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:21:01,278 - father_agent.py:564 - Average Virtual Goal Value = 9.53858757019043
2025-03-24 15:21:01,278 - father_agent.py:566 - Goal Reach Probability = 0.19077175313268532
2025-03-24 15:21:01,278 - father_agent.py:568 - Trap Reach Probability = 0.8092282468673146
2025-03-24 15:21:01,278 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:21:01,278 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:21:01,278 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:21:01,301 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:21:01,380 - father_agent.py:359 - Training agent on-policy
2025-03-24 15:21:08,453 - father_agent.py:306 - Step: 0, Training loss: 126.90843963623047
2025-03-24 15:21:08,479 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:21:17,597 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:21:17,597 - father_agent.py:564 - Average Virtual Goal Value = 0.003508033463731408
2025-03-24 15:21:17,597 - father_agent.py:566 - Goal Reach Probability = 7.016066792955869e-05
2025-03-24 15:21:17,597 - father_agent.py:568 - Trap Reach Probability = 0.9999298393320705
2025-03-24 15:21:17,597 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:21:17,597 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:21:17,597 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:21:17,622 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:21:24,537 - father_agent.py:306 - Step: 5, Training loss: 110.63273620605469
2025-03-24 15:21:25,842 - father_agent.py:306 - Step: 10, Training loss: 60.409000396728516
2025-03-24 15:21:27,179 - father_agent.py:306 - Step: 15, Training loss: 35.68070983886719
2025-03-24 15:21:28,513 - father_agent.py:306 - Step: 20, Training loss: 18.523591995239258
2025-03-24 15:21:29,803 - father_agent.py:306 - Step: 25, Training loss: 9.193354606628418
2025-03-24 15:21:31,196 - father_agent.py:306 - Step: 30, Training loss: 3.7071304321289062
2025-03-24 15:21:32,856 - father_agent.py:306 - Step: 35, Training loss: 1.5958174467086792
2025-03-24 15:21:34,687 - father_agent.py:306 - Step: 40, Training loss: 0.6293871998786926
2025-03-24 15:21:36,028 - father_agent.py:306 - Step: 45, Training loss: 0.35825303196907043
2025-03-24 15:21:37,358 - father_agent.py:306 - Step: 50, Training loss: 0.4019458591938019
2025-03-24 15:21:38,699 - father_agent.py:306 - Step: 55, Training loss: 0.47309792041778564
2025-03-24 15:21:39,989 - father_agent.py:306 - Step: 60, Training loss: 0.42635607719421387
2025-03-24 15:21:41,342 - father_agent.py:306 - Step: 65, Training loss: 0.45382562279701233
2025-03-24 15:21:42,609 - father_agent.py:306 - Step: 70, Training loss: 0.7395070791244507
2025-03-24 15:21:43,841 - father_agent.py:306 - Step: 75, Training loss: 0.3369466960430145
2025-03-24 15:21:45,232 - father_agent.py:306 - Step: 80, Training loss: 0.4255833923816681
2025-03-24 15:21:46,569 - father_agent.py:306 - Step: 85, Training loss: 0.7605268955230713
2025-03-24 15:21:47,828 - father_agent.py:306 - Step: 90, Training loss: 0.45732200145721436
2025-03-24 15:21:49,147 - father_agent.py:306 - Step: 95, Training loss: 0.2946830987930298
2025-03-24 15:21:50,504 - father_agent.py:306 - Step: 100, Training loss: 0.4400922358036041
2025-03-24 15:21:50,531 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:21:59,230 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:21:59,230 - father_agent.py:564 - Average Virtual Goal Value = 1.2179535627365112
2025-03-24 15:21:59,230 - father_agent.py:566 - Goal Reach Probability = 0.024359070746560407
2025-03-24 15:21:59,230 - father_agent.py:568 - Trap Reach Probability = 0.9756409292534396
2025-03-24 15:21:59,230 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:21:59,230 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:21:59,230 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:21:59,263 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:22:01,165 - father_agent.py:306 - Step: 105, Training loss: 0.6765953898429871
2025-03-24 15:22:02,414 - father_agent.py:306 - Step: 110, Training loss: 0.26889103651046753
2025-03-24 15:22:03,626 - father_agent.py:306 - Step: 115, Training loss: 0.4816659092903137
2025-03-24 15:22:04,977 - father_agent.py:306 - Step: 120, Training loss: 0.6343021392822266
2025-03-24 15:22:06,199 - father_agent.py:306 - Step: 125, Training loss: 0.4317385256290436
2025-03-24 15:22:07,419 - father_agent.py:306 - Step: 130, Training loss: 0.3805564343929291
2025-03-24 15:22:08,771 - father_agent.py:306 - Step: 135, Training loss: 0.2781250476837158
2025-03-24 15:22:09,978 - father_agent.py:306 - Step: 140, Training loss: 0.39634642004966736
2025-03-24 15:22:11,171 - father_agent.py:306 - Step: 145, Training loss: 0.5425558090209961
2025-03-24 15:22:12,457 - father_agent.py:306 - Step: 150, Training loss: 0.5634011030197144
2025-03-24 15:22:13,664 - father_agent.py:306 - Step: 155, Training loss: 0.3375627398490906
2025-03-24 15:22:14,877 - father_agent.py:306 - Step: 160, Training loss: 0.4206005036830902
2025-03-24 15:22:16,155 - father_agent.py:306 - Step: 165, Training loss: 0.48914456367492676
2025-03-24 15:22:17,384 - father_agent.py:306 - Step: 170, Training loss: 0.5438818335533142
2025-03-24 15:22:18,661 - father_agent.py:306 - Step: 175, Training loss: 0.48762232065200806
2025-03-24 15:22:19,858 - father_agent.py:306 - Step: 180, Training loss: 0.34433263540267944
2025-03-24 15:22:21,152 - father_agent.py:306 - Step: 185, Training loss: 0.33893004059791565
2025-03-24 15:22:22,334 - father_agent.py:306 - Step: 190, Training loss: 0.27531012892723083
2025-03-24 15:22:23,554 - father_agent.py:306 - Step: 195, Training loss: 0.24376994371414185
2025-03-24 15:22:24,843 - father_agent.py:306 - Step: 200, Training loss: 0.4856167435646057
2025-03-24 15:22:24,874 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:22:34,023 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:22:34,023 - father_agent.py:564 - Average Virtual Goal Value = 1.1029411554336548
2025-03-24 15:22:34,023 - father_agent.py:566 - Goal Reach Probability = 0.022058823529411766
2025-03-24 15:22:34,023 - father_agent.py:568 - Trap Reach Probability = 0.9779411764705882
2025-03-24 15:22:34,023 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:22:34,023 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:22:34,023 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:22:34,060 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:22:35,960 - father_agent.py:306 - Step: 205, Training loss: 0.411141574382782
2025-03-24 15:22:37,152 - father_agent.py:306 - Step: 210, Training loss: 0.3688949644565582
2025-03-24 15:22:38,338 - father_agent.py:306 - Step: 215, Training loss: 0.43194863200187683
2025-03-24 15:22:39,603 - father_agent.py:306 - Step: 220, Training loss: 0.3776710629463196
2025-03-24 15:22:40,861 - father_agent.py:306 - Step: 225, Training loss: 0.33530259132385254
2025-03-24 15:22:42,056 - father_agent.py:306 - Step: 230, Training loss: 0.3700220286846161
2025-03-24 15:22:43,334 - father_agent.py:306 - Step: 235, Training loss: 0.5186340808868408
2025-03-24 15:22:44,606 - father_agent.py:306 - Step: 240, Training loss: 0.463293194770813
2025-03-24 15:22:45,807 - father_agent.py:306 - Step: 245, Training loss: 0.4305512607097626
2025-03-24 15:22:47,114 - father_agent.py:306 - Step: 250, Training loss: 0.4007124900817871
2025-03-24 15:22:48,334 - father_agent.py:306 - Step: 255, Training loss: 0.6224655508995056
2025-03-24 15:22:49,534 - father_agent.py:306 - Step: 260, Training loss: 0.4439041018486023
2025-03-24 15:22:51,045 - father_agent.py:306 - Step: 265, Training loss: 0.4015159606933594
2025-03-24 15:22:52,320 - father_agent.py:306 - Step: 270, Training loss: 0.39241331815719604
2025-03-24 15:22:53,635 - father_agent.py:306 - Step: 275, Training loss: 0.3594280779361725
2025-03-24 15:22:54,946 - father_agent.py:306 - Step: 280, Training loss: 0.449529767036438
2025-03-24 15:22:56,295 - father_agent.py:306 - Step: 285, Training loss: 0.6137319803237915
2025-03-24 15:22:57,958 - father_agent.py:306 - Step: 290, Training loss: 0.47767168283462524
2025-03-24 15:22:59,364 - father_agent.py:306 - Step: 295, Training loss: 0.2828004062175751
2025-03-24 15:23:00,551 - father_agent.py:306 - Step: 300, Training loss: 0.36003023386001587
2025-03-24 15:23:00,581 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:23:09,625 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:23:09,626 - father_agent.py:564 - Average Virtual Goal Value = 1.1030312776565552
2025-03-24 15:23:09,626 - father_agent.py:566 - Goal Reach Probability = 0.02206062586812648
2025-03-24 15:23:09,626 - father_agent.py:568 - Trap Reach Probability = 0.9779393741318735
2025-03-24 15:23:09,626 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:23:09,626 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:23:09,626 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:23:09,653 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:23:11,519 - father_agent.py:306 - Step: 305, Training loss: 0.645442008972168
2025-03-24 15:23:12,806 - father_agent.py:306 - Step: 310, Training loss: 0.4166404902935028
2025-03-24 15:23:14,025 - father_agent.py:306 - Step: 315, Training loss: 0.4028109908103943
2025-03-24 15:23:15,310 - father_agent.py:306 - Step: 320, Training loss: 0.46651414036750793
2025-03-24 15:23:16,587 - father_agent.py:306 - Step: 325, Training loss: 0.4075191617012024
2025-03-24 15:23:17,966 - father_agent.py:306 - Step: 330, Training loss: 0.29520493745803833
2025-03-24 15:23:19,484 - father_agent.py:306 - Step: 335, Training loss: 0.3786161243915558
2025-03-24 15:23:20,793 - father_agent.py:306 - Step: 340, Training loss: 0.3974173069000244
2025-03-24 15:23:22,238 - father_agent.py:306 - Step: 345, Training loss: 0.5811178088188171
2025-03-24 15:23:23,497 - father_agent.py:306 - Step: 350, Training loss: 0.5420918464660645
2025-03-24 15:23:24,709 - father_agent.py:306 - Step: 355, Training loss: 0.548970639705658
2025-03-24 15:23:25,899 - father_agent.py:306 - Step: 360, Training loss: 0.4132652282714844
2025-03-24 15:23:27,235 - father_agent.py:306 - Step: 365, Training loss: 0.39388424158096313
2025-03-24 15:23:28,506 - father_agent.py:306 - Step: 370, Training loss: 0.43628600239753723
2025-03-24 15:23:29,960 - father_agent.py:306 - Step: 375, Training loss: 0.45063260197639465
2025-03-24 15:23:31,288 - father_agent.py:306 - Step: 380, Training loss: 0.4525032639503479
2025-03-24 15:23:32,552 - father_agent.py:306 - Step: 385, Training loss: 0.6075172424316406
2025-03-24 15:23:33,733 - father_agent.py:306 - Step: 390, Training loss: 0.4670383632183075
2025-03-24 15:23:35,020 - father_agent.py:306 - Step: 395, Training loss: 0.5585407614707947
2025-03-24 15:23:36,297 - father_agent.py:306 - Step: 400, Training loss: 0.4396060109138489
2025-03-24 15:23:36,334 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:23:45,853 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:23:45,853 - father_agent.py:564 - Average Virtual Goal Value = 1.23080575466156
2025-03-24 15:23:45,853 - father_agent.py:566 - Goal Reach Probability = 0.024616115244578123
2025-03-24 15:23:45,853 - father_agent.py:568 - Trap Reach Probability = 0.9753838847554219
2025-03-24 15:23:45,853 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:23:45,853 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:23:45,853 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:23:45,885 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:23:48,152 - father_agent.py:306 - Step: 405, Training loss: 0.30544549226760864
2025-03-24 15:23:49,492 - father_agent.py:306 - Step: 410, Training loss: 0.6169687509536743
2025-03-24 15:23:50,906 - father_agent.py:306 - Step: 415, Training loss: 0.6111363172531128
2025-03-24 15:23:52,366 - father_agent.py:306 - Step: 420, Training loss: 0.5483278036117554
2025-03-24 15:23:53,574 - father_agent.py:306 - Step: 425, Training loss: 0.3627450168132782
2025-03-24 15:23:54,896 - father_agent.py:306 - Step: 430, Training loss: 0.4519306719303131
2025-03-24 15:23:56,189 - father_agent.py:306 - Step: 435, Training loss: 0.517198383808136
2025-03-24 15:23:57,531 - father_agent.py:306 - Step: 440, Training loss: 0.4394316077232361
2025-03-24 15:23:58,959 - father_agent.py:306 - Step: 445, Training loss: 0.45296695828437805
2025-03-24 15:24:00,322 - father_agent.py:306 - Step: 450, Training loss: 0.5088931918144226
2025-03-24 15:24:01,574 - father_agent.py:306 - Step: 455, Training loss: 0.3966235816478729
2025-03-24 15:24:03,004 - father_agent.py:306 - Step: 460, Training loss: 0.4208582937717438
2025-03-24 15:24:04,207 - father_agent.py:306 - Step: 465, Training loss: 0.3550140857696533
2025-03-24 15:24:05,535 - father_agent.py:306 - Step: 470, Training loss: 0.5467191338539124
2025-03-24 15:24:06,952 - father_agent.py:306 - Step: 475, Training loss: 0.45938438177108765
2025-03-24 15:24:08,234 - father_agent.py:306 - Step: 480, Training loss: 0.4150669574737549
2025-03-24 15:24:09,489 - father_agent.py:306 - Step: 485, Training loss: 0.577329158782959
2025-03-24 15:24:10,873 - father_agent.py:306 - Step: 490, Training loss: 0.37455475330352783
2025-03-24 15:24:12,133 - father_agent.py:306 - Step: 495, Training loss: 0.6175531148910522
2025-03-24 15:24:13,461 - father_agent.py:306 - Step: 500, Training loss: 0.49979323148727417
2025-03-24 15:24:13,495 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:24:23,110 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:24:23,110 - father_agent.py:564 - Average Virtual Goal Value = 1.5100284814834595
2025-03-24 15:24:23,110 - father_agent.py:566 - Goal Reach Probability = 0.030200568662107123
2025-03-24 15:24:23,110 - father_agent.py:568 - Trap Reach Probability = 0.9697994313378929
2025-03-24 15:24:23,110 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:24:23,110 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:24:23,110 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:24:23,139 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:24:25,571 - father_agent.py:306 - Step: 505, Training loss: 0.30322402715682983
2025-03-24 15:24:26,924 - father_agent.py:306 - Step: 510, Training loss: 0.376029908657074
2025-03-24 15:24:28,383 - father_agent.py:306 - Step: 515, Training loss: 0.31878662109375
2025-03-24 15:24:29,739 - father_agent.py:306 - Step: 520, Training loss: 0.4753822684288025
2025-03-24 15:24:31,069 - father_agent.py:306 - Step: 525, Training loss: 0.5138754844665527
2025-03-24 15:24:32,470 - father_agent.py:306 - Step: 530, Training loss: 0.4224581718444824
2025-03-24 15:24:33,709 - father_agent.py:306 - Step: 535, Training loss: 0.3398025333881378
2025-03-24 15:24:35,020 - father_agent.py:306 - Step: 540, Training loss: 0.4812145531177521
2025-03-24 15:24:36,328 - father_agent.py:306 - Step: 545, Training loss: 0.4277713894844055
2025-03-24 15:24:37,654 - father_agent.py:306 - Step: 550, Training loss: 0.5975775718688965
2025-03-24 15:24:39,035 - father_agent.py:306 - Step: 555, Training loss: 0.2790483236312866
2025-03-24 15:24:40,350 - father_agent.py:306 - Step: 560, Training loss: 0.5449653267860413
2025-03-24 15:24:41,607 - father_agent.py:306 - Step: 565, Training loss: 0.39232340455055237
2025-03-24 15:24:43,040 - father_agent.py:306 - Step: 570, Training loss: 0.3831464350223541
2025-03-24 15:24:44,402 - father_agent.py:306 - Step: 575, Training loss: 0.5794238448143005
2025-03-24 15:24:45,660 - father_agent.py:306 - Step: 580, Training loss: 0.48350048065185547
2025-03-24 15:24:47,067 - father_agent.py:306 - Step: 585, Training loss: 0.3659731149673462
2025-03-24 15:24:48,366 - father_agent.py:306 - Step: 590, Training loss: 0.39826756715774536
2025-03-24 15:24:49,649 - father_agent.py:306 - Step: 595, Training loss: 0.23585113883018494
2025-03-24 15:24:51,054 - father_agent.py:306 - Step: 600, Training loss: 0.36720603704452515
2025-03-24 15:24:51,086 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:25:01,368 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:25:01,369 - father_agent.py:564 - Average Virtual Goal Value = 1.3205901384353638
2025-03-24 15:25:01,369 - father_agent.py:566 - Goal Reach Probability = 0.026411802030456854
2025-03-24 15:25:01,369 - father_agent.py:568 - Trap Reach Probability = 0.9735881979695431
2025-03-24 15:25:01,369 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:25:01,369 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:25:01,369 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:25:01,409 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:25:03,636 - father_agent.py:306 - Step: 605, Training loss: 0.5608510971069336
2025-03-24 15:25:05,239 - father_agent.py:306 - Step: 610, Training loss: 0.6417296528816223
2025-03-24 15:25:06,640 - father_agent.py:306 - Step: 615, Training loss: 0.44812750816345215
2025-03-24 15:25:08,146 - father_agent.py:306 - Step: 620, Training loss: 0.5885863304138184
2025-03-24 15:25:09,406 - father_agent.py:306 - Step: 625, Training loss: 0.34826579689979553
2025-03-24 15:25:10,780 - father_agent.py:306 - Step: 630, Training loss: 0.46173179149627686
2025-03-24 15:25:12,120 - father_agent.py:306 - Step: 635, Training loss: 0.3208251893520355
2025-03-24 15:25:13,371 - father_agent.py:306 - Step: 640, Training loss: 0.38689762353897095
2025-03-24 15:25:14,787 - father_agent.py:306 - Step: 645, Training loss: 0.5703758001327515
2025-03-24 15:25:16,066 - father_agent.py:306 - Step: 650, Training loss: 0.4538228511810303
2025-03-24 15:25:17,381 - father_agent.py:306 - Step: 655, Training loss: 0.4647621512413025
2025-03-24 15:25:18,882 - father_agent.py:306 - Step: 660, Training loss: 0.46301642060279846
2025-03-24 15:25:20,323 - father_agent.py:306 - Step: 665, Training loss: 0.2832331657409668
2025-03-24 15:25:21,746 - father_agent.py:306 - Step: 670, Training loss: 0.3385896384716034
2025-03-24 15:25:23,302 - father_agent.py:306 - Step: 675, Training loss: 0.4455607533454895
2025-03-24 15:25:24,711 - father_agent.py:306 - Step: 680, Training loss: 0.6146403551101685
2025-03-24 15:25:26,356 - father_agent.py:306 - Step: 685, Training loss: 0.47870469093322754
2025-03-24 15:25:27,904 - father_agent.py:306 - Step: 690, Training loss: 0.703345775604248
2025-03-24 15:25:29,359 - father_agent.py:306 - Step: 695, Training loss: 0.48905080556869507
2025-03-24 15:25:30,893 - father_agent.py:306 - Step: 700, Training loss: 0.47981059551239014
2025-03-24 15:25:30,929 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:25:40,895 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:25:40,895 - father_agent.py:564 - Average Virtual Goal Value = 1.7163540124893188
2025-03-24 15:25:40,895 - father_agent.py:566 - Goal Reach Probability = 0.03432708025219896
2025-03-24 15:25:40,895 - father_agent.py:568 - Trap Reach Probability = 0.965672919747801
2025-03-24 15:25:40,896 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:25:40,896 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:25:40,896 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:25:40,933 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:25:41,022 - father_agent.py:455 - Training finished.
2025-03-24 15:25:41,027 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:25:41,124 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 15:25:50,407 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:25:50,407 - father_agent.py:564 - Average Virtual Goal Value = 1.8162726163864136
2025-03-24 15:25:50,407 - father_agent.py:566 - Goal Reach Probability = 0.03632545115121344
2025-03-24 15:25:50,407 - father_agent.py:568 - Trap Reach Probability = 0.9636745488487866
2025-03-24 15:25:50,407 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:25:50,407 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:25:50,407 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:25:50,438 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:26:07,666 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:26:07,666 - evaluators.py:130 - Average Virtual Goal Value = 1.4328285455703735
2025-03-24 15:26:07,666 - evaluators.py:132 - Goal Reach Probability = 0.028656571606777783
2025-03-24 15:26:07,666 - evaluators.py:134 - Trap Reach Probability = 0.9713434283932222
2025-03-24 15:26:07,666 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:26:07,666 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:26:07,666 - evaluators.py:140 - Current Best Reach Probability = 0.028656571606777783
2025-03-24 15:26:07,678 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:26:50,789 - cloned_fsc_actor_policy.py:189 - Epoch 0, Loss: 2.0496
2025-03-24 15:26:50,789 - cloned_fsc_actor_policy.py:190 - Epoch 0, Accuracy: 0.3275
2025-03-24 15:26:50,818 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:27:08,820 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:27:08,820 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 15:27:08,820 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 15:27:08,820 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 15:27:08,820 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:27:08,820 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:27:08,820 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 15:27:08,839 - attrs.py:205 - Creating converter from 5 to 3
2025-03-24 15:27:12,583 - cloned_fsc_actor_policy.py:189 - Epoch 1000, Loss: 0.8603
2025-03-24 15:27:12,584 - cloned_fsc_actor_policy.py:190 - Epoch 1000, Accuracy: 0.6978
2025-03-24 15:27:16,281 - cloned_fsc_actor_policy.py:189 - Epoch 2000, Loss: 0.4479
2025-03-24 15:27:16,282 - cloned_fsc_actor_policy.py:190 - Epoch 2000, Accuracy: 0.8119
2025-03-24 15:27:19,849 - cloned_fsc_actor_policy.py:189 - Epoch 3000, Loss: 0.3903
2025-03-24 15:27:19,849 - cloned_fsc_actor_policy.py:190 - Epoch 3000, Accuracy: 0.8308
2025-03-24 15:27:23,433 - cloned_fsc_actor_policy.py:189 - Epoch 4000, Loss: 0.3040
2025-03-24 15:27:23,433 - cloned_fsc_actor_policy.py:190 - Epoch 4000, Accuracy: 0.8644
2025-03-24 15:27:26,982 - cloned_fsc_actor_policy.py:189 - Epoch 5000, Loss: 0.2481
2025-03-24 15:27:26,982 - cloned_fsc_actor_policy.py:190 - Epoch 5000, Accuracy: 0.8866
2025-03-24 15:27:27,016 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:27:44,580 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:27:44,580 - evaluators.py:130 - Average Virtual Goal Value = 0.04398540407419205
2025-03-24 15:27:44,580 - evaluators.py:132 - Goal Reach Probability = 0.0008797080672487945
2025-03-24 15:27:44,580 - evaluators.py:134 - Trap Reach Probability = 0.9991202919327512
2025-03-24 15:27:44,580 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:27:44,580 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:27:44,580 - evaluators.py:140 - Current Best Reach Probability = 0.0008797080672487945
2025-03-24 15:27:48,853 - cloned_fsc_actor_policy.py:189 - Epoch 6000, Loss: 0.2390
2025-03-24 15:27:48,853 - cloned_fsc_actor_policy.py:190 - Epoch 6000, Accuracy: 0.8851
2025-03-24 15:27:52,584 - cloned_fsc_actor_policy.py:189 - Epoch 7000, Loss: 0.2290
2025-03-24 15:27:52,584 - cloned_fsc_actor_policy.py:190 - Epoch 7000, Accuracy: 0.8803
2025-03-24 15:27:56,362 - cloned_fsc_actor_policy.py:189 - Epoch 8000, Loss: 0.2199
2025-03-24 15:27:56,362 - cloned_fsc_actor_policy.py:190 - Epoch 8000, Accuracy: 0.8794
2025-03-24 15:28:00,314 - cloned_fsc_actor_policy.py:189 - Epoch 9000, Loss: 0.2116
2025-03-24 15:28:00,314 - cloned_fsc_actor_policy.py:190 - Epoch 9000, Accuracy: 0.8791
2025-03-24 15:28:04,347 - cloned_fsc_actor_policy.py:189 - Epoch 10000, Loss: 0.2048
2025-03-24 15:28:04,347 - cloned_fsc_actor_policy.py:190 - Epoch 10000, Accuracy: 0.8809
2025-03-24 15:28:04,381 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:28:19,513 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:28:19,513 - evaluators.py:130 - Average Virtual Goal Value = 4.943212032318115
2025-03-24 15:28:19,513 - evaluators.py:132 - Goal Reach Probability = 0.09886424270142541
2025-03-24 15:28:19,513 - evaluators.py:134 - Trap Reach Probability = 0.9011357572985745
2025-03-24 15:28:19,513 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:28:19,513 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:28:19,513 - evaluators.py:140 - Current Best Reach Probability = 0.09886424270142541
2025-03-24 15:28:23,134 - cloned_fsc_actor_policy.py:189 - Epoch 11000, Loss: 0.1997
2025-03-24 15:28:23,134 - cloned_fsc_actor_policy.py:190 - Epoch 11000, Accuracy: 0.8816
2025-03-24 15:28:26,694 - cloned_fsc_actor_policy.py:189 - Epoch 12000, Loss: 0.1967
2025-03-24 15:28:26,694 - cloned_fsc_actor_policy.py:190 - Epoch 12000, Accuracy: 0.8845
2025-03-24 15:28:30,325 - cloned_fsc_actor_policy.py:189 - Epoch 13000, Loss: 0.1901
2025-03-24 15:28:30,325 - cloned_fsc_actor_policy.py:190 - Epoch 13000, Accuracy: 0.8964
2025-03-24 15:28:33,858 - cloned_fsc_actor_policy.py:189 - Epoch 14000, Loss: 0.1806
2025-03-24 15:28:33,858 - cloned_fsc_actor_policy.py:190 - Epoch 14000, Accuracy: 0.9151
2025-03-24 15:28:37,412 - cloned_fsc_actor_policy.py:189 - Epoch 15000, Loss: 0.1723
2025-03-24 15:28:37,412 - cloned_fsc_actor_policy.py:190 - Epoch 15000, Accuracy: 0.9287
2025-03-24 15:28:37,442 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:28:52,342 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:28:52,342 - evaluators.py:130 - Average Virtual Goal Value = 5.0531816482543945
2025-03-24 15:28:52,342 - evaluators.py:132 - Goal Reach Probability = 0.10106362987183824
2025-03-24 15:28:52,342 - evaluators.py:134 - Trap Reach Probability = 0.8989363701281617
2025-03-24 15:28:52,342 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:28:52,342 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:28:52,342 - evaluators.py:140 - Current Best Reach Probability = 0.10106362987183824
2025-03-24 15:28:56,054 - cloned_fsc_actor_policy.py:189 - Epoch 16000, Loss: 0.1662
2025-03-24 15:28:56,054 - cloned_fsc_actor_policy.py:190 - Epoch 16000, Accuracy: 0.9306
2025-03-24 15:28:59,681 - cloned_fsc_actor_policy.py:189 - Epoch 17000, Loss: 0.1588
2025-03-24 15:28:59,681 - cloned_fsc_actor_policy.py:190 - Epoch 17000, Accuracy: 0.9321
2025-03-24 15:29:03,435 - cloned_fsc_actor_policy.py:189 - Epoch 18000, Loss: 0.1509
2025-03-24 15:29:03,435 - cloned_fsc_actor_policy.py:190 - Epoch 18000, Accuracy: 0.9322
2025-03-24 15:29:07,096 - cloned_fsc_actor_policy.py:189 - Epoch 19000, Loss: 0.1397
2025-03-24 15:29:07,097 - cloned_fsc_actor_policy.py:190 - Epoch 19000, Accuracy: 0.9346
2025-03-24 15:29:11,368 - cloned_fsc_actor_policy.py:189 - Epoch 20000, Loss: 0.1253
2025-03-24 15:29:11,368 - cloned_fsc_actor_policy.py:190 - Epoch 20000, Accuracy: 0.9583
2025-03-24 15:29:11,398 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:29:27,146 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:29:27,147 - evaluators.py:130 - Average Virtual Goal Value = 3.202650547027588
2025-03-24 15:29:27,147 - evaluators.py:132 - Goal Reach Probability = 0.06405300938707896
2025-03-24 15:29:27,147 - evaluators.py:134 - Trap Reach Probability = 0.9359469906129211
2025-03-24 15:29:27,147 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:29:27,147 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:29:27,147 - evaluators.py:140 - Current Best Reach Probability = 0.10106362987183824
2025-03-24 15:29:30,635 - cloned_fsc_actor_policy.py:189 - Epoch 21000, Loss: 0.1175
2025-03-24 15:29:30,635 - cloned_fsc_actor_policy.py:190 - Epoch 21000, Accuracy: 0.9606
2025-03-24 15:29:34,484 - cloned_fsc_actor_policy.py:189 - Epoch 22000, Loss: 0.1116
2025-03-24 15:29:34,484 - cloned_fsc_actor_policy.py:190 - Epoch 22000, Accuracy: 0.9615
2025-03-24 15:29:38,387 - cloned_fsc_actor_policy.py:189 - Epoch 23000, Loss: 0.1058
2025-03-24 15:29:38,387 - cloned_fsc_actor_policy.py:190 - Epoch 23000, Accuracy: 0.9619
2025-03-24 15:29:42,113 - cloned_fsc_actor_policy.py:189 - Epoch 24000, Loss: 0.1026
2025-03-24 15:29:42,114 - cloned_fsc_actor_policy.py:190 - Epoch 24000, Accuracy: 0.9613
2025-03-24 15:29:46,061 - cloned_fsc_actor_policy.py:189 - Epoch 25000, Loss: 0.0984
2025-03-24 15:29:46,062 - cloned_fsc_actor_policy.py:190 - Epoch 25000, Accuracy: 0.9618
2025-03-24 15:29:46,098 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:30:02,747 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:30:02,748 - evaluators.py:130 - Average Virtual Goal Value = 7.004868030548096
2025-03-24 15:30:02,748 - evaluators.py:132 - Goal Reach Probability = 0.1400973590402085
2025-03-24 15:30:02,748 - evaluators.py:134 - Trap Reach Probability = 0.8599026409597915
2025-03-24 15:30:02,748 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:30:02,748 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:30:02,748 - evaluators.py:140 - Current Best Reach Probability = 0.1400973590402085
2025-03-24 15:30:06,397 - cloned_fsc_actor_policy.py:189 - Epoch 26000, Loss: 0.0968
2025-03-24 15:30:06,397 - cloned_fsc_actor_policy.py:190 - Epoch 26000, Accuracy: 0.9612
2025-03-24 15:30:10,004 - cloned_fsc_actor_policy.py:189 - Epoch 27000, Loss: 0.0953
2025-03-24 15:30:10,004 - cloned_fsc_actor_policy.py:190 - Epoch 27000, Accuracy: 0.9607
2025-03-24 15:30:13,627 - cloned_fsc_actor_policy.py:189 - Epoch 28000, Loss: 0.0913
2025-03-24 15:30:13,627 - cloned_fsc_actor_policy.py:190 - Epoch 28000, Accuracy: 0.9614
2025-03-24 15:30:17,332 - cloned_fsc_actor_policy.py:189 - Epoch 29000, Loss: 0.0889
2025-03-24 15:30:17,333 - cloned_fsc_actor_policy.py:190 - Epoch 29000, Accuracy: 0.9618
2025-03-24 15:30:21,273 - cloned_fsc_actor_policy.py:189 - Epoch 30000, Loss: 0.0870
2025-03-24 15:30:21,273 - cloned_fsc_actor_policy.py:190 - Epoch 30000, Accuracy: 0.9615
2025-03-24 15:30:21,306 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:30:35,869 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:30:35,869 - evaluators.py:130 - Average Virtual Goal Value = 7.055685520172119
2025-03-24 15:30:35,869 - evaluators.py:132 - Goal Reach Probability = 0.14111371224622943
2025-03-24 15:30:35,869 - evaluators.py:134 - Trap Reach Probability = 0.8588862877537706
2025-03-24 15:30:35,869 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:30:35,869 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:30:35,869 - evaluators.py:140 - Current Best Reach Probability = 0.14111371224622943
2025-03-24 15:30:39,441 - cloned_fsc_actor_policy.py:189 - Epoch 31000, Loss: 0.0849
2025-03-24 15:30:39,442 - cloned_fsc_actor_policy.py:190 - Epoch 31000, Accuracy: 0.9610
2025-03-24 15:30:43,118 - cloned_fsc_actor_policy.py:189 - Epoch 32000, Loss: 0.0826
2025-03-24 15:30:43,118 - cloned_fsc_actor_policy.py:190 - Epoch 32000, Accuracy: 0.9616
2025-03-24 15:30:46,734 - cloned_fsc_actor_policy.py:189 - Epoch 33000, Loss: 0.0820
2025-03-24 15:30:46,734 - cloned_fsc_actor_policy.py:190 - Epoch 33000, Accuracy: 0.9612
2025-03-24 15:30:50,353 - cloned_fsc_actor_policy.py:189 - Epoch 34000, Loss: 0.0789
2025-03-24 15:30:50,354 - cloned_fsc_actor_policy.py:190 - Epoch 34000, Accuracy: 0.9621
2025-03-24 15:30:54,079 - cloned_fsc_actor_policy.py:189 - Epoch 35000, Loss: 0.0778
2025-03-24 15:30:54,079 - cloned_fsc_actor_policy.py:190 - Epoch 35000, Accuracy: 0.9619
2025-03-24 15:30:54,209 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:31:09,220 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:31:09,220 - evaluators.py:130 - Average Virtual Goal Value = 7.035715579986572
2025-03-24 15:31:09,220 - evaluators.py:132 - Goal Reach Probability = 0.14071431311620056
2025-03-24 15:31:09,220 - evaluators.py:134 - Trap Reach Probability = 0.8592856868837995
2025-03-24 15:31:09,220 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:31:09,220 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:31:09,220 - evaluators.py:140 - Current Best Reach Probability = 0.14111371224622943
2025-03-24 15:31:13,067 - cloned_fsc_actor_policy.py:189 - Epoch 36000, Loss: 0.0777
2025-03-24 15:31:13,067 - cloned_fsc_actor_policy.py:190 - Epoch 36000, Accuracy: 0.9617
2025-03-24 15:31:17,382 - cloned_fsc_actor_policy.py:189 - Epoch 37000, Loss: 0.0759
2025-03-24 15:31:17,382 - cloned_fsc_actor_policy.py:190 - Epoch 37000, Accuracy: 0.9618
2025-03-24 15:31:21,149 - cloned_fsc_actor_policy.py:189 - Epoch 38000, Loss: 0.0771
2025-03-24 15:31:21,149 - cloned_fsc_actor_policy.py:190 - Epoch 38000, Accuracy: 0.9616
2025-03-24 15:31:24,841 - cloned_fsc_actor_policy.py:189 - Epoch 39000, Loss: 0.0751
2025-03-24 15:31:24,841 - cloned_fsc_actor_policy.py:190 - Epoch 39000, Accuracy: 0.9619
2025-03-24 15:31:28,601 - cloned_fsc_actor_policy.py:189 - Epoch 40000, Loss: 0.0734
2025-03-24 15:31:28,601 - cloned_fsc_actor_policy.py:190 - Epoch 40000, Accuracy: 0.9628
2025-03-24 15:31:28,638 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:31:43,407 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:31:43,407 - evaluators.py:130 - Average Virtual Goal Value = 6.741359710693359
2025-03-24 15:31:43,407 - evaluators.py:132 - Goal Reach Probability = 0.13482719331560958
2025-03-24 15:31:43,407 - evaluators.py:134 - Trap Reach Probability = 0.8651728066843904
2025-03-24 15:31:43,407 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:31:43,407 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:31:43,407 - evaluators.py:140 - Current Best Reach Probability = 0.14111371224622943
2025-03-24 15:31:46,975 - cloned_fsc_actor_policy.py:189 - Epoch 41000, Loss: 0.0729
2025-03-24 15:31:46,975 - cloned_fsc_actor_policy.py:190 - Epoch 41000, Accuracy: 0.9627
2025-03-24 15:31:50,656 - cloned_fsc_actor_policy.py:189 - Epoch 42000, Loss: 0.0723
2025-03-24 15:31:50,656 - cloned_fsc_actor_policy.py:190 - Epoch 42000, Accuracy: 0.9632
2025-03-24 15:31:54,536 - cloned_fsc_actor_policy.py:189 - Epoch 43000, Loss: 0.0723
2025-03-24 15:31:54,536 - cloned_fsc_actor_policy.py:190 - Epoch 43000, Accuracy: 0.9627
2025-03-24 15:31:58,310 - cloned_fsc_actor_policy.py:189 - Epoch 44000, Loss: 0.0712
2025-03-24 15:31:58,310 - cloned_fsc_actor_policy.py:190 - Epoch 44000, Accuracy: 0.9629
2025-03-24 15:32:01,745 - cloned_fsc_actor_policy.py:189 - Epoch 45000, Loss: 0.0716
2025-03-24 15:32:01,746 - cloned_fsc_actor_policy.py:190 - Epoch 45000, Accuracy: 0.9623
2025-03-24 15:32:01,775 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:32:16,843 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:32:16,843 - evaluators.py:130 - Average Virtual Goal Value = 6.703208923339844
2025-03-24 15:32:16,843 - evaluators.py:132 - Goal Reach Probability = 0.13406417722964487
2025-03-24 15:32:16,843 - evaluators.py:134 - Trap Reach Probability = 0.8659358227703552
2025-03-24 15:32:16,843 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:32:16,843 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:32:16,843 - evaluators.py:140 - Current Best Reach Probability = 0.14111371224622943
2025-03-24 15:32:20,715 - cloned_fsc_actor_policy.py:189 - Epoch 46000, Loss: 0.0711
2025-03-24 15:32:20,715 - cloned_fsc_actor_policy.py:190 - Epoch 46000, Accuracy: 0.9628
2025-03-24 15:32:24,428 - cloned_fsc_actor_policy.py:189 - Epoch 47000, Loss: 0.0703
2025-03-24 15:32:24,428 - cloned_fsc_actor_policy.py:190 - Epoch 47000, Accuracy: 0.9630
2025-03-24 15:32:28,072 - cloned_fsc_actor_policy.py:189 - Epoch 48000, Loss: 0.0726
2025-03-24 15:32:28,072 - cloned_fsc_actor_policy.py:190 - Epoch 48000, Accuracy: 0.9620
2025-03-24 15:32:31,719 - cloned_fsc_actor_policy.py:189 - Epoch 49000, Loss: 0.0705
2025-03-24 15:32:31,719 - cloned_fsc_actor_policy.py:190 - Epoch 49000, Accuracy: 0.9627
2025-03-24 15:32:35,636 - cloned_fsc_actor_policy.py:189 - Epoch 50000, Loss: 0.0698
2025-03-24 15:32:35,637 - cloned_fsc_actor_policy.py:190 - Epoch 50000, Accuracy: 0.9633
2025-03-24 15:32:35,667 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:32:51,333 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:32:51,333 - evaluators.py:130 - Average Virtual Goal Value = 5.83845329284668
2025-03-24 15:32:51,333 - evaluators.py:132 - Goal Reach Probability = 0.11676906260600807
2025-03-24 15:32:51,333 - evaluators.py:134 - Trap Reach Probability = 0.8832309373939919
2025-03-24 15:32:51,333 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:32:51,333 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:32:51,333 - evaluators.py:140 - Current Best Reach Probability = 0.14111371224622943
2025-03-24 15:32:54,909 - cloned_fsc_actor_policy.py:189 - Epoch 51000, Loss: 0.0705
2025-03-24 15:32:54,909 - cloned_fsc_actor_policy.py:190 - Epoch 51000, Accuracy: 0.9626
2025-03-24 15:32:58,434 - cloned_fsc_actor_policy.py:189 - Epoch 52000, Loss: 0.0704
2025-03-24 15:32:58,434 - cloned_fsc_actor_policy.py:190 - Epoch 52000, Accuracy: 0.9629
2025-03-24 15:33:02,043 - cloned_fsc_actor_policy.py:189 - Epoch 53000, Loss: 0.0710
2025-03-24 15:33:02,044 - cloned_fsc_actor_policy.py:190 - Epoch 53000, Accuracy: 0.9631
2025-03-24 15:33:06,081 - cloned_fsc_actor_policy.py:189 - Epoch 54000, Loss: 0.0682
2025-03-24 15:33:06,082 - cloned_fsc_actor_policy.py:190 - Epoch 54000, Accuracy: 0.9636
2025-03-24 15:33:09,743 - cloned_fsc_actor_policy.py:189 - Epoch 55000, Loss: 0.0694
2025-03-24 15:33:09,743 - cloned_fsc_actor_policy.py:190 - Epoch 55000, Accuracy: 0.9632
2025-03-24 15:33:09,777 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:33:25,466 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:33:25,467 - evaluators.py:130 - Average Virtual Goal Value = 5.200119972229004
2025-03-24 15:33:25,467 - evaluators.py:132 - Goal Reach Probability = 0.10400240294360592
2025-03-24 15:33:25,467 - evaluators.py:134 - Trap Reach Probability = 0.895997597056394
2025-03-24 15:33:25,467 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:33:25,467 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:33:25,467 - evaluators.py:140 - Current Best Reach Probability = 0.14111371224622943
2025-03-24 15:33:29,173 - cloned_fsc_actor_policy.py:189 - Epoch 56000, Loss: 0.0687
2025-03-24 15:33:29,173 - cloned_fsc_actor_policy.py:190 - Epoch 56000, Accuracy: 0.9625
2025-03-24 15:33:33,004 - cloned_fsc_actor_policy.py:189 - Epoch 57000, Loss: 0.0691
2025-03-24 15:33:33,004 - cloned_fsc_actor_policy.py:190 - Epoch 57000, Accuracy: 0.9629
2025-03-24 15:33:36,842 - cloned_fsc_actor_policy.py:189 - Epoch 58000, Loss: 0.0687
2025-03-24 15:33:36,842 - cloned_fsc_actor_policy.py:190 - Epoch 58000, Accuracy: 0.9631
2025-03-24 15:33:40,413 - cloned_fsc_actor_policy.py:189 - Epoch 59000, Loss: 0.0686
2025-03-24 15:33:40,413 - cloned_fsc_actor_policy.py:190 - Epoch 59000, Accuracy: 0.9625
2025-03-24 15:33:43,974 - cloned_fsc_actor_policy.py:189 - Epoch 60000, Loss: 0.0693
2025-03-24 15:33:43,974 - cloned_fsc_actor_policy.py:190 - Epoch 60000, Accuracy: 0.9630
2025-03-24 15:33:44,009 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:33:59,605 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:33:59,605 - evaluators.py:130 - Average Virtual Goal Value = 5.155992031097412
2025-03-24 15:33:59,605 - evaluators.py:132 - Goal Reach Probability = 0.10311984400779961
2025-03-24 15:33:59,605 - evaluators.py:134 - Trap Reach Probability = 0.8968801559922004
2025-03-24 15:33:59,605 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:33:59,605 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:33:59,605 - evaluators.py:140 - Current Best Reach Probability = 0.14111371224622943
2025-03-24 15:33:59,613 - attrs.py:78 - Creating converter from 3 to 5
2025-03-24 15:34:00,516 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:34:14,691 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:34:14,691 - evaluators.py:130 - Average Virtual Goal Value = 6.895628452301025
2025-03-24 15:34:14,691 - evaluators.py:132 - Goal Reach Probability = 0.13791257123188128
2025-03-24 15:34:14,691 - evaluators.py:134 - Trap Reach Probability = 0.8620874287681187
2025-03-24 15:34:14,691 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:34:14,691 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:34:14,691 - evaluators.py:140 - Current Best Reach Probability = 0.13791257123188128
2025-03-24 15:34:14,693 - pomdp.py:349 - unfolding 5-FSC template into POMDP...
2025-03-24 15:34:15,701 - pomdp.py:355 - constructed quotient MDP having 33898 states and 617106 actions.
2025-03-24 15:34:17,357 - rl_family_extractor.py:329 - Building family from FFNN...
2025-03-24 15:34:17,449 - rl_family_extractor.py:357 - Number of misses: 1 out of 1000
2025-03-24 15:34:17,449 - rl_family_extractor.py:359 - Number of complete misses: 1 out of 1000
2025-03-24 15:34:17,449 - statistic.py:67 - synthesis initiated, design space: 4
2025-03-24 15:34:17,503 - synthesizer.py:192 - printing synthesized assignment below:
2025-03-24 15:34:17,503 - synthesizer.py:193 - A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=3, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=3, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=3, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=3, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=3, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=3, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=3, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=3, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=2, M([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=1, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=1, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=3, A([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=3, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=3, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=3, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=3, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=3, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=3, A([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=10	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=4, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=4, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=4, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=4, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=4, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=4, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=3, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=2, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=4, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=4, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=4, A([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=1, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=1, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=4, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=2, M([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=3, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=11	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=1, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=3, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=4, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=4, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=4, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=3, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=3, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=14	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=4, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=4, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=2, M([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=12	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, A([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=1, M([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=1	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=11	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=13	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=4, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=4, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=4, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=0, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=0	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=0	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=1, M([start	& fuel=0	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=0	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=0	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=2, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=14	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=10	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=east, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=east, A([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=9	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=19	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=13	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=east, A([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=9	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, A([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=3	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, A([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=6	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, A([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=6	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=2, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=0	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=15	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=4	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=3, A([start	& fuel=2	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=east, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=south, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=south, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=south, A([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=south, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=17	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=0	& hascrash=0	& refuelAllowed=0],4)=0, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=east, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=east, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=east, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=east, A([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=east, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],1)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],2)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],3)=0, M([start	& fuel=18	& amdone=0	& cangoeast=1	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],4)=0, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],0)=2, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],1)=2, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],2)=2, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],3)=2, M([start	& fuel=16	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=1],4)=2, A([start	& fuel=15	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=15	& amdone=0	& cangoeast=0	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=0, A([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=8	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, A([start	& fuel=12	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=east, A([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=8	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, A([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=3	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, A([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=2	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, A([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=5	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, A([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=7	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, A([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=south, M([start	& fuel=7	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=0	& cangowest=0	& hascrash=0	& refuelAllowed=0],0)=4, A([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=5	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1, A([start	& fuel=1	& amdone=0	& cangoeast=1	& cangonorth=1	& cangosouth=1	& cangowest=1	& hascrash=1	& refuelAllowed=0],0)=south, A([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=east, M([start	& fuel=4	& amdone=0	& cangoeast=0	& cangonorth=0	& cangosouth=1	& cangowest=1	& hascrash=0	& refuelAllowed=0],0)=1
2025-03-24 15:34:17,513 - synthesizer.py:198 - double-checking specification satisfiability:  : 0.14048203124848768
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 0.05 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 100 %
DTMC stats: avg DTMC size: 1322, iterations: 4

optimum: 0.140482
--------------------
2025-03-24 15:34:17,514 - statistic.py:67 - synthesis initiated, design space: 1e609
> progress 0.0%, elapsed 3 s, estimated 3004739 s (34 days), iters = {DTMC: 389}, opt = 0.1405
> progress 0.0%, elapsed 6 s, estimated 6010824 s (69 days), iters = {DTMC: 781}, opt = 0.1405
> progress 0.0%, elapsed 9 s, estimated 9013314 s (104 days), iters = {DTMC: 1159}, opt = 0.1405
> progress 0.0%, elapsed 12 s, estimated 12018557 s (139 days), iters = {DTMC: 1541}, opt = 0.1405
> progress 0.0%, elapsed 15 s, estimated 15024852 s (173 days), iters = {DTMC: 1928}, opt = 0.1405
> progress 0.0%, elapsed 18 s, estimated 18025600 s (208 days), iters = {DTMC: 2310}, opt = 0.1405
> progress 0.0%, elapsed 21 s, estimated 21032003 s (243 days), iters = {DTMC: 2679}, opt = 0.1405
> progress 0.0%, elapsed 24 s, estimated 24032640 s (278 days), iters = {DTMC: 3045}, opt = 0.1405
> progress 0.0%, elapsed 27 s, estimated 27040034 s (312 days), iters = {DTMC: 3410}, opt = 0.1405
> progress 0.0%, elapsed 30 s, estimated 30045777 s (347 days), iters = {DTMC: 3783}, opt = 0.1405
> progress 0.0%, elapsed 33 s, estimated 33048775 s (1 year), iters = {DTMC: 4175}, opt = 0.1405
> progress 0.0%, elapsed 36 s, estimated 36055818 s (1 year), iters = {DTMC: 4578}, opt = 0.1405
> progress 0.0%, elapsed 39 s, estimated 39060875 s (1 year), iters = {DTMC: 4977}, opt = 0.1405
> progress 0.0%, elapsed 42 s, estimated 42066748 s (1 year), iters = {DTMC: 5383}, opt = 0.1405
> progress 0.0%, elapsed 45 s, estimated 45069438 s (1 year), iters = {DTMC: 5788}, opt = 0.1405
> progress 0.0%, elapsed 48 s, estimated 48074308 s (1 year), iters = {DTMC: 6194}, opt = 0.1405
> progress 0.0%, elapsed 51 s, estimated 51074621 s (1 year), iters = {DTMC: 6575}, opt = 0.1405
> progress 0.0%, elapsed 54 s, estimated 54081767 s (1 year), iters = {DTMC: 6982}, opt = 0.1405
> progress 0.0%, elapsed 57 s, estimated 57082896 s (1 year), iters = {DTMC: 7372}, opt = 0.1405
2025-03-24 15:35:17,520 - synthesizer_onebyone.py:19 - Time limit reached
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.01 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 0 %
DTMC stats: avg DTMC size: 212, iterations: 7703

optimum: 0.140482
--------------------
2025-03-24 15:35:17,520 - synthesizer_rl_storm_paynt.py:280 - No improving assignment found.
2025-03-24 15:35:18,098 - storm_pomdp_control.py:246 - Interactive Storm resumed
2025-03-24 15:35:18,098 - storm_pomdp_control.py:291 - Updating FSC values in Storm
2025-03-24 15:35:49,112 - storm_pomdp_control.py:305 - Pausing Storm
-----------Storm-----------               
Value = 0.23461992224252154 | Time elapsed = 1553.3s | FSC size = 1636

2025-03-24 15:36:24,296 - synthesizer_pomdp.py:253 - Timeout for PAYNT started
2025-03-24 15:36:24,693 - synthesizer_ar_storm.py:136 - Resuming synthesis
2025-03-24 15:36:24,694 - synthesizer_ar_storm.py:147 - PAYNT's value is better. Prioritizing synthesis results
2025-03-24 15:36:54,466 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 15:36:54,537 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:37:03,038 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:37:03,038 - father_agent.py:564 - Average Virtual Goal Value = 1.8853164911270142
2025-03-24 15:37:03,038 - father_agent.py:566 - Goal Reach Probability = 0.037706328717828363
2025-03-24 15:37:03,038 - father_agent.py:568 - Trap Reach Probability = 0.9622936712821716
2025-03-24 15:37:03,038 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:37:03,038 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:37:03,038 - father_agent.py:574 - Current Best Reach Probability = 0.19077175313268532
2025-03-24 15:37:05,376 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:37:08,913 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:37:08,913 - evaluators.py:130 - Average Virtual Goal Value = 0.4584151804447174
2025-03-24 15:37:08,913 - evaluators.py:132 - Goal Reach Probability = 0.0091683038637852
2025-03-24 15:37:08,914 - evaluators.py:134 - Trap Reach Probability = 0.9908316961362148
2025-03-24 15:37:08,914 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:37:08,914 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:37:08,914 - evaluators.py:140 - Current Best Reach Probability = 0.19752186588921283
2025-03-24 15:37:08,914 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 1.6079766750335693
Epoch: 0, Critic Loss: 7.507443428039551
Epoch: 10, Actor Loss: 0.5735230445861816
Epoch: 10, Critic Loss: 8.488554954528809
Epoch: 20, Actor Loss: 0.34171509742736816
Epoch: 20, Critic Loss: 11.304956436157227
Epoch: 30, Actor Loss: 0.2708735167980194
Epoch: 30, Critic Loss: 17.994243621826172
Epoch: 40, Actor Loss: 0.19669660925865173
Epoch: 40, Critic Loss: 10.50168514251709
2025-03-24 15:37:43,577 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:37:47,096 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:37:47,097 - evaluators.py:130 - Average Virtual Goal Value = 5.785782814025879
2025-03-24 15:37:47,097 - evaluators.py:132 - Goal Reach Probability = 0.1157156518967166
2025-03-24 15:37:47,097 - evaluators.py:134 - Trap Reach Probability = 0.8842843481032834
2025-03-24 15:37:47,097 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:37:47,097 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:37:47,097 - evaluators.py:140 - Current Best Reach Probability = 0.19752186588921283
2025-03-24 15:37:47,097 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.14362820982933044
Epoch: 50, Critic Loss: 6.633063316345215
Epoch: 60, Actor Loss: 0.1303909420967102
Epoch: 60, Critic Loss: 9.029075622558594
Epoch: 70, Actor Loss: 0.09824424982070923
Epoch: 70, Critic Loss: 12.814257621765137
Epoch: 80, Actor Loss: 0.09975574165582657
Epoch: 80, Critic Loss: 12.035379409790039
Epoch: 90, Actor Loss: 0.09057212620973587
Epoch: 90, Critic Loss: 5.801657676696777
2025-03-24 15:38:21,345 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:38:24,496 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:38:24,497 - evaluators.py:130 - Average Virtual Goal Value = 8.716094017028809
2025-03-24 15:38:24,497 - evaluators.py:132 - Goal Reach Probability = 0.17432188065099458
2025-03-24 15:38:24,497 - evaluators.py:134 - Trap Reach Probability = 0.8256781193490055
2025-03-24 15:38:24,497 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:38:24,497 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:38:24,497 - evaluators.py:140 - Current Best Reach Probability = 0.19752186588921283
2025-03-24 15:38:24,497 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.07859668880701065
Epoch: 100, Critic Loss: 12.05129337310791
Epoch: 110, Actor Loss: 0.07786310464143753
Epoch: 110, Critic Loss: 17.955440521240234
Epoch: 120, Actor Loss: 0.057625580579042435
Epoch: 120, Critic Loss: 16.776046752929688
Epoch: 130, Actor Loss: 0.061807747930288315
Epoch: 130, Critic Loss: 10.505640029907227
Epoch: 140, Actor Loss: 0.06525125354528427
Epoch: 140, Critic Loss: 14.102027893066406
2025-03-24 15:38:59,514 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:39:02,941 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:39:02,941 - evaluators.py:130 - Average Virtual Goal Value = 10.12281322479248
2025-03-24 15:39:02,941 - evaluators.py:132 - Goal Reach Probability = 0.20245627093412727
2025-03-24 15:39:02,941 - evaluators.py:134 - Trap Reach Probability = 0.7975437290658727
2025-03-24 15:39:02,941 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:39:02,941 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:39:02,941 - evaluators.py:140 - Current Best Reach Probability = 0.20245627093412727
2025-03-24 15:39:02,942 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.062463436275720596
Epoch: 150, Critic Loss: 8.796431541442871
Epoch: 160, Actor Loss: 0.05264538899064064
Epoch: 160, Critic Loss: 4.57941198348999
Epoch: 170, Actor Loss: 0.05808599293231964
Epoch: 170, Critic Loss: 15.414636611938477
Epoch: 180, Actor Loss: 0.05741250142455101
Epoch: 180, Critic Loss: 14.59394359588623
Epoch: 190, Actor Loss: 0.045642729848623276
Epoch: 190, Critic Loss: 14.631452560424805
2025-03-24 15:39:37,436 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:39:40,636 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:39:40,637 - evaluators.py:130 - Average Virtual Goal Value = 9.398906707763672
2025-03-24 15:39:40,637 - evaluators.py:132 - Goal Reach Probability = 0.18797814207650274
2025-03-24 15:39:40,637 - evaluators.py:134 - Trap Reach Probability = 0.8120218579234972
2025-03-24 15:39:40,637 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:39:40,637 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:39:40,637 - evaluators.py:140 - Current Best Reach Probability = 0.20245627093412727
2025-03-24 15:39:40,637 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.03425656259059906
Epoch: 200, Critic Loss: 13.244159698486328
Epoch: 210, Actor Loss: 0.030196456238627434
Epoch: 210, Critic Loss: 13.231955528259277
Epoch: 220, Actor Loss: 0.028075726702809334
Epoch: 220, Critic Loss: 12.293573379516602
Epoch: 230, Actor Loss: 0.04998096823692322
Epoch: 230, Critic Loss: 12.34994888305664
Epoch: 240, Actor Loss: 0.028453145176172256
Epoch: 240, Critic Loss: 8.398889541625977
2025-03-24 15:40:14,256 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:40:17,571 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:40:17,571 - evaluators.py:130 - Average Virtual Goal Value = 9.267319679260254
2025-03-24 15:40:17,571 - evaluators.py:132 - Goal Reach Probability = 0.18534639100471528
2025-03-24 15:40:17,571 - evaluators.py:134 - Trap Reach Probability = 0.8146536089952847
2025-03-24 15:40:17,571 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:40:17,571 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:40:17,571 - evaluators.py:140 - Current Best Reach Probability = 0.20245627093412727
2025-03-24 15:40:17,571 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.03386703506112099
Epoch: 250, Critic Loss: 8.778287887573242
Epoch: 260, Actor Loss: 0.033014215528964996
Epoch: 260, Critic Loss: 12.940706253051758
Epoch: 270, Actor Loss: 0.02319103293120861
Epoch: 270, Critic Loss: 14.535537719726562
Epoch: 280, Actor Loss: 0.02303648553788662
Epoch: 280, Critic Loss: 9.62894058227539
Epoch: 290, Actor Loss: 0.04234139993786812
Epoch: 290, Critic Loss: 13.764257431030273
2025-03-24 15:40:54,273 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:40:57,321 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:40:57,321 - evaluators.py:130 - Average Virtual Goal Value = 9.104151725769043
2025-03-24 15:40:57,321 - evaluators.py:132 - Goal Reach Probability = 0.1820830298616169
2025-03-24 15:40:57,321 - evaluators.py:134 - Trap Reach Probability = 0.817916970138383
2025-03-24 15:40:57,321 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:40:57,321 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:40:57,321 - evaluators.py:140 - Current Best Reach Probability = 0.20245627093412727
2025-03-24 15:40:57,321 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.018148284405469894
Epoch: 300, Critic Loss: 13.631006240844727
Epoch: 310, Actor Loss: 0.03259781375527382
Epoch: 310, Critic Loss: 9.143033981323242
Epoch: 320, Actor Loss: 0.04369538277387619
Epoch: 320, Critic Loss: 5.969420433044434
Epoch: 330, Actor Loss: 0.02571036107838154
Epoch: 330, Critic Loss: 14.8900146484375
Epoch: 340, Actor Loss: 0.03314545378088951
Epoch: 340, Critic Loss: 10.00709342956543
2025-03-24 15:41:31,375 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:41:35,013 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:41:35,013 - evaluators.py:130 - Average Virtual Goal Value = 9.78660774230957
2025-03-24 15:41:35,013 - evaluators.py:132 - Goal Reach Probability = 0.19573215599705665
2025-03-24 15:41:35,013 - evaluators.py:134 - Trap Reach Probability = 0.8042678440029434
2025-03-24 15:41:35,013 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:41:35,013 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:41:35,013 - evaluators.py:140 - Current Best Reach Probability = 0.20245627093412727
2025-03-24 15:41:35,014 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.033987440168857574
Epoch: 350, Critic Loss: 9.857901573181152
Epoch: 360, Actor Loss: 0.037419725209474564
Epoch: 360, Critic Loss: 9.721006393432617
Epoch: 370, Actor Loss: 0.020188095048069954
Epoch: 370, Critic Loss: 10.57686710357666
Epoch: 380, Actor Loss: 0.019502654671669006
Epoch: 380, Critic Loss: 11.043755531311035
Epoch: 390, Actor Loss: 0.023021768778562546
Epoch: 390, Critic Loss: 15.549614906311035
2025-03-24 15:42:09,008 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:42:12,061 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:42:12,062 - evaluators.py:130 - Average Virtual Goal Value = 10.415122032165527
2025-03-24 15:42:12,062 - evaluators.py:132 - Goal Reach Probability = 0.20830244625648628
2025-03-24 15:42:12,062 - evaluators.py:134 - Trap Reach Probability = 0.7916975537435137
2025-03-24 15:42:12,062 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:42:12,062 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:42:12,062 - evaluators.py:140 - Current Best Reach Probability = 0.20830244625648628
2025-03-24 15:42:12,062 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.023648979142308235
Epoch: 400, Critic Loss: 11.466412544250488
Epoch: 410, Actor Loss: 0.02904537506401539
Epoch: 410, Critic Loss: 15.113724708557129
Epoch: 420, Actor Loss: 0.021350307390093803
Epoch: 420, Critic Loss: 9.48924446105957
Epoch: 430, Actor Loss: 0.025809746235609055
Epoch: 430, Critic Loss: 10.155115127563477
Epoch: 440, Actor Loss: 0.0258177537471056
Epoch: 440, Critic Loss: 10.51375961303711
2025-03-24 15:42:44,660 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:42:47,579 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:42:47,579 - evaluators.py:130 - Average Virtual Goal Value = 9.970070838928223
2025-03-24 15:42:47,579 - evaluators.py:132 - Goal Reach Probability = 0.19940142162364385
2025-03-24 15:42:47,579 - evaluators.py:134 - Trap Reach Probability = 0.8005985783763562
2025-03-24 15:42:47,579 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:42:47,579 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:42:47,579 - evaluators.py:140 - Current Best Reach Probability = 0.20830244625648628
2025-03-24 15:42:47,579 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.03760712221264839
Epoch: 450, Critic Loss: 10.303998947143555
Epoch: 460, Actor Loss: 0.0341743603348732
Epoch: 460, Critic Loss: 10.164715766906738
Epoch: 470, Actor Loss: 0.031041381880640984
Epoch: 470, Critic Loss: 8.958322525024414
Epoch: 480, Actor Loss: 0.02315681427717209
Epoch: 480, Critic Loss: 11.023707389831543
Epoch: 490, Actor Loss: 0.016979390755295753
Epoch: 490, Critic Loss: 10.53211784362793
2025-03-24 15:43:19,806 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:43:23,057 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:43:23,058 - evaluators.py:130 - Average Virtual Goal Value = 9.789947509765625
2025-03-24 15:43:23,058 - evaluators.py:132 - Goal Reach Probability = 0.19579894973743436
2025-03-24 15:43:23,058 - evaluators.py:134 - Trap Reach Probability = 0.8042010502625656
2025-03-24 15:43:23,058 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:43:23,058 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:43:23,058 - evaluators.py:140 - Current Best Reach Probability = 0.20830244625648628
2025-03-24 15:43:23,058 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.02462497539818287
Epoch: 500, Critic Loss: 11.217599868774414
Epoch: 510, Actor Loss: 0.027966246008872986
Epoch: 510, Critic Loss: 11.250282287597656
Epoch: 520, Actor Loss: 0.027003077790141106
Epoch: 520, Critic Loss: 22.714475631713867
Epoch: 530, Actor Loss: 0.03438207507133484
Epoch: 530, Critic Loss: 6.162081718444824
Epoch: 540, Actor Loss: 0.021774180233478546
Epoch: 540, Critic Loss: 10.939478874206543
2025-03-24 15:43:56,361 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:43:59,432 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:43:59,432 - evaluators.py:130 - Average Virtual Goal Value = 10.272961616516113
2025-03-24 15:43:59,432 - evaluators.py:132 - Goal Reach Probability = 0.20545924013279232
2025-03-24 15:43:59,432 - evaluators.py:134 - Trap Reach Probability = 0.7945407598672076
2025-03-24 15:43:59,432 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:43:59,432 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:43:59,432 - evaluators.py:140 - Current Best Reach Probability = 0.20830244625648628
2025-03-24 15:43:59,432 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.03247220814228058
Epoch: 550, Critic Loss: 11.743674278259277
Epoch: 560, Actor Loss: 0.02097863145172596
Epoch: 560, Critic Loss: 14.485616683959961
Epoch: 570, Actor Loss: 0.02864736132323742
Epoch: 570, Critic Loss: 10.474597930908203
Epoch: 580, Actor Loss: 0.026322850957512856
Epoch: 580, Critic Loss: 14.683868408203125
Epoch: 590, Actor Loss: 0.02573798969388008
Epoch: 590, Critic Loss: 11.035144805908203
2025-03-24 15:44:32,552 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:44:35,572 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:44:35,572 - evaluators.py:130 - Average Virtual Goal Value = 10.344175338745117
2025-03-24 15:44:35,572 - evaluators.py:132 - Goal Reach Probability = 0.20688350983358547
2025-03-24 15:44:35,572 - evaluators.py:134 - Trap Reach Probability = 0.7931164901664145
2025-03-24 15:44:35,572 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:44:35,573 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:44:35,573 - evaluators.py:140 - Current Best Reach Probability = 0.20830244625648628
2025-03-24 15:44:35,573 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.01780065707862377
Epoch: 600, Critic Loss: 11.314043045043945
Epoch: 610, Actor Loss: 0.025391487404704094
Epoch: 610, Critic Loss: 9.344400405883789
Epoch: 620, Actor Loss: 0.03817639499902725
Epoch: 620, Critic Loss: 14.313461303710938
Epoch: 630, Actor Loss: 0.03505890816450119
Epoch: 630, Critic Loss: 12.16290283203125
Epoch: 640, Actor Loss: 0.0221466775983572
Epoch: 640, Critic Loss: 14.384533882141113
2025-03-24 15:45:10,327 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:45:13,895 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:45:13,895 - evaluators.py:130 - Average Virtual Goal Value = 10.671418190002441
2025-03-24 15:45:13,895 - evaluators.py:132 - Goal Reach Probability = 0.21342835708927232
2025-03-24 15:45:13,895 - evaluators.py:134 - Trap Reach Probability = 0.7865716429107277
2025-03-24 15:45:13,895 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:45:13,895 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:45:13,895 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 15:45:13,895 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.021117685362696648
Epoch: 650, Critic Loss: 12.65275764465332
Epoch: 660, Actor Loss: 0.03418436646461487
Epoch: 660, Critic Loss: 10.102302551269531
Epoch: 670, Actor Loss: 0.02435213141143322
Epoch: 670, Critic Loss: 9.31708812713623
Epoch: 680, Actor Loss: 0.03446662425994873
Epoch: 680, Critic Loss: 17.346162796020508
Epoch: 690, Actor Loss: 0.02341318689286709
Epoch: 690, Critic Loss: 12.431671142578125
2025-03-24 15:45:48,283 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:45:51,613 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:45:51,613 - evaluators.py:130 - Average Virtual Goal Value = 9.776951789855957
2025-03-24 15:45:51,613 - evaluators.py:132 - Goal Reach Probability = 0.19553903345724907
2025-03-24 15:45:51,613 - evaluators.py:134 - Trap Reach Probability = 0.8044609665427509
2025-03-24 15:45:51,613 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:45:51,613 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:45:51,613 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 15:45:51,614 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.021713418886065483
Epoch: 700, Critic Loss: 4.650979042053223
2025-03-24 15:45:51,616 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 15:45:51,616 - father_agent.py:449 - Before training evaluation.
2025-03-24 15:45:51,616 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:45:58,828 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:45:58,828 - father_agent.py:564 - Average Virtual Goal Value = 10.41590690612793
2025-03-24 15:45:58,828 - father_agent.py:566 - Goal Reach Probability = 0.2083181320685881
2025-03-24 15:45:58,828 - father_agent.py:568 - Trap Reach Probability = 0.7916818679314119
2025-03-24 15:45:58,828 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:45:58,828 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:45:58,828 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:45:58,864 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:45:58,958 - father_agent.py:359 - Training agent on-policy
2025-03-24 15:46:02,511 - father_agent.py:306 - Step: 0, Training loss: 86.65226745605469
2025-03-24 15:46:02,548 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:46:10,519 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:46:10,519 - father_agent.py:564 - Average Virtual Goal Value = 9.759723663330078
2025-03-24 15:46:10,519 - father_agent.py:566 - Goal Reach Probability = 0.19519448078981802
2025-03-24 15:46:10,519 - father_agent.py:568 - Trap Reach Probability = 0.804805519210182
2025-03-24 15:46:10,519 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:46:10,519 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:46:10,519 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:46:10,551 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:46:12,579 - father_agent.py:306 - Step: 5, Training loss: 54.79087829589844
2025-03-24 15:46:14,040 - father_agent.py:306 - Step: 10, Training loss: 13.57603931427002
2025-03-24 15:46:15,287 - father_agent.py:306 - Step: 15, Training loss: 4.124535083770752
2025-03-24 15:46:16,532 - father_agent.py:306 - Step: 20, Training loss: 1.6043940782546997
2025-03-24 15:46:17,669 - father_agent.py:306 - Step: 25, Training loss: 0.9400182962417603
2025-03-24 15:46:22,561 - father_agent.py:306 - Step: 30, Training loss: 0.6798598766326904
2025-03-24 15:46:23,786 - father_agent.py:306 - Step: 35, Training loss: 1.0054631233215332
2025-03-24 15:46:24,985 - father_agent.py:306 - Step: 40, Training loss: 1.2172075510025024
2025-03-24 15:46:26,217 - father_agent.py:306 - Step: 45, Training loss: 1.4913749694824219
2025-03-24 15:46:27,446 - father_agent.py:306 - Step: 50, Training loss: 1.1714143753051758
2025-03-24 15:46:28,645 - father_agent.py:306 - Step: 55, Training loss: 1.2631990909576416
2025-03-24 15:46:29,834 - father_agent.py:306 - Step: 60, Training loss: 1.4842162132263184
2025-03-24 15:46:31,130 - father_agent.py:306 - Step: 65, Training loss: 1.0475857257843018
2025-03-24 15:46:32,474 - father_agent.py:306 - Step: 70, Training loss: 1.020269751548767
2025-03-24 15:46:33,626 - father_agent.py:306 - Step: 75, Training loss: 1.126533031463623
2025-03-24 15:46:34,852 - father_agent.py:306 - Step: 80, Training loss: 1.6015139818191528
2025-03-24 15:46:36,067 - father_agent.py:306 - Step: 85, Training loss: 1.4091742038726807
2025-03-24 15:46:37,213 - father_agent.py:306 - Step: 90, Training loss: 1.3245350122451782
2025-03-24 15:46:38,456 - father_agent.py:306 - Step: 95, Training loss: 1.1519256830215454
2025-03-24 15:46:39,656 - father_agent.py:306 - Step: 100, Training loss: 1.2807482481002808
2025-03-24 15:46:39,691 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:46:47,994 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:46:47,994 - father_agent.py:564 - Average Virtual Goal Value = 4.525605201721191
2025-03-24 15:46:47,994 - father_agent.py:566 - Goal Reach Probability = 0.09051210797935688
2025-03-24 15:46:47,994 - father_agent.py:568 - Trap Reach Probability = 0.9094878920206431
2025-03-24 15:46:47,994 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:46:47,994 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:46:47,994 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:46:48,261 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:46:50,090 - father_agent.py:306 - Step: 105, Training loss: 1.3234648704528809
2025-03-24 15:46:51,252 - father_agent.py:306 - Step: 110, Training loss: 1.1675822734832764
2025-03-24 15:46:52,495 - father_agent.py:306 - Step: 115, Training loss: 1.3041349649429321
2025-03-24 15:46:53,672 - father_agent.py:306 - Step: 120, Training loss: 1.3299345970153809
2025-03-24 15:46:54,870 - father_agent.py:306 - Step: 125, Training loss: 1.355785608291626
2025-03-24 15:46:56,016 - father_agent.py:306 - Step: 130, Training loss: 1.0534021854400635
2025-03-24 15:46:57,257 - father_agent.py:306 - Step: 135, Training loss: 1.433779001235962
2025-03-24 15:46:58,555 - father_agent.py:306 - Step: 140, Training loss: 1.5510690212249756
2025-03-24 15:46:59,836 - father_agent.py:306 - Step: 145, Training loss: 1.437225103378296
2025-03-24 15:47:01,318 - father_agent.py:306 - Step: 150, Training loss: 1.5880091190338135
2025-03-24 15:47:02,592 - father_agent.py:306 - Step: 155, Training loss: 1.3724921941757202
2025-03-24 15:47:03,872 - father_agent.py:306 - Step: 160, Training loss: 1.9588514566421509
2025-03-24 15:47:05,303 - father_agent.py:306 - Step: 165, Training loss: 1.3663382530212402
2025-03-24 15:47:06,747 - father_agent.py:306 - Step: 170, Training loss: 1.6789497137069702
2025-03-24 15:47:07,851 - father_agent.py:306 - Step: 175, Training loss: 1.6190768480300903
2025-03-24 15:47:09,283 - father_agent.py:306 - Step: 180, Training loss: 1.41689133644104
2025-03-24 15:47:10,418 - father_agent.py:306 - Step: 185, Training loss: 1.2974838018417358
2025-03-24 15:47:11,535 - father_agent.py:306 - Step: 190, Training loss: 1.5921396017074585
2025-03-24 15:47:12,757 - father_agent.py:306 - Step: 195, Training loss: 1.4924688339233398
2025-03-24 15:47:13,861 - father_agent.py:306 - Step: 200, Training loss: 1.4066908359527588
2025-03-24 15:47:13,895 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:47:22,177 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:47:22,177 - father_agent.py:564 - Average Virtual Goal Value = 6.4880595207214355
2025-03-24 15:47:22,177 - father_agent.py:566 - Goal Reach Probability = 0.12976119147479243
2025-03-24 15:47:22,177 - father_agent.py:568 - Trap Reach Probability = 0.8702388085252075
2025-03-24 15:47:22,177 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:47:22,177 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:47:22,177 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:47:22,274 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:47:24,084 - father_agent.py:306 - Step: 205, Training loss: 1.3564813137054443
2025-03-24 15:47:25,132 - father_agent.py:306 - Step: 210, Training loss: 1.7782251834869385
2025-03-24 15:47:26,311 - father_agent.py:306 - Step: 215, Training loss: 1.5439021587371826
2025-03-24 15:47:27,443 - father_agent.py:306 - Step: 220, Training loss: 1.3010790348052979
2025-03-24 15:47:28,660 - father_agent.py:306 - Step: 225, Training loss: 1.4908603429794312
2025-03-24 15:47:29,787 - father_agent.py:306 - Step: 230, Training loss: 1.1903802156448364
2025-03-24 15:47:30,891 - father_agent.py:306 - Step: 235, Training loss: 1.4198224544525146
2025-03-24 15:47:32,139 - father_agent.py:306 - Step: 240, Training loss: 1.3849966526031494
2025-03-24 15:47:33,275 - father_agent.py:306 - Step: 245, Training loss: 1.2627134323120117
2025-03-24 15:47:34,465 - father_agent.py:306 - Step: 250, Training loss: 1.2561678886413574
2025-03-24 15:47:35,655 - father_agent.py:306 - Step: 255, Training loss: 1.3845322132110596
2025-03-24 15:47:36,756 - father_agent.py:306 - Step: 260, Training loss: 1.8516721725463867
2025-03-24 15:47:37,993 - father_agent.py:306 - Step: 265, Training loss: 1.5128920078277588
2025-03-24 15:47:39,374 - father_agent.py:306 - Step: 270, Training loss: 1.3880919218063354
2025-03-24 15:47:40,686 - father_agent.py:306 - Step: 275, Training loss: 1.5435899496078491
2025-03-24 15:47:41,756 - father_agent.py:306 - Step: 280, Training loss: 1.4742093086242676
2025-03-24 15:47:43,129 - father_agent.py:306 - Step: 285, Training loss: 1.8297759294509888
2025-03-24 15:47:44,480 - father_agent.py:306 - Step: 290, Training loss: 1.4160747528076172
2025-03-24 15:47:45,708 - father_agent.py:306 - Step: 295, Training loss: 1.3349648714065552
2025-03-24 15:47:47,144 - father_agent.py:306 - Step: 300, Training loss: 1.7403948307037354
2025-03-24 15:47:47,190 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:47:55,243 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:47:55,243 - father_agent.py:564 - Average Virtual Goal Value = 7.193962097167969
2025-03-24 15:47:55,243 - father_agent.py:566 - Goal Reach Probability = 0.1438792426489835
2025-03-24 15:47:55,243 - father_agent.py:568 - Trap Reach Probability = 0.8561207573510164
2025-03-24 15:47:55,243 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:47:55,243 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:47:55,243 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:47:55,276 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:47:57,182 - father_agent.py:306 - Step: 305, Training loss: 1.207952857017517
2025-03-24 15:47:58,426 - father_agent.py:306 - Step: 310, Training loss: 1.5004879236221313
2025-03-24 15:47:59,790 - father_agent.py:306 - Step: 315, Training loss: 1.1745423078536987
2025-03-24 15:48:01,269 - father_agent.py:306 - Step: 320, Training loss: 1.5573925971984863
2025-03-24 15:48:02,378 - father_agent.py:306 - Step: 325, Training loss: 1.2266374826431274
2025-03-24 15:48:03,474 - father_agent.py:306 - Step: 330, Training loss: 1.438283920288086
2025-03-24 15:48:04,724 - father_agent.py:306 - Step: 335, Training loss: 1.292275071144104
2025-03-24 15:48:05,919 - father_agent.py:306 - Step: 340, Training loss: 1.3837623596191406
2025-03-24 15:48:07,135 - father_agent.py:306 - Step: 345, Training loss: 1.0290731191635132
2025-03-24 15:48:08,308 - father_agent.py:306 - Step: 350, Training loss: 1.0676515102386475
2025-03-24 15:48:09,480 - father_agent.py:306 - Step: 355, Training loss: 0.9760833978652954
2025-03-24 15:48:10,704 - father_agent.py:306 - Step: 360, Training loss: 1.3001800775527954
2025-03-24 15:48:11,813 - father_agent.py:306 - Step: 365, Training loss: 1.0677833557128906
2025-03-24 15:48:13,513 - father_agent.py:306 - Step: 370, Training loss: 1.469141960144043
2025-03-24 15:48:14,770 - father_agent.py:306 - Step: 375, Training loss: 1.4614657163619995
2025-03-24 15:48:16,000 - father_agent.py:306 - Step: 380, Training loss: 1.3487745523452759
2025-03-24 15:48:17,154 - father_agent.py:306 - Step: 385, Training loss: 1.2727673053741455
2025-03-24 15:48:18,269 - father_agent.py:306 - Step: 390, Training loss: 1.1091246604919434
2025-03-24 15:48:19,350 - father_agent.py:306 - Step: 395, Training loss: 1.264346957206726
2025-03-24 15:48:20,643 - father_agent.py:306 - Step: 400, Training loss: 1.3585021495819092
2025-03-24 15:48:20,678 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:48:30,450 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:48:30,450 - father_agent.py:564 - Average Virtual Goal Value = 4.383242607116699
2025-03-24 15:48:30,450 - father_agent.py:566 - Goal Reach Probability = 0.08766485647788984
2025-03-24 15:48:30,450 - father_agent.py:568 - Trap Reach Probability = 0.9123351435221102
2025-03-24 15:48:30,450 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:48:30,450 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:48:30,450 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:48:30,483 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:48:32,601 - father_agent.py:306 - Step: 405, Training loss: 0.8601336479187012
2025-03-24 15:48:33,844 - father_agent.py:306 - Step: 410, Training loss: 0.9360164999961853
2025-03-24 15:48:35,051 - father_agent.py:306 - Step: 415, Training loss: 1.2063500881195068
2025-03-24 15:48:36,404 - father_agent.py:306 - Step: 420, Training loss: 0.9560387134552002
2025-03-24 15:48:37,563 - father_agent.py:306 - Step: 425, Training loss: 0.9470398426055908
2025-03-24 15:48:38,878 - father_agent.py:306 - Step: 430, Training loss: 0.7160040140151978
2025-03-24 15:48:40,256 - father_agent.py:306 - Step: 435, Training loss: 0.9341241121292114
2025-03-24 15:48:41,586 - father_agent.py:306 - Step: 440, Training loss: 0.6563881039619446
2025-03-24 15:48:42,774 - father_agent.py:306 - Step: 445, Training loss: 0.86724853515625
2025-03-24 15:48:44,151 - father_agent.py:306 - Step: 450, Training loss: 0.6192018389701843
2025-03-24 15:48:45,565 - father_agent.py:306 - Step: 455, Training loss: 0.634551465511322
2025-03-24 15:48:46,946 - father_agent.py:306 - Step: 460, Training loss: 0.5439876914024353
2025-03-24 15:48:48,258 - father_agent.py:306 - Step: 465, Training loss: 0.9073442816734314
2025-03-24 15:48:49,440 - father_agent.py:306 - Step: 470, Training loss: 0.5694772005081177
2025-03-24 15:48:50,643 - father_agent.py:306 - Step: 475, Training loss: 0.650327205657959
2025-03-24 15:48:51,836 - father_agent.py:306 - Step: 480, Training loss: 0.8980754613876343
2025-03-24 15:48:53,108 - father_agent.py:306 - Step: 485, Training loss: 0.8318512439727783
2025-03-24 15:48:54,372 - father_agent.py:306 - Step: 490, Training loss: 0.5957046151161194
2025-03-24 15:48:55,622 - father_agent.py:306 - Step: 495, Training loss: 0.6167229413986206
2025-03-24 15:48:56,802 - father_agent.py:306 - Step: 500, Training loss: 0.7646229863166809
2025-03-24 15:48:56,840 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:49:05,659 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:49:05,660 - father_agent.py:564 - Average Virtual Goal Value = 2.1009104251861572
2025-03-24 15:49:05,660 - father_agent.py:566 - Goal Reach Probability = 0.04201821067579985
2025-03-24 15:49:05,660 - father_agent.py:568 - Trap Reach Probability = 0.9579817893242002
2025-03-24 15:49:05,660 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:49:05,660 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:49:05,660 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:49:05,693 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:49:07,576 - father_agent.py:306 - Step: 505, Training loss: 0.5160987377166748
2025-03-24 15:49:08,877 - father_agent.py:306 - Step: 510, Training loss: 0.6316953301429749
2025-03-24 15:49:10,129 - father_agent.py:306 - Step: 515, Training loss: 0.5270071029663086
2025-03-24 15:49:11,412 - father_agent.py:306 - Step: 520, Training loss: 0.6060682535171509
2025-03-24 15:49:12,654 - father_agent.py:306 - Step: 525, Training loss: 0.46881940960884094
2025-03-24 15:49:14,017 - father_agent.py:306 - Step: 530, Training loss: 0.557597279548645
2025-03-24 15:49:15,367 - father_agent.py:306 - Step: 535, Training loss: 0.4775148630142212
2025-03-24 15:49:16,681 - father_agent.py:306 - Step: 540, Training loss: 0.5843684673309326
2025-03-24 15:49:18,017 - father_agent.py:306 - Step: 545, Training loss: 0.7286210656166077
2025-03-24 15:49:19,321 - father_agent.py:306 - Step: 550, Training loss: 0.8441999554634094
2025-03-24 15:49:20,636 - father_agent.py:306 - Step: 555, Training loss: 0.6791163086891174
2025-03-24 15:49:21,848 - father_agent.py:306 - Step: 560, Training loss: 0.6331713795661926
2025-03-24 15:49:23,144 - father_agent.py:306 - Step: 565, Training loss: 0.6789506077766418
2025-03-24 15:49:24,395 - father_agent.py:306 - Step: 570, Training loss: 0.5210981965065002
2025-03-24 15:49:25,604 - father_agent.py:306 - Step: 575, Training loss: 0.6720058917999268
2025-03-24 15:49:26,869 - father_agent.py:306 - Step: 580, Training loss: 0.7990819215774536
2025-03-24 15:49:28,128 - father_agent.py:306 - Step: 585, Training loss: 0.8411508798599243
2025-03-24 15:49:29,358 - father_agent.py:306 - Step: 590, Training loss: 0.6767052412033081
2025-03-24 15:49:30,578 - father_agent.py:306 - Step: 595, Training loss: 0.7346107959747314
2025-03-24 15:49:31,773 - father_agent.py:306 - Step: 600, Training loss: 0.6691663265228271
2025-03-24 15:49:31,807 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:49:41,006 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:49:41,006 - father_agent.py:564 - Average Virtual Goal Value = 2.666774034500122
2025-03-24 15:49:41,006 - father_agent.py:566 - Goal Reach Probability = 0.053335481791814375
2025-03-24 15:49:41,006 - father_agent.py:568 - Trap Reach Probability = 0.9466645182081856
2025-03-24 15:49:41,006 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:49:41,006 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:49:41,006 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:49:41,038 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:49:43,023 - father_agent.py:306 - Step: 605, Training loss: 0.7221181392669678
2025-03-24 15:49:44,255 - father_agent.py:306 - Step: 610, Training loss: 0.6643227934837341
2025-03-24 15:49:45,549 - father_agent.py:306 - Step: 615, Training loss: 0.6227272748947144
2025-03-24 15:49:46,823 - father_agent.py:306 - Step: 620, Training loss: 0.6263943314552307
2025-03-24 15:49:48,001 - father_agent.py:306 - Step: 625, Training loss: 0.8218029737472534
2025-03-24 15:49:49,364 - father_agent.py:306 - Step: 630, Training loss: 0.7604627013206482
2025-03-24 15:49:50,910 - father_agent.py:306 - Step: 635, Training loss: 0.5361648201942444
2025-03-24 15:49:52,543 - father_agent.py:306 - Step: 640, Training loss: 0.5799047946929932
2025-03-24 15:49:54,009 - father_agent.py:306 - Step: 645, Training loss: 0.39477232098579407
2025-03-24 15:49:55,339 - father_agent.py:306 - Step: 650, Training loss: 0.536756157875061
2025-03-24 15:49:56,536 - father_agent.py:306 - Step: 655, Training loss: 0.469272643327713
2025-03-24 15:49:57,778 - father_agent.py:306 - Step: 660, Training loss: 0.6191487312316895
2025-03-24 15:49:59,115 - father_agent.py:306 - Step: 665, Training loss: 0.46411123871803284
2025-03-24 15:50:00,437 - father_agent.py:306 - Step: 670, Training loss: 0.5366519689559937
2025-03-24 15:50:01,709 - father_agent.py:306 - Step: 675, Training loss: 0.46699756383895874
2025-03-24 15:50:03,028 - father_agent.py:306 - Step: 680, Training loss: 0.2386036217212677
2025-03-24 15:50:04,286 - father_agent.py:306 - Step: 685, Training loss: 0.35121870040893555
2025-03-24 15:50:05,514 - father_agent.py:306 - Step: 690, Training loss: 0.7221097350120544
2025-03-24 15:50:06,868 - father_agent.py:306 - Step: 695, Training loss: 0.32952505350112915
2025-03-24 15:50:08,186 - father_agent.py:306 - Step: 700, Training loss: 0.6463875770568848
2025-03-24 15:50:08,307 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:50:17,438 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:50:17,438 - father_agent.py:564 - Average Virtual Goal Value = 1.944828748703003
2025-03-24 15:50:17,438 - father_agent.py:566 - Goal Reach Probability = 0.03889657417202376
2025-03-24 15:50:17,438 - father_agent.py:568 - Trap Reach Probability = 0.9611034258279763
2025-03-24 15:50:17,438 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:50:17,438 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:50:17,438 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:50:17,472 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:50:17,562 - father_agent.py:455 - Training finished.
2025-03-24 15:50:17,568 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:50:17,655 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 15:50:26,700 - father_agent.py:562 - Average Return = 0.0
2025-03-24 15:50:26,700 - father_agent.py:564 - Average Virtual Goal Value = 1.9191919565200806
2025-03-24 15:50:26,700 - father_agent.py:566 - Goal Reach Probability = 0.03838383838383838
2025-03-24 15:50:26,700 - father_agent.py:568 - Trap Reach Probability = 0.9616161616161616
2025-03-24 15:50:26,700 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 15:50:26,700 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 15:50:26,700 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 15:50:26,743 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:50:45,125 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:50:45,125 - evaluators.py:130 - Average Virtual Goal Value = 0.8847259283065796
2025-03-24 15:50:45,125 - evaluators.py:132 - Goal Reach Probability = 0.017694518508258598
2025-03-24 15:50:45,125 - evaluators.py:134 - Trap Reach Probability = 0.9823054814917414
2025-03-24 15:50:45,125 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:50:45,125 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:50:45,126 - evaluators.py:140 - Current Best Reach Probability = 0.017694518508258598
2025-03-24 15:50:45,140 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:51:31,472 - cloned_fsc_actor_policy.py:189 - Epoch 0, Loss: 1.9130
2025-03-24 15:51:31,472 - cloned_fsc_actor_policy.py:190 - Epoch 0, Accuracy: 0.0606
2025-03-24 15:51:31,508 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:51:47,783 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:51:47,784 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 15:51:47,784 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 15:51:47,784 - evaluators.py:134 - Trap Reach Probability = 0.341796875
2025-03-24 15:51:47,784 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:51:47,784 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:51:47,784 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 15:51:51,500 - cloned_fsc_actor_policy.py:189 - Epoch 1000, Loss: 1.0733
2025-03-24 15:51:51,501 - cloned_fsc_actor_policy.py:190 - Epoch 1000, Accuracy: 0.5866
2025-03-24 15:51:55,329 - cloned_fsc_actor_policy.py:189 - Epoch 2000, Loss: 0.5040
2025-03-24 15:51:55,330 - cloned_fsc_actor_policy.py:190 - Epoch 2000, Accuracy: 0.7704
2025-03-24 15:51:59,351 - cloned_fsc_actor_policy.py:189 - Epoch 3000, Loss: 0.3392
2025-03-24 15:51:59,352 - cloned_fsc_actor_policy.py:190 - Epoch 3000, Accuracy: 0.8502
2025-03-24 15:52:02,937 - cloned_fsc_actor_policy.py:189 - Epoch 4000, Loss: 0.2501
2025-03-24 15:52:02,937 - cloned_fsc_actor_policy.py:190 - Epoch 4000, Accuracy: 0.8878
2025-03-24 15:52:06,567 - cloned_fsc_actor_policy.py:189 - Epoch 5000, Loss: 0.2240
2025-03-24 15:52:06,567 - cloned_fsc_actor_policy.py:190 - Epoch 5000, Accuracy: 0.8883
2025-03-24 15:52:06,602 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:52:24,442 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:52:24,442 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 15:52:24,442 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 15:52:24,442 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 15:52:24,442 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:52:24,442 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:52:24,442 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 15:52:27,930 - cloned_fsc_actor_policy.py:189 - Epoch 6000, Loss: 0.2081
2025-03-24 15:52:27,930 - cloned_fsc_actor_policy.py:190 - Epoch 6000, Accuracy: 0.8872
2025-03-24 15:52:31,462 - cloned_fsc_actor_policy.py:189 - Epoch 7000, Loss: 0.1934
2025-03-24 15:52:31,462 - cloned_fsc_actor_policy.py:190 - Epoch 7000, Accuracy: 0.8866
2025-03-24 15:52:35,435 - cloned_fsc_actor_policy.py:189 - Epoch 8000, Loss: 0.1798
2025-03-24 15:52:35,436 - cloned_fsc_actor_policy.py:190 - Epoch 8000, Accuracy: 0.8949
2025-03-24 15:52:39,192 - cloned_fsc_actor_policy.py:189 - Epoch 9000, Loss: 0.1708
2025-03-24 15:52:39,192 - cloned_fsc_actor_policy.py:190 - Epoch 9000, Accuracy: 0.9010
2025-03-24 15:52:42,721 - cloned_fsc_actor_policy.py:189 - Epoch 10000, Loss: 0.1621
2025-03-24 15:52:42,721 - cloned_fsc_actor_policy.py:190 - Epoch 10000, Accuracy: 0.9116
2025-03-24 15:52:42,758 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:52:59,422 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:52:59,423 - evaluators.py:130 - Average Virtual Goal Value = 0.11580870300531387
2025-03-24 15:52:59,423 - evaluators.py:132 - Goal Reach Probability = 0.002316174071899263
2025-03-24 15:52:59,423 - evaluators.py:134 - Trap Reach Probability = 0.9976838259281008
2025-03-24 15:52:59,423 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:52:59,423 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:52:59,423 - evaluators.py:140 - Current Best Reach Probability = 0.002316174071899263
2025-03-24 15:53:02,867 - cloned_fsc_actor_policy.py:189 - Epoch 11000, Loss: 0.1513
2025-03-24 15:53:02,867 - cloned_fsc_actor_policy.py:190 - Epoch 11000, Accuracy: 0.9329
2025-03-24 15:53:06,308 - cloned_fsc_actor_policy.py:189 - Epoch 12000, Loss: 0.1369
2025-03-24 15:53:06,308 - cloned_fsc_actor_policy.py:190 - Epoch 12000, Accuracy: 0.9388
2025-03-24 15:53:09,786 - cloned_fsc_actor_policy.py:189 - Epoch 13000, Loss: 0.1273
2025-03-24 15:53:09,786 - cloned_fsc_actor_policy.py:190 - Epoch 13000, Accuracy: 0.9484
2025-03-24 15:53:13,602 - cloned_fsc_actor_policy.py:189 - Epoch 14000, Loss: 0.1203
2025-03-24 15:53:13,602 - cloned_fsc_actor_policy.py:190 - Epoch 14000, Accuracy: 0.9565
2025-03-24 15:53:17,037 - cloned_fsc_actor_policy.py:189 - Epoch 15000, Loss: 0.1156
2025-03-24 15:53:17,037 - cloned_fsc_actor_policy.py:190 - Epoch 15000, Accuracy: 0.9583
2025-03-24 15:53:17,069 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:53:33,835 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:53:33,836 - evaluators.py:130 - Average Virtual Goal Value = 0.12233712524175644
2025-03-24 15:53:33,836 - evaluators.py:132 - Goal Reach Probability = 0.0024467425700583955
2025-03-24 15:53:33,836 - evaluators.py:134 - Trap Reach Probability = 0.9975532574299416
2025-03-24 15:53:33,836 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:53:33,836 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:53:33,836 - evaluators.py:140 - Current Best Reach Probability = 0.0024467425700583955
2025-03-24 15:53:37,431 - cloned_fsc_actor_policy.py:189 - Epoch 16000, Loss: 0.1133
2025-03-24 15:53:37,431 - cloned_fsc_actor_policy.py:190 - Epoch 16000, Accuracy: 0.9585
2025-03-24 15:53:41,069 - cloned_fsc_actor_policy.py:189 - Epoch 17000, Loss: 0.1107
2025-03-24 15:53:41,069 - cloned_fsc_actor_policy.py:190 - Epoch 17000, Accuracy: 0.9585
2025-03-24 15:53:44,689 - cloned_fsc_actor_policy.py:189 - Epoch 18000, Loss: 0.1080
2025-03-24 15:53:44,690 - cloned_fsc_actor_policy.py:190 - Epoch 18000, Accuracy: 0.9591
2025-03-24 15:53:48,292 - cloned_fsc_actor_policy.py:189 - Epoch 19000, Loss: 0.1068
2025-03-24 15:53:48,292 - cloned_fsc_actor_policy.py:190 - Epoch 19000, Accuracy: 0.9592
2025-03-24 15:53:51,823 - cloned_fsc_actor_policy.py:189 - Epoch 20000, Loss: 0.1063
2025-03-24 15:53:51,823 - cloned_fsc_actor_policy.py:190 - Epoch 20000, Accuracy: 0.9590
2025-03-24 15:53:51,853 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:54:10,046 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:54:10,046 - evaluators.py:130 - Average Virtual Goal Value = 0.13868041336536407
2025-03-24 15:54:10,046 - evaluators.py:132 - Goal Reach Probability = 0.002773608301246492
2025-03-24 15:54:10,046 - evaluators.py:134 - Trap Reach Probability = 0.9972263916987535
2025-03-24 15:54:10,046 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:54:10,046 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:54:10,046 - evaluators.py:140 - Current Best Reach Probability = 0.002773608301246492
2025-03-24 15:54:13,598 - cloned_fsc_actor_policy.py:189 - Epoch 21000, Loss: 0.1052
2025-03-24 15:54:13,599 - cloned_fsc_actor_policy.py:190 - Epoch 21000, Accuracy: 0.9594
2025-03-24 15:54:17,075 - cloned_fsc_actor_policy.py:189 - Epoch 22000, Loss: 0.1046
2025-03-24 15:54:17,075 - cloned_fsc_actor_policy.py:190 - Epoch 22000, Accuracy: 0.9595
2025-03-24 15:54:20,616 - cloned_fsc_actor_policy.py:189 - Epoch 23000, Loss: 0.1045
2025-03-24 15:54:20,616 - cloned_fsc_actor_policy.py:190 - Epoch 23000, Accuracy: 0.9596
2025-03-24 15:54:24,437 - cloned_fsc_actor_policy.py:189 - Epoch 24000, Loss: 0.1037
2025-03-24 15:54:24,437 - cloned_fsc_actor_policy.py:190 - Epoch 24000, Accuracy: 0.9596
2025-03-24 15:54:28,019 - cloned_fsc_actor_policy.py:189 - Epoch 25000, Loss: 0.1040
2025-03-24 15:54:28,019 - cloned_fsc_actor_policy.py:190 - Epoch 25000, Accuracy: 0.9595
2025-03-24 15:54:28,269 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:54:45,298 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:54:45,298 - evaluators.py:130 - Average Virtual Goal Value = 1.8572825193405151
2025-03-24 15:54:45,298 - evaluators.py:132 - Goal Reach Probability = 0.03714565004887586
2025-03-24 15:54:45,298 - evaluators.py:134 - Trap Reach Probability = 0.9628543499511242
2025-03-24 15:54:45,298 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:54:45,298 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:54:45,298 - evaluators.py:140 - Current Best Reach Probability = 0.03714565004887586
2025-03-24 15:54:48,785 - cloned_fsc_actor_policy.py:189 - Epoch 26000, Loss: 0.1031
2025-03-24 15:54:48,785 - cloned_fsc_actor_policy.py:190 - Epoch 26000, Accuracy: 0.9601
2025-03-24 15:54:52,508 - cloned_fsc_actor_policy.py:189 - Epoch 27000, Loss: 0.1023
2025-03-24 15:54:52,508 - cloned_fsc_actor_policy.py:190 - Epoch 27000, Accuracy: 0.9605
2025-03-24 15:54:56,318 - cloned_fsc_actor_policy.py:189 - Epoch 28000, Loss: 0.1026
2025-03-24 15:54:56,318 - cloned_fsc_actor_policy.py:190 - Epoch 28000, Accuracy: 0.9603
2025-03-24 15:54:59,868 - cloned_fsc_actor_policy.py:189 - Epoch 29000, Loss: 0.1026
2025-03-24 15:54:59,868 - cloned_fsc_actor_policy.py:190 - Epoch 29000, Accuracy: 0.9601
2025-03-24 15:55:03,431 - cloned_fsc_actor_policy.py:189 - Epoch 30000, Loss: 0.1028
2025-03-24 15:55:03,432 - cloned_fsc_actor_policy.py:190 - Epoch 30000, Accuracy: 0.9604
2025-03-24 15:55:03,461 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:55:20,226 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:55:20,226 - evaluators.py:130 - Average Virtual Goal Value = 0.10925575345754623
2025-03-24 15:55:20,226 - evaluators.py:132 - Goal Reach Probability = 0.002185115126214859
2025-03-24 15:55:20,226 - evaluators.py:134 - Trap Reach Probability = 0.9978148848737851
2025-03-24 15:55:20,226 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:55:20,226 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:55:20,226 - evaluators.py:140 - Current Best Reach Probability = 0.03714565004887586
2025-03-24 15:55:23,702 - cloned_fsc_actor_policy.py:189 - Epoch 31000, Loss: 0.1021
2025-03-24 15:55:23,702 - cloned_fsc_actor_policy.py:190 - Epoch 31000, Accuracy: 0.9605
2025-03-24 15:55:27,169 - cloned_fsc_actor_policy.py:189 - Epoch 32000, Loss: 0.1026
2025-03-24 15:55:27,169 - cloned_fsc_actor_policy.py:190 - Epoch 32000, Accuracy: 0.9604
2025-03-24 15:55:30,650 - cloned_fsc_actor_policy.py:189 - Epoch 33000, Loss: 0.1012
2025-03-24 15:55:30,650 - cloned_fsc_actor_policy.py:190 - Epoch 33000, Accuracy: 0.9610
2025-03-24 15:55:34,158 - cloned_fsc_actor_policy.py:189 - Epoch 34000, Loss: 0.1013
2025-03-24 15:55:34,158 - cloned_fsc_actor_policy.py:190 - Epoch 34000, Accuracy: 0.9608
2025-03-24 15:55:37,507 - cloned_fsc_actor_policy.py:189 - Epoch 35000, Loss: 0.1011
2025-03-24 15:55:37,507 - cloned_fsc_actor_policy.py:190 - Epoch 35000, Accuracy: 0.9607
2025-03-24 15:55:37,539 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:55:54,162 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:55:54,163 - evaluators.py:130 - Average Virtual Goal Value = 0.11742065846920013
2025-03-24 15:55:54,163 - evaluators.py:132 - Goal Reach Probability = 0.002348413190254085
2025-03-24 15:55:54,163 - evaluators.py:134 - Trap Reach Probability = 0.9976515868097459
2025-03-24 15:55:54,163 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:55:54,163 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:55:54,163 - evaluators.py:140 - Current Best Reach Probability = 0.03714565004887586
2025-03-24 15:55:57,513 - cloned_fsc_actor_policy.py:189 - Epoch 36000, Loss: 0.1012
2025-03-24 15:55:57,513 - cloned_fsc_actor_policy.py:190 - Epoch 36000, Accuracy: 0.9608
2025-03-24 15:56:01,030 - cloned_fsc_actor_policy.py:189 - Epoch 37000, Loss: 0.1007
2025-03-24 15:56:01,030 - cloned_fsc_actor_policy.py:190 - Epoch 37000, Accuracy: 0.9610
2025-03-24 15:56:04,945 - cloned_fsc_actor_policy.py:189 - Epoch 38000, Loss: 0.1009
2025-03-24 15:56:04,945 - cloned_fsc_actor_policy.py:190 - Epoch 38000, Accuracy: 0.9609
2025-03-24 15:56:08,652 - cloned_fsc_actor_policy.py:189 - Epoch 39000, Loss: 0.1014
2025-03-24 15:56:08,652 - cloned_fsc_actor_policy.py:190 - Epoch 39000, Accuracy: 0.9606
2025-03-24 15:56:12,227 - cloned_fsc_actor_policy.py:189 - Epoch 40000, Loss: 0.1005
2025-03-24 15:56:12,227 - cloned_fsc_actor_policy.py:190 - Epoch 40000, Accuracy: 0.9610
2025-03-24 15:56:12,257 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:56:28,511 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:56:28,511 - evaluators.py:130 - Average Virtual Goal Value = 1.7851128578186035
2025-03-24 15:56:28,511 - evaluators.py:132 - Goal Reach Probability = 0.035702256652071405
2025-03-24 15:56:28,511 - evaluators.py:134 - Trap Reach Probability = 0.9642977433479286
2025-03-24 15:56:28,511 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:56:28,511 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:56:28,511 - evaluators.py:140 - Current Best Reach Probability = 0.03714565004887586
2025-03-24 15:56:31,917 - cloned_fsc_actor_policy.py:189 - Epoch 41000, Loss: 0.1007
2025-03-24 15:56:31,917 - cloned_fsc_actor_policy.py:190 - Epoch 41000, Accuracy: 0.9610
2025-03-24 15:56:35,473 - cloned_fsc_actor_policy.py:189 - Epoch 42000, Loss: 0.1003
2025-03-24 15:56:35,473 - cloned_fsc_actor_policy.py:190 - Epoch 42000, Accuracy: 0.9606
2025-03-24 15:56:38,962 - cloned_fsc_actor_policy.py:189 - Epoch 43000, Loss: 0.0996
2025-03-24 15:56:38,962 - cloned_fsc_actor_policy.py:190 - Epoch 43000, Accuracy: 0.9605
2025-03-24 15:56:42,899 - cloned_fsc_actor_policy.py:189 - Epoch 44000, Loss: 0.0977
2025-03-24 15:56:42,900 - cloned_fsc_actor_policy.py:190 - Epoch 44000, Accuracy: 0.9603
2025-03-24 15:56:46,454 - cloned_fsc_actor_policy.py:189 - Epoch 45000, Loss: 0.0955
2025-03-24 15:56:46,454 - cloned_fsc_actor_policy.py:190 - Epoch 45000, Accuracy: 0.9616
2025-03-24 15:56:46,485 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:57:03,820 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:57:03,820 - evaluators.py:130 - Average Virtual Goal Value = 0.11744364351034164
2025-03-24 15:57:03,820 - evaluators.py:132 - Goal Reach Probability = 0.0023488728672560596
2025-03-24 15:57:03,820 - evaluators.py:134 - Trap Reach Probability = 0.997651127132744
2025-03-24 15:57:03,820 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:57:03,820 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:57:03,820 - evaluators.py:140 - Current Best Reach Probability = 0.03714565004887586
2025-03-24 15:57:07,158 - cloned_fsc_actor_policy.py:189 - Epoch 46000, Loss: 0.0930
2025-03-24 15:57:07,158 - cloned_fsc_actor_policy.py:190 - Epoch 46000, Accuracy: 0.9640
2025-03-24 15:57:10,505 - cloned_fsc_actor_policy.py:189 - Epoch 47000, Loss: 0.0919
2025-03-24 15:57:10,505 - cloned_fsc_actor_policy.py:190 - Epoch 47000, Accuracy: 0.9638
2025-03-24 15:57:14,112 - cloned_fsc_actor_policy.py:189 - Epoch 48000, Loss: 0.0899
2025-03-24 15:57:14,113 - cloned_fsc_actor_policy.py:190 - Epoch 48000, Accuracy: 0.9642
2025-03-24 15:57:17,539 - cloned_fsc_actor_policy.py:189 - Epoch 49000, Loss: 0.0905
2025-03-24 15:57:17,540 - cloned_fsc_actor_policy.py:190 - Epoch 49000, Accuracy: 0.9636
2025-03-24 15:57:20,975 - cloned_fsc_actor_policy.py:189 - Epoch 50000, Loss: 0.0893
2025-03-24 15:57:20,975 - cloned_fsc_actor_policy.py:190 - Epoch 50000, Accuracy: 0.9639
2025-03-24 15:57:21,006 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:57:37,579 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:57:37,579 - evaluators.py:130 - Average Virtual Goal Value = 0.11578981578350067
2025-03-24 15:57:37,579 - evaluators.py:132 - Goal Reach Probability = 0.002315796340389445
2025-03-24 15:57:37,579 - evaluators.py:134 - Trap Reach Probability = 0.9976842036596105
2025-03-24 15:57:37,579 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:57:37,579 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:57:37,579 - evaluators.py:140 - Current Best Reach Probability = 0.03714565004887586
2025-03-24 15:57:41,157 - cloned_fsc_actor_policy.py:189 - Epoch 51000, Loss: 0.0882
2025-03-24 15:57:41,158 - cloned_fsc_actor_policy.py:190 - Epoch 51000, Accuracy: 0.9649
2025-03-24 15:57:44,728 - cloned_fsc_actor_policy.py:189 - Epoch 52000, Loss: 0.0882
2025-03-24 15:57:44,728 - cloned_fsc_actor_policy.py:190 - Epoch 52000, Accuracy: 0.9652
2025-03-24 15:57:48,244 - cloned_fsc_actor_policy.py:189 - Epoch 53000, Loss: 0.0877
2025-03-24 15:57:48,244 - cloned_fsc_actor_policy.py:190 - Epoch 53000, Accuracy: 0.9659
2025-03-24 15:57:51,568 - cloned_fsc_actor_policy.py:189 - Epoch 54000, Loss: 0.0874
2025-03-24 15:57:51,568 - cloned_fsc_actor_policy.py:190 - Epoch 54000, Accuracy: 0.9660
2025-03-24 15:57:55,166 - cloned_fsc_actor_policy.py:189 - Epoch 55000, Loss: 0.0867
2025-03-24 15:57:55,166 - cloned_fsc_actor_policy.py:190 - Epoch 55000, Accuracy: 0.9667
2025-03-24 15:57:55,210 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:58:12,823 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:58:12,823 - evaluators.py:130 - Average Virtual Goal Value = 0.14522551000118256
2025-03-24 15:58:12,823 - evaluators.py:132 - Goal Reach Probability = 0.0029045101494680503
2025-03-24 15:58:12,823 - evaluators.py:134 - Trap Reach Probability = 0.997095489850532
2025-03-24 15:58:12,823 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:58:12,823 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:58:12,823 - evaluators.py:140 - Current Best Reach Probability = 0.03714565004887586
2025-03-24 15:58:16,316 - cloned_fsc_actor_policy.py:189 - Epoch 56000, Loss: 0.0853
2025-03-24 15:58:16,316 - cloned_fsc_actor_policy.py:190 - Epoch 56000, Accuracy: 0.9674
2025-03-24 15:58:19,785 - cloned_fsc_actor_policy.py:189 - Epoch 57000, Loss: 0.0849
2025-03-24 15:58:19,786 - cloned_fsc_actor_policy.py:190 - Epoch 57000, Accuracy: 0.9680
2025-03-24 15:58:23,239 - cloned_fsc_actor_policy.py:189 - Epoch 58000, Loss: 0.0849
2025-03-24 15:58:23,239 - cloned_fsc_actor_policy.py:190 - Epoch 58000, Accuracy: 0.9683
2025-03-24 15:58:26,758 - cloned_fsc_actor_policy.py:189 - Epoch 59000, Loss: 0.0839
2025-03-24 15:58:26,758 - cloned_fsc_actor_policy.py:190 - Epoch 59000, Accuracy: 0.9687
2025-03-24 15:58:30,424 - cloned_fsc_actor_policy.py:189 - Epoch 60000, Loss: 0.0828
2025-03-24 15:58:30,424 - cloned_fsc_actor_policy.py:190 - Epoch 60000, Accuracy: 0.9691
2025-03-24 15:58:30,455 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:58:48,477 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:58:48,477 - evaluators.py:130 - Average Virtual Goal Value = 0.10764262825250626
2025-03-24 15:58:48,477 - evaluators.py:132 - Goal Reach Probability = 0.002152852529601722
2025-03-24 15:58:48,477 - evaluators.py:134 - Trap Reach Probability = 0.9978471474703983
2025-03-24 15:58:48,477 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:58:48,477 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:58:48,477 - evaluators.py:140 - Current Best Reach Probability = 0.03714565004887586
2025-03-24 15:58:49,302 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 15:59:05,743 - evaluators.py:128 - Average Return = 0.0
2025-03-24 15:59:05,743 - evaluators.py:130 - Average Virtual Goal Value = 1.8599488735198975
2025-03-24 15:59:05,743 - evaluators.py:132 - Goal Reach Probability = 0.03719897753262478
2025-03-24 15:59:05,743 - evaluators.py:134 - Trap Reach Probability = 0.9628010224673752
2025-03-24 15:59:05,743 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 15:59:05,743 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 15:59:05,743 - evaluators.py:140 - Current Best Reach Probability = 0.03719897753262478
2025-03-24 15:59:05,802 - pomdp.py:349 - unfolding 5-FSC template into POMDP...
2025-03-24 15:59:06,955 - pomdp.py:355 - constructed quotient MDP having 33898 states and 617106 actions.
2025-03-24 15:59:08,267 - rl_family_extractor.py:329 - Building family from FFNN...
2025-03-24 15:59:08,351 - rl_family_extractor.py:357 - Number of misses: 3 out of 1000
2025-03-24 15:59:08,351 - rl_family_extractor.py:359 - Number of complete misses: 3 out of 1000
2025-03-24 15:59:08,352 - statistic.py:67 - synthesis initiated, design space: 64
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 0.56 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 100 %
DTMC stats: avg DTMC size: 1151, iterations: 64

optimum: 0.140482
--------------------
2025-03-24 15:59:08,915 - statistic.py:67 - synthesis initiated, design space: 1e609
> progress 0.0%, elapsed 3 s, estimated 3004109 s (34 days), iters = {DTMC: 371}, opt = 0.1405
> progress 0.0%, elapsed 6 s, estimated 6008664 s (69 days), iters = {DTMC: 744}, opt = 0.1405
> progress 0.0%, elapsed 9 s, estimated 9009673 s (104 days), iters = {DTMC: 1119}, opt = 0.1405
> progress 0.0%, elapsed 12 s, estimated 12015096 s (139 days), iters = {DTMC: 1488}, opt = 0.1405
> progress 0.0%, elapsed 15 s, estimated 15017076 s (173 days), iters = {DTMC: 1860}, opt = 0.1405
> progress 0.0%, elapsed 18 s, estimated 18022455 s (208 days), iters = {DTMC: 2239}, opt = 0.1405
> progress 0.0%, elapsed 21 s, estimated 21029141 s (243 days), iters = {DTMC: 2620}, opt = 0.1405
> progress 0.0%, elapsed 24 s, estimated 24032009 s (278 days), iters = {DTMC: 2996}, opt = 0.1405
> progress 0.0%, elapsed 27 s, estimated 27040786 s (312 days), iters = {DTMC: 3371}, opt = 0.1405
> progress 0.0%, elapsed 30 s, estimated 30047417 s (347 days), iters = {DTMC: 3741}, opt = 0.1405
> progress 0.0%, elapsed 33 s, estimated 33053537 s (1 year), iters = {DTMC: 4117}, opt = 0.1405
> progress 0.0%, elapsed 36 s, estimated 36053929 s (1 year), iters = {DTMC: 4494}, opt = 0.1405
> progress 0.0%, elapsed 39 s, estimated 39057387 s (1 year), iters = {DTMC: 4820}, opt = 0.1405
> progress 0.0%, elapsed 42 s, estimated 42060051 s (1 year), iters = {DTMC: 5202}, opt = 0.1405
> progress 0.0%, elapsed 45 s, estimated 45066728 s (1 year), iters = {DTMC: 5581}, opt = 0.1405
> progress 0.0%, elapsed 48 s, estimated 48068510 s (1 year), iters = {DTMC: 5966}, opt = 0.1405
> progress 0.0%, elapsed 51 s, estimated 51075703 s (1 year), iters = {DTMC: 6350}, opt = 0.1405
> progress 0.0%, elapsed 54 s, estimated 54081466 s (1 year), iters = {DTMC: 6736}, opt = 0.1405
> progress 0.0%, elapsed 57 s, estimated 57084952 s (1 year), iters = {DTMC: 7119}, opt = 0.1405
2025-03-24 16:00:08,924 - synthesizer_onebyone.py:19 - Time limit reached
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.01 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 0 %
DTMC stats: avg DTMC size: 212, iterations: 7485

optimum: 0.140482
--------------------
2025-03-24 16:00:08,925 - synthesizer_rl_storm_paynt.py:280 - No improving assignment found.
2025-03-24 16:00:08,925 - storm_pomdp_control.py:246 - Interactive Storm resumed
2025-03-24 16:00:08,934 - storm_pomdp_control.py:291 - Updating FSC values in Storm
2025-03-24 16:00:39,941 - storm_pomdp_control.py:305 - Pausing Storm
-----------Storm-----------               
Value = 0.23461992224252154 | Time elapsed = 3086.1s | FSC size = 1636

2025-03-24 16:01:57,248 - synthesizer_pomdp.py:253 - Timeout for PAYNT started
2025-03-24 16:01:57,499 - synthesizer_ar_storm.py:136 - Resuming synthesis
2025-03-24 16:01:57,507 - synthesizer_ar_storm.py:147 - PAYNT's value is better. Prioritizing synthesis results
2025-03-24 16:02:27,407 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 16:02:27,481 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:02:38,690 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:02:38,691 - father_agent.py:564 - Average Virtual Goal Value = 2.0819804668426514
2025-03-24 16:02:38,691 - father_agent.py:566 - Goal Reach Probability = 0.04163961038961039
2025-03-24 16:02:38,691 - father_agent.py:568 - Trap Reach Probability = 0.9583603896103896
2025-03-24 16:02:38,691 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:02:38,691 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:02:38,691 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:02:42,846 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:02:47,022 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:02:47,022 - evaluators.py:130 - Average Virtual Goal Value = 0.09791921824216843
2025-03-24 16:02:47,022 - evaluators.py:132 - Goal Reach Probability = 0.0019583843329253367
2025-03-24 16:02:47,022 - evaluators.py:134 - Trap Reach Probability = 0.9980416156670746
2025-03-24 16:02:47,022 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:02:47,022 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:02:47,022 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 16:02:47,022 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 1.5839874744415283
Epoch: 0, Critic Loss: 14.12883472442627
Epoch: 10, Actor Loss: 0.12533573806285858
Epoch: 10, Critic Loss: 9.313962936401367
Epoch: 20, Actor Loss: 0.08968061208724976
Epoch: 20, Critic Loss: 32.53839874267578
Epoch: 30, Actor Loss: 0.07130451500415802
Epoch: 30, Critic Loss: 11.172767639160156
Epoch: 40, Actor Loss: 0.04321857541799545
Epoch: 40, Critic Loss: 7.911523342132568
2025-03-24 16:03:21,712 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:03:25,214 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:03:25,214 - evaluators.py:130 - Average Virtual Goal Value = 8.260869979858398
2025-03-24 16:03:25,214 - evaluators.py:132 - Goal Reach Probability = 0.16521739130434782
2025-03-24 16:03:25,214 - evaluators.py:134 - Trap Reach Probability = 0.8347826086956521
2025-03-24 16:03:25,214 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:03:25,214 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:03:25,214 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 16:03:25,214 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.026544611901044846
Epoch: 50, Critic Loss: 11.93024730682373
Epoch: 60, Actor Loss: 0.031041521579027176
Epoch: 60, Critic Loss: 10.419487953186035
Epoch: 70, Actor Loss: 0.041933540254831314
Epoch: 70, Critic Loss: 10.057415008544922
Epoch: 80, Actor Loss: 0.03257237374782562
Epoch: 80, Critic Loss: 12.95926284790039
Epoch: 90, Actor Loss: 0.04915019869804382
Epoch: 90, Critic Loss: 8.364236831665039
2025-03-24 16:03:59,075 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:04:04,617 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:04:04,617 - evaluators.py:130 - Average Virtual Goal Value = 9.804283142089844
2025-03-24 16:04:04,617 - evaluators.py:132 - Goal Reach Probability = 0.19608567208271788
2025-03-24 16:04:04,617 - evaluators.py:134 - Trap Reach Probability = 0.8039143279172821
2025-03-24 16:04:04,618 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:04:04,618 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:04:04,618 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 16:04:04,618 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.026736846193671227
Epoch: 100, Critic Loss: 5.339143753051758
Epoch: 110, Actor Loss: 0.018812399357557297
Epoch: 110, Critic Loss: 14.533828735351562
Epoch: 120, Actor Loss: 0.04108002781867981
Epoch: 120, Critic Loss: 13.906174659729004
Epoch: 130, Actor Loss: 0.03453421965241432
Epoch: 130, Critic Loss: 11.546177864074707
Epoch: 140, Actor Loss: 0.031957171857357025
Epoch: 140, Critic Loss: 10.718232154846191
2025-03-24 16:04:40,433 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:04:43,733 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:04:43,734 - evaluators.py:130 - Average Virtual Goal Value = 10.175176620483398
2025-03-24 16:04:43,734 - evaluators.py:132 - Goal Reach Probability = 0.20350354081252328
2025-03-24 16:04:43,734 - evaluators.py:134 - Trap Reach Probability = 0.7964964591874767
2025-03-24 16:04:43,734 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:04:43,734 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:04:43,734 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 16:04:43,734 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.03395992890000343
Epoch: 150, Critic Loss: 11.207107543945312
Epoch: 160, Actor Loss: 0.022543100640177727
Epoch: 160, Critic Loss: 8.735892295837402
Epoch: 170, Actor Loss: 0.01923571340739727
Epoch: 170, Critic Loss: 10.578283309936523
Epoch: 180, Actor Loss: 0.04190506786108017
Epoch: 180, Critic Loss: 13.878915786743164
Epoch: 190, Actor Loss: 0.022974906489253044
Epoch: 190, Critic Loss: 13.14604663848877
2025-03-24 16:05:18,187 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:05:21,447 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:05:21,447 - evaluators.py:130 - Average Virtual Goal Value = 10.10324478149414
2025-03-24 16:05:21,447 - evaluators.py:132 - Goal Reach Probability = 0.20206489675516223
2025-03-24 16:05:21,447 - evaluators.py:134 - Trap Reach Probability = 0.7979351032448377
2025-03-24 16:05:21,447 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:05:21,447 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:05:21,447 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 16:05:21,447 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.02384575828909874
Epoch: 200, Critic Loss: 11.494585037231445
Epoch: 210, Actor Loss: 0.016213329508900642
Epoch: 210, Critic Loss: 14.603667259216309
Epoch: 220, Actor Loss: 0.02292804792523384
Epoch: 220, Critic Loss: 10.98695182800293
Epoch: 230, Actor Loss: 0.018328862264752388
Epoch: 230, Critic Loss: 12.717665672302246
Epoch: 240, Actor Loss: 0.014874194748699665
Epoch: 240, Critic Loss: 13.395898818969727
2025-03-24 16:05:54,557 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:05:57,729 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:05:57,729 - evaluators.py:130 - Average Virtual Goal Value = 9.067641258239746
2025-03-24 16:05:57,730 - evaluators.py:132 - Goal Reach Probability = 0.1813528336380256
2025-03-24 16:05:57,730 - evaluators.py:134 - Trap Reach Probability = 0.8186471663619744
2025-03-24 16:05:57,730 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:05:57,730 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:05:57,730 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 16:05:57,730 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.035917118191719055
Epoch: 250, Critic Loss: 15.052517890930176
Epoch: 260, Actor Loss: 0.03017137572169304
Epoch: 260, Critic Loss: 14.313949584960938
Epoch: 270, Actor Loss: 0.031815480440855026
Epoch: 270, Critic Loss: 3.9220664501190186
Epoch: 280, Actor Loss: 0.024936459958553314
Epoch: 280, Critic Loss: 10.919371604919434
Epoch: 290, Actor Loss: 0.019087180495262146
Epoch: 290, Critic Loss: 14.321004867553711
2025-03-24 16:06:32,422 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:06:35,679 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:06:35,679 - evaluators.py:130 - Average Virtual Goal Value = 10.340951919555664
2025-03-24 16:06:35,679 - evaluators.py:132 - Goal Reach Probability = 0.2068190333458224
2025-03-24 16:06:35,679 - evaluators.py:134 - Trap Reach Probability = 0.7931809666541776
2025-03-24 16:06:35,679 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:06:35,679 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:06:35,679 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 16:06:35,679 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.021138906478881836
Epoch: 300, Critic Loss: 8.36703872680664
Epoch: 310, Actor Loss: 0.00918898917734623
Epoch: 310, Critic Loss: 6.600638389587402
Epoch: 320, Actor Loss: 0.020202957093715668
Epoch: 320, Critic Loss: 15.988178253173828
Epoch: 330, Actor Loss: 0.029558634385466576
Epoch: 330, Critic Loss: 11.771757125854492
Epoch: 340, Actor Loss: 0.030350012704730034
Epoch: 340, Critic Loss: 11.9568510055542
2025-03-24 16:07:09,676 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:07:13,041 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:07:13,041 - evaluators.py:130 - Average Virtual Goal Value = 10.435267448425293
2025-03-24 16:07:13,041 - evaluators.py:132 - Goal Reach Probability = 0.20870535714285715
2025-03-24 16:07:13,041 - evaluators.py:134 - Trap Reach Probability = 0.7912946428571429
2025-03-24 16:07:13,041 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:07:13,041 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:07:13,041 - evaluators.py:140 - Current Best Reach Probability = 0.21342835708927232
2025-03-24 16:07:13,041 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.027032827958464622
Epoch: 350, Critic Loss: 12.177848815917969
Epoch: 360, Actor Loss: 0.013404169119894505
Epoch: 360, Critic Loss: 7.104917526245117
Epoch: 370, Actor Loss: 0.03636237978935242
Epoch: 370, Critic Loss: 11.270414352416992
Epoch: 380, Actor Loss: 0.030733920633792877
Epoch: 380, Critic Loss: 11.853791236877441
Epoch: 390, Actor Loss: 0.03390456363558769
Epoch: 390, Critic Loss: 16.166278839111328
2025-03-24 16:07:46,902 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:07:50,082 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:07:50,082 - evaluators.py:130 - Average Virtual Goal Value = 10.918020248413086
2025-03-24 16:07:50,082 - evaluators.py:132 - Goal Reach Probability = 0.21836040800906686
2025-03-24 16:07:50,082 - evaluators.py:134 - Trap Reach Probability = 0.7816395919909331
2025-03-24 16:07:50,082 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:07:50,082 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:07:50,082 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:07:50,082 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.027921803295612335
Epoch: 400, Critic Loss: 11.11307144165039
Epoch: 410, Actor Loss: 0.0245473962277174
Epoch: 410, Critic Loss: 10.882360458374023
Epoch: 420, Actor Loss: 0.027087781578302383
Epoch: 420, Critic Loss: 8.318241119384766
Epoch: 430, Actor Loss: 0.007717053405940533
Epoch: 430, Critic Loss: 10.694826126098633
Epoch: 440, Actor Loss: 0.02730604261159897
Epoch: 440, Critic Loss: 6.937361240386963
2025-03-24 16:08:23,702 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:08:27,742 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:08:27,742 - evaluators.py:130 - Average Virtual Goal Value = 8.662123680114746
2025-03-24 16:08:27,742 - evaluators.py:132 - Goal Reach Probability = 0.17324246771879484
2025-03-24 16:08:27,742 - evaluators.py:134 - Trap Reach Probability = 0.8267575322812052
2025-03-24 16:08:27,742 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:08:27,742 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:08:27,742 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:08:27,742 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.022409282624721527
Epoch: 450, Critic Loss: 13.950687408447266
Epoch: 460, Actor Loss: 0.03208707645535469
Epoch: 460, Critic Loss: 6.513482093811035
Epoch: 470, Actor Loss: 0.0380498431622982
Epoch: 470, Critic Loss: 14.541240692138672
Epoch: 480, Actor Loss: 0.02209816873073578
Epoch: 480, Critic Loss: 7.5235795974731445
Epoch: 490, Actor Loss: 0.013135576620697975
Epoch: 490, Critic Loss: 6.348214149475098
2025-03-24 16:09:02,408 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:09:05,808 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:09:05,808 - evaluators.py:130 - Average Virtual Goal Value = 9.04332160949707
2025-03-24 16:09:05,808 - evaluators.py:132 - Goal Reach Probability = 0.18086642599277977
2025-03-24 16:09:05,809 - evaluators.py:134 - Trap Reach Probability = 0.8191335740072202
2025-03-24 16:09:05,809 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:09:05,809 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:09:05,809 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:09:05,809 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.018691090866923332
Epoch: 500, Critic Loss: 10.462688446044922
Epoch: 510, Actor Loss: 0.01606394723057747
Epoch: 510, Critic Loss: 9.223554611206055
Epoch: 520, Actor Loss: 0.026244692504405975
Epoch: 520, Critic Loss: 14.81307601928711
Epoch: 530, Actor Loss: 0.034166134893894196
Epoch: 530, Critic Loss: 12.713178634643555
Epoch: 540, Actor Loss: 0.02165261097252369
Epoch: 540, Critic Loss: 6.276409149169922
2025-03-24 16:09:38,511 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:09:41,698 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:09:41,698 - evaluators.py:130 - Average Virtual Goal Value = 10.1414213180542
2025-03-24 16:09:41,698 - evaluators.py:132 - Goal Reach Probability = 0.20282843319687383
2025-03-24 16:09:41,698 - evaluators.py:134 - Trap Reach Probability = 0.7971715668031262
2025-03-24 16:09:41,698 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:09:41,698 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:09:41,698 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:09:41,698 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.021478809416294098
Epoch: 550, Critic Loss: 11.828886985778809
Epoch: 560, Actor Loss: 0.015467531979084015
Epoch: 560, Critic Loss: 8.376068115234375
Epoch: 570, Actor Loss: 0.020336927846074104
Epoch: 570, Critic Loss: 13.139741897583008
Epoch: 580, Actor Loss: 0.017802298069000244
Epoch: 580, Critic Loss: 8.184720993041992
Epoch: 590, Actor Loss: 0.02652507834136486
Epoch: 590, Critic Loss: 11.045675277709961
2025-03-24 16:10:14,424 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:10:17,464 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:10:17,464 - evaluators.py:130 - Average Virtual Goal Value = 10.058308601379395
2025-03-24 16:10:17,464 - evaluators.py:132 - Goal Reach Probability = 0.20116618075801748
2025-03-24 16:10:17,464 - evaluators.py:134 - Trap Reach Probability = 0.7988338192419825
2025-03-24 16:10:17,464 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:10:17,464 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:10:17,464 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:10:17,464 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.03583288565278053
Epoch: 600, Critic Loss: 17.25294303894043
Epoch: 610, Actor Loss: 0.02064565010368824
Epoch: 610, Critic Loss: 7.154696464538574
Epoch: 620, Actor Loss: 0.017169931903481483
Epoch: 620, Critic Loss: 7.673447608947754
Epoch: 630, Actor Loss: 0.030843783169984818
Epoch: 630, Critic Loss: 10.052640914916992
Epoch: 640, Actor Loss: 0.02961081825196743
Epoch: 640, Critic Loss: 11.42319107055664
2025-03-24 16:10:50,418 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:10:53,537 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:10:53,538 - evaluators.py:130 - Average Virtual Goal Value = 9.643255233764648
2025-03-24 16:10:53,538 - evaluators.py:132 - Goal Reach Probability = 0.19286510590858416
2025-03-24 16:10:53,538 - evaluators.py:134 - Trap Reach Probability = 0.8071348940914158
2025-03-24 16:10:53,538 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:10:53,538 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:10:53,538 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:10:53,538 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.0245832446962595
Epoch: 650, Critic Loss: 21.634469985961914
Epoch: 660, Actor Loss: 0.023452162742614746
Epoch: 660, Critic Loss: 12.003538131713867
Epoch: 670, Actor Loss: 0.018406040966510773
Epoch: 670, Critic Loss: 13.661338806152344
Epoch: 680, Actor Loss: 0.02941887080669403
Epoch: 680, Critic Loss: 11.56940746307373
Epoch: 690, Actor Loss: 0.03870300576090813
Epoch: 690, Critic Loss: 15.35770034790039
2025-03-24 16:11:26,416 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:11:29,343 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:11:29,344 - evaluators.py:130 - Average Virtual Goal Value = 9.934853553771973
2025-03-24 16:11:29,344 - evaluators.py:132 - Goal Reach Probability = 0.1986970684039088
2025-03-24 16:11:29,344 - evaluators.py:134 - Trap Reach Probability = 0.8013029315960912
2025-03-24 16:11:29,344 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:11:29,344 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:11:29,344 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:11:29,344 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.036691855639219284
Epoch: 700, Critic Loss: 5.040122032165527
2025-03-24 16:11:29,346 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 16:11:29,346 - father_agent.py:449 - Before training evaluation.
2025-03-24 16:11:29,346 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:11:36,696 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:11:36,696 - father_agent.py:564 - Average Virtual Goal Value = 9.238625526428223
2025-03-24 16:11:36,696 - father_agent.py:566 - Goal Reach Probability = 0.18477251624883936
2025-03-24 16:11:36,696 - father_agent.py:568 - Trap Reach Probability = 0.8152274837511606
2025-03-24 16:11:36,696 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:11:36,696 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:11:36,696 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:11:36,738 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:11:36,842 - father_agent.py:359 - Training agent on-policy
2025-03-24 16:11:40,499 - father_agent.py:306 - Step: 0, Training loss: 84.01007080078125
2025-03-24 16:11:40,539 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:11:48,497 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:11:48,497 - father_agent.py:564 - Average Virtual Goal Value = 9.998784065246582
2025-03-24 16:11:48,498 - father_agent.py:566 - Goal Reach Probability = 0.19997567205936018
2025-03-24 16:11:48,498 - father_agent.py:568 - Trap Reach Probability = 0.8000243279406398
2025-03-24 16:11:48,498 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:11:48,498 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:11:48,498 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:11:48,529 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:11:50,484 - father_agent.py:306 - Step: 5, Training loss: 45.58745193481445
2025-03-24 16:11:51,577 - father_agent.py:306 - Step: 10, Training loss: 25.04905891418457
2025-03-24 16:11:52,686 - father_agent.py:306 - Step: 15, Training loss: 8.999978065490723
2025-03-24 16:11:53,870 - father_agent.py:306 - Step: 20, Training loss: 3.058170795440674
2025-03-24 16:11:58,372 - father_agent.py:306 - Step: 25, Training loss: 1.5549756288528442
2025-03-24 16:11:59,645 - father_agent.py:306 - Step: 30, Training loss: 1.0540176630020142
2025-03-24 16:12:00,964 - father_agent.py:306 - Step: 35, Training loss: 0.31632792949676514
2025-03-24 16:12:02,431 - father_agent.py:306 - Step: 40, Training loss: 0.20791971683502197
2025-03-24 16:12:03,657 - father_agent.py:306 - Step: 45, Training loss: 0.3167293965816498
2025-03-24 16:12:05,025 - father_agent.py:306 - Step: 50, Training loss: 0.504950761795044
2025-03-24 16:12:06,387 - father_agent.py:306 - Step: 55, Training loss: 0.6222135424613953
2025-03-24 16:12:07,602 - father_agent.py:306 - Step: 60, Training loss: 0.4784173369407654
2025-03-24 16:12:08,809 - father_agent.py:306 - Step: 65, Training loss: 0.8117133975028992
2025-03-24 16:12:10,128 - father_agent.py:306 - Step: 70, Training loss: 0.9172291159629822
2025-03-24 16:12:11,353 - father_agent.py:306 - Step: 75, Training loss: 0.43682870268821716
2025-03-24 16:12:12,575 - father_agent.py:306 - Step: 80, Training loss: 0.6038402318954468
2025-03-24 16:12:13,843 - father_agent.py:306 - Step: 85, Training loss: 0.6517330408096313
2025-03-24 16:12:15,195 - father_agent.py:306 - Step: 90, Training loss: 0.46955326199531555
2025-03-24 16:12:16,543 - father_agent.py:306 - Step: 95, Training loss: 0.25294029712677
2025-03-24 16:12:17,851 - father_agent.py:306 - Step: 100, Training loss: 0.43049097061157227
2025-03-24 16:12:17,887 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:12:27,398 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:12:27,398 - father_agent.py:564 - Average Virtual Goal Value = 0.6803471446037292
2025-03-24 16:12:27,398 - father_agent.py:566 - Goal Reach Probability = 0.013606943218593704
2025-03-24 16:12:27,398 - father_agent.py:568 - Trap Reach Probability = 0.9863930567814063
2025-03-24 16:12:27,399 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:12:27,399 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:12:27,399 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:12:27,440 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:12:30,033 - father_agent.py:306 - Step: 105, Training loss: 0.9345393180847168
2025-03-24 16:12:31,309 - father_agent.py:306 - Step: 110, Training loss: 0.3042553961277008
2025-03-24 16:12:32,615 - father_agent.py:306 - Step: 115, Training loss: 0.17474398016929626
2025-03-24 16:12:33,892 - father_agent.py:306 - Step: 120, Training loss: 0.3583144247531891
2025-03-24 16:12:35,250 - father_agent.py:306 - Step: 125, Training loss: 0.517020583152771
2025-03-24 16:12:36,996 - father_agent.py:306 - Step: 130, Training loss: 0.24079175293445587
2025-03-24 16:12:38,943 - father_agent.py:306 - Step: 135, Training loss: 0.19777938723564148
2025-03-24 16:12:40,208 - father_agent.py:306 - Step: 140, Training loss: 0.15814697742462158
2025-03-24 16:12:41,507 - father_agent.py:306 - Step: 145, Training loss: 0.28245770931243896
2025-03-24 16:12:42,880 - father_agent.py:306 - Step: 150, Training loss: 0.23484858870506287
2025-03-24 16:12:44,193 - father_agent.py:306 - Step: 155, Training loss: 0.3603341281414032
2025-03-24 16:12:45,489 - father_agent.py:306 - Step: 160, Training loss: 0.32308486104011536
2025-03-24 16:12:46,854 - father_agent.py:306 - Step: 165, Training loss: 0.15588559210300446
2025-03-24 16:12:48,139 - father_agent.py:306 - Step: 170, Training loss: 0.6270160675048828
2025-03-24 16:12:49,432 - father_agent.py:306 - Step: 175, Training loss: 0.16217778623104095
2025-03-24 16:12:50,814 - father_agent.py:306 - Step: 180, Training loss: 0.28463396430015564
2025-03-24 16:12:52,182 - father_agent.py:306 - Step: 185, Training loss: 0.3150860071182251
2025-03-24 16:12:53,489 - father_agent.py:306 - Step: 190, Training loss: 0.14744897186756134
2025-03-24 16:12:54,859 - father_agent.py:306 - Step: 195, Training loss: 0.22008942067623138
2025-03-24 16:12:56,172 - father_agent.py:306 - Step: 200, Training loss: 0.15500913560390472
2025-03-24 16:12:56,210 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:13:05,491 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:13:05,491 - father_agent.py:564 - Average Virtual Goal Value = 0.19937041401863098
2025-03-24 16:13:05,491 - father_agent.py:566 - Goal Reach Probability = 0.003987408184679958
2025-03-24 16:13:05,491 - father_agent.py:568 - Trap Reach Probability = 0.9960125918153201
2025-03-24 16:13:05,491 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:13:05,491 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:13:05,491 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:13:05,526 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:13:07,859 - father_agent.py:306 - Step: 205, Training loss: 0.15365546941757202
2025-03-24 16:13:09,135 - father_agent.py:306 - Step: 210, Training loss: 0.2864183187484741
2025-03-24 16:13:10,523 - father_agent.py:306 - Step: 215, Training loss: 0.14209364354610443
2025-03-24 16:13:11,792 - father_agent.py:306 - Step: 220, Training loss: 0.19746056199073792
2025-03-24 16:13:13,273 - father_agent.py:306 - Step: 225, Training loss: 0.14226101338863373
2025-03-24 16:13:15,054 - father_agent.py:306 - Step: 230, Training loss: 0.21812987327575684
2025-03-24 16:13:16,390 - father_agent.py:306 - Step: 235, Training loss: 0.4782601296901703
2025-03-24 16:13:17,649 - father_agent.py:306 - Step: 240, Training loss: 0.31766462326049805
2025-03-24 16:13:19,065 - father_agent.py:306 - Step: 245, Training loss: 0.29876452684402466
2025-03-24 16:13:20,347 - father_agent.py:306 - Step: 250, Training loss: 0.14932754635810852
2025-03-24 16:13:21,624 - father_agent.py:306 - Step: 255, Training loss: 0.26559045910835266
2025-03-24 16:13:23,041 - father_agent.py:306 - Step: 260, Training loss: 0.4658154845237732
2025-03-24 16:13:24,304 - father_agent.py:306 - Step: 265, Training loss: 0.13999587297439575
2025-03-24 16:13:25,557 - father_agent.py:306 - Step: 270, Training loss: 0.4127138555049896
2025-03-24 16:13:27,009 - father_agent.py:306 - Step: 275, Training loss: 0.5543404817581177
2025-03-24 16:13:28,262 - father_agent.py:306 - Step: 280, Training loss: 0.6230499744415283
2025-03-24 16:13:29,497 - father_agent.py:306 - Step: 285, Training loss: 0.6179530620574951
2025-03-24 16:13:30,895 - father_agent.py:306 - Step: 290, Training loss: 0.46222156286239624
2025-03-24 16:13:32,106 - father_agent.py:306 - Step: 295, Training loss: 0.3346138596534729
2025-03-24 16:13:33,345 - father_agent.py:306 - Step: 300, Training loss: 0.6067637205123901
2025-03-24 16:13:33,392 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:13:42,586 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:13:42,586 - father_agent.py:564 - Average Virtual Goal Value = 2.111603021621704
2025-03-24 16:13:42,586 - father_agent.py:566 - Goal Reach Probability = 0.0422320586314491
2025-03-24 16:13:42,586 - father_agent.py:568 - Trap Reach Probability = 0.9577679413685509
2025-03-24 16:13:42,586 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:13:42,586 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:13:42,586 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:13:42,624 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:13:44,486 - father_agent.py:306 - Step: 305, Training loss: 0.9086745381355286
2025-03-24 16:13:45,661 - father_agent.py:306 - Step: 310, Training loss: 0.510152280330658
2025-03-24 16:13:46,966 - father_agent.py:306 - Step: 315, Training loss: 1.3645288944244385
2025-03-24 16:13:48,249 - father_agent.py:306 - Step: 320, Training loss: 1.0702766180038452
2025-03-24 16:13:49,455 - father_agent.py:306 - Step: 325, Training loss: 0.9143567681312561
2025-03-24 16:13:50,720 - father_agent.py:306 - Step: 330, Training loss: 1.2649641036987305
2025-03-24 16:13:51,954 - father_agent.py:306 - Step: 335, Training loss: 1.1060302257537842
2025-03-24 16:13:53,089 - father_agent.py:306 - Step: 340, Training loss: 1.1723754405975342
2025-03-24 16:13:54,339 - father_agent.py:306 - Step: 345, Training loss: 1.2173110246658325
2025-03-24 16:13:55,614 - father_agent.py:306 - Step: 350, Training loss: 0.9926084280014038
2025-03-24 16:13:56,910 - father_agent.py:306 - Step: 355, Training loss: 1.4055125713348389
2025-03-24 16:13:58,068 - father_agent.py:306 - Step: 360, Training loss: 0.8994984030723572
2025-03-24 16:13:59,282 - father_agent.py:306 - Step: 365, Training loss: 0.8964186906814575
2025-03-24 16:14:00,615 - father_agent.py:306 - Step: 370, Training loss: 0.8492507934570312
2025-03-24 16:14:01,776 - father_agent.py:306 - Step: 375, Training loss: 1.1971616744995117
2025-03-24 16:14:03,017 - father_agent.py:306 - Step: 380, Training loss: 1.0190240144729614
2025-03-24 16:14:04,311 - father_agent.py:306 - Step: 385, Training loss: 1.0811562538146973
2025-03-24 16:14:05,493 - father_agent.py:306 - Step: 390, Training loss: 1.2868751287460327
2025-03-24 16:14:06,792 - father_agent.py:306 - Step: 395, Training loss: 0.7941079139709473
2025-03-24 16:14:08,095 - father_agent.py:306 - Step: 400, Training loss: 0.8974533677101135
2025-03-24 16:14:08,440 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:14:16,890 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:14:16,890 - father_agent.py:564 - Average Virtual Goal Value = 3.8334901332855225
2025-03-24 16:14:16,890 - father_agent.py:566 - Goal Reach Probability = 0.0766698024459078
2025-03-24 16:14:16,890 - father_agent.py:568 - Trap Reach Probability = 0.9233301975540922
2025-03-24 16:14:16,890 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:14:16,890 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:14:16,890 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:14:16,928 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:14:18,809 - father_agent.py:306 - Step: 405, Training loss: 0.9599480628967285
2025-03-24 16:14:20,029 - father_agent.py:306 - Step: 410, Training loss: 0.8811894655227661
2025-03-24 16:14:21,265 - father_agent.py:306 - Step: 415, Training loss: 1.0142593383789062
2025-03-24 16:14:22,532 - father_agent.py:306 - Step: 420, Training loss: 0.815741777420044
2025-03-24 16:14:23,692 - father_agent.py:306 - Step: 425, Training loss: 1.3530635833740234
2025-03-24 16:14:25,075 - father_agent.py:306 - Step: 430, Training loss: 0.9350849986076355
2025-03-24 16:14:26,345 - father_agent.py:306 - Step: 435, Training loss: 0.79387366771698
2025-03-24 16:14:27,504 - father_agent.py:306 - Step: 440, Training loss: 0.6325904726982117
2025-03-24 16:14:28,796 - father_agent.py:306 - Step: 445, Training loss: 0.7575817704200745
2025-03-24 16:14:30,080 - father_agent.py:306 - Step: 450, Training loss: 0.8309462666511536
2025-03-24 16:14:31,342 - father_agent.py:306 - Step: 455, Training loss: 1.0432255268096924
2025-03-24 16:14:32,652 - father_agent.py:306 - Step: 460, Training loss: 0.8064655661582947
2025-03-24 16:14:33,916 - father_agent.py:306 - Step: 465, Training loss: 0.8182759881019592
2025-03-24 16:14:35,088 - father_agent.py:306 - Step: 470, Training loss: 1.039068341255188
2025-03-24 16:14:36,371 - father_agent.py:306 - Step: 475, Training loss: 0.8907886743545532
2025-03-24 16:14:37,636 - father_agent.py:306 - Step: 480, Training loss: 0.6561316251754761
2025-03-24 16:14:38,844 - father_agent.py:306 - Step: 485, Training loss: 0.8803111910820007
2025-03-24 16:14:40,193 - father_agent.py:306 - Step: 490, Training loss: 0.7113399505615234
2025-03-24 16:14:41,782 - father_agent.py:306 - Step: 495, Training loss: 0.9210786819458008
2025-03-24 16:14:43,456 - father_agent.py:306 - Step: 500, Training loss: 0.9308991432189941
2025-03-24 16:14:43,494 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:14:52,810 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:14:52,810 - father_agent.py:564 - Average Virtual Goal Value = 3.321147918701172
2025-03-24 16:14:52,810 - father_agent.py:566 - Goal Reach Probability = 0.06642296025511395
2025-03-24 16:14:52,810 - father_agent.py:568 - Trap Reach Probability = 0.933577039744886
2025-03-24 16:14:52,810 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:14:52,810 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:14:52,810 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:14:52,848 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:14:54,888 - father_agent.py:306 - Step: 505, Training loss: 0.74036705493927
2025-03-24 16:14:56,092 - father_agent.py:306 - Step: 510, Training loss: 0.9864141941070557
2025-03-24 16:14:57,272 - father_agent.py:306 - Step: 515, Training loss: 0.8140074014663696
2025-03-24 16:14:58,624 - father_agent.py:306 - Step: 520, Training loss: 0.5747342705726624
2025-03-24 16:14:59,815 - father_agent.py:306 - Step: 525, Training loss: 1.044766902923584
2025-03-24 16:15:01,138 - father_agent.py:306 - Step: 530, Training loss: 0.6671435832977295
2025-03-24 16:15:02,405 - father_agent.py:306 - Step: 535, Training loss: 0.7161047458648682
2025-03-24 16:15:03,586 - father_agent.py:306 - Step: 540, Training loss: 0.9182522296905518
2025-03-24 16:15:04,890 - father_agent.py:306 - Step: 545, Training loss: 0.9370678067207336
2025-03-24 16:15:06,164 - father_agent.py:306 - Step: 550, Training loss: 0.7797011733055115
2025-03-24 16:15:07,348 - father_agent.py:306 - Step: 555, Training loss: 0.828319787979126
2025-03-24 16:15:08,626 - father_agent.py:306 - Step: 560, Training loss: 0.8273821473121643
2025-03-24 16:15:09,897 - father_agent.py:306 - Step: 565, Training loss: 0.8440039753913879
2025-03-24 16:15:11,221 - father_agent.py:306 - Step: 570, Training loss: 0.9409542083740234
2025-03-24 16:15:12,402 - father_agent.py:306 - Step: 575, Training loss: 0.8887814283370972
2025-03-24 16:15:13,640 - father_agent.py:306 - Step: 580, Training loss: 0.8349506855010986
2025-03-24 16:15:14,972 - father_agent.py:306 - Step: 585, Training loss: 0.7150420546531677
2025-03-24 16:15:16,282 - father_agent.py:306 - Step: 590, Training loss: 0.9726220369338989
2025-03-24 16:15:17,757 - father_agent.py:306 - Step: 595, Training loss: 0.8129556179046631
2025-03-24 16:15:19,401 - father_agent.py:306 - Step: 600, Training loss: 0.8906133770942688
2025-03-24 16:15:19,439 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:15:27,950 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:15:27,950 - father_agent.py:564 - Average Virtual Goal Value = 3.1389272212982178
2025-03-24 16:15:27,950 - father_agent.py:566 - Goal Reach Probability = 0.0627785461810358
2025-03-24 16:15:27,950 - father_agent.py:568 - Trap Reach Probability = 0.9372214538189642
2025-03-24 16:15:27,950 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:15:27,950 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:15:27,950 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:15:27,988 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:15:29,935 - father_agent.py:306 - Step: 605, Training loss: 0.741843044757843
2025-03-24 16:15:31,273 - father_agent.py:306 - Step: 610, Training loss: 0.6135911345481873
2025-03-24 16:15:32,558 - father_agent.py:306 - Step: 615, Training loss: 0.9889000654220581
2025-03-24 16:15:33,830 - father_agent.py:306 - Step: 620, Training loss: 0.90367192029953
2025-03-24 16:15:35,146 - father_agent.py:306 - Step: 625, Training loss: 1.0356216430664062
2025-03-24 16:15:36,455 - father_agent.py:306 - Step: 630, Training loss: 0.8748309016227722
2025-03-24 16:15:37,736 - father_agent.py:306 - Step: 635, Training loss: 0.7949267029762268
2025-03-24 16:15:38,918 - father_agent.py:306 - Step: 640, Training loss: 0.8755604028701782
2025-03-24 16:15:40,206 - father_agent.py:306 - Step: 645, Training loss: 0.6357738971710205
2025-03-24 16:15:41,526 - father_agent.py:306 - Step: 650, Training loss: 0.9467025995254517
2025-03-24 16:15:42,837 - father_agent.py:306 - Step: 655, Training loss: 0.7021029591560364
2025-03-24 16:15:44,099 - father_agent.py:306 - Step: 660, Training loss: 0.6764037609100342
2025-03-24 16:15:45,385 - father_agent.py:306 - Step: 665, Training loss: 0.6400055289268494
2025-03-24 16:15:46,576 - father_agent.py:306 - Step: 670, Training loss: 0.7702509164810181
2025-03-24 16:15:47,783 - father_agent.py:306 - Step: 675, Training loss: 0.9318185448646545
2025-03-24 16:15:49,135 - father_agent.py:306 - Step: 680, Training loss: 0.6852850914001465
2025-03-24 16:15:50,339 - father_agent.py:306 - Step: 685, Training loss: 0.8198258280754089
2025-03-24 16:15:51,538 - father_agent.py:306 - Step: 690, Training loss: 0.816504180431366
2025-03-24 16:15:52,905 - father_agent.py:306 - Step: 695, Training loss: 0.6993752717971802
2025-03-24 16:15:54,174 - father_agent.py:306 - Step: 700, Training loss: 0.717450737953186
2025-03-24 16:15:54,444 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:16:03,090 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:16:03,090 - father_agent.py:564 - Average Virtual Goal Value = 3.0212767124176025
2025-03-24 16:16:03,090 - father_agent.py:566 - Goal Reach Probability = 0.06042553191489362
2025-03-24 16:16:03,090 - father_agent.py:568 - Trap Reach Probability = 0.9395744680851064
2025-03-24 16:16:03,090 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:16:03,090 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:16:03,090 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:16:03,124 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:16:03,221 - father_agent.py:455 - Training finished.
2025-03-24 16:16:03,227 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:16:03,317 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 16:16:12,558 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:16:12,558 - father_agent.py:564 - Average Virtual Goal Value = 2.9833953380584717
2025-03-24 16:16:12,558 - father_agent.py:566 - Goal Reach Probability = 0.05966790813159528
2025-03-24 16:16:12,558 - father_agent.py:568 - Trap Reach Probability = 0.9403320918684047
2025-03-24 16:16:12,558 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:16:12,558 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:16:12,558 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:16:12,603 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:16:28,571 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:16:28,571 - evaluators.py:130 - Average Virtual Goal Value = 5.419373512268066
2025-03-24 16:16:28,571 - evaluators.py:132 - Goal Reach Probability = 0.10838746578420558
2025-03-24 16:16:28,571 - evaluators.py:134 - Trap Reach Probability = 0.8916125342157944
2025-03-24 16:16:28,571 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:16:28,571 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:16:28,571 - evaluators.py:140 - Current Best Reach Probability = 0.10838746578420558
2025-03-24 16:16:28,588 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:17:07,012 - cloned_fsc_actor_policy.py:189 - Epoch 0, Loss: 1.9487
2025-03-24 16:17:07,012 - cloned_fsc_actor_policy.py:190 - Epoch 0, Accuracy: 0.0200
2025-03-24 16:17:07,050 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:17:23,686 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:17:23,687 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:17:23,687 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:17:23,687 - evaluators.py:134 - Trap Reach Probability = 0.9897066392177046
2025-03-24 16:17:23,687 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:17:23,687 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:17:23,687 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:17:27,155 - cloned_fsc_actor_policy.py:189 - Epoch 1000, Loss: 1.0800
2025-03-24 16:17:27,155 - cloned_fsc_actor_policy.py:190 - Epoch 1000, Accuracy: 0.5639
2025-03-24 16:17:30,535 - cloned_fsc_actor_policy.py:189 - Epoch 2000, Loss: 0.5337
2025-03-24 16:17:30,535 - cloned_fsc_actor_policy.py:190 - Epoch 2000, Accuracy: 0.7511
2025-03-24 16:17:33,981 - cloned_fsc_actor_policy.py:189 - Epoch 3000, Loss: 0.4041
2025-03-24 16:17:33,981 - cloned_fsc_actor_policy.py:190 - Epoch 3000, Accuracy: 0.8341
2025-03-24 16:17:37,373 - cloned_fsc_actor_policy.py:189 - Epoch 4000, Loss: 0.3486
2025-03-24 16:17:37,373 - cloned_fsc_actor_policy.py:190 - Epoch 4000, Accuracy: 0.8416
2025-03-24 16:17:40,804 - cloned_fsc_actor_policy.py:189 - Epoch 5000, Loss: 0.3117
2025-03-24 16:17:40,804 - cloned_fsc_actor_policy.py:190 - Epoch 5000, Accuracy: 0.8435
2025-03-24 16:17:40,839 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:17:57,567 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:17:57,568 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:17:57,568 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:17:57,568 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:17:57,568 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:17:57,568 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:17:57,568 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:18:00,976 - cloned_fsc_actor_policy.py:189 - Epoch 6000, Loss: 0.2858
2025-03-24 16:18:00,976 - cloned_fsc_actor_policy.py:190 - Epoch 6000, Accuracy: 0.8494
2025-03-24 16:18:04,339 - cloned_fsc_actor_policy.py:189 - Epoch 7000, Loss: 0.2586
2025-03-24 16:18:04,339 - cloned_fsc_actor_policy.py:190 - Epoch 7000, Accuracy: 0.8568
2025-03-24 16:18:07,653 - cloned_fsc_actor_policy.py:189 - Epoch 8000, Loss: 0.2277
2025-03-24 16:18:07,653 - cloned_fsc_actor_policy.py:190 - Epoch 8000, Accuracy: 0.8791
2025-03-24 16:18:11,118 - cloned_fsc_actor_policy.py:189 - Epoch 9000, Loss: 0.2111
2025-03-24 16:18:11,119 - cloned_fsc_actor_policy.py:190 - Epoch 9000, Accuracy: 0.8810
2025-03-24 16:18:14,501 - cloned_fsc_actor_policy.py:189 - Epoch 10000, Loss: 0.2041
2025-03-24 16:18:14,501 - cloned_fsc_actor_policy.py:190 - Epoch 10000, Accuracy: 0.8822
2025-03-24 16:18:14,534 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:18:31,308 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:18:31,308 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:18:31,308 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:18:31,308 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:18:31,308 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:18:31,308 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:18:31,308 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:18:34,678 - cloned_fsc_actor_policy.py:189 - Epoch 11000, Loss: 0.1997
2025-03-24 16:18:34,678 - cloned_fsc_actor_policy.py:190 - Epoch 11000, Accuracy: 0.8829
2025-03-24 16:18:38,054 - cloned_fsc_actor_policy.py:189 - Epoch 12000, Loss: 0.1948
2025-03-24 16:18:38,054 - cloned_fsc_actor_policy.py:190 - Epoch 12000, Accuracy: 0.8836
2025-03-24 16:18:41,539 - cloned_fsc_actor_policy.py:189 - Epoch 13000, Loss: 0.1914
2025-03-24 16:18:41,539 - cloned_fsc_actor_policy.py:190 - Epoch 13000, Accuracy: 0.8863
2025-03-24 16:18:45,255 - cloned_fsc_actor_policy.py:189 - Epoch 14000, Loss: 0.1854
2025-03-24 16:18:45,255 - cloned_fsc_actor_policy.py:190 - Epoch 14000, Accuracy: 0.8921
2025-03-24 16:18:48,753 - cloned_fsc_actor_policy.py:189 - Epoch 15000, Loss: 0.1773
2025-03-24 16:18:48,753 - cloned_fsc_actor_policy.py:190 - Epoch 15000, Accuracy: 0.9063
2025-03-24 16:18:48,785 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:19:06,054 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:19:06,054 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:19:06,054 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:19:06,054 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:19:06,054 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:19:06,054 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:19:06,054 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:19:09,404 - cloned_fsc_actor_policy.py:189 - Epoch 16000, Loss: 0.1720
2025-03-24 16:19:09,404 - cloned_fsc_actor_policy.py:190 - Epoch 16000, Accuracy: 0.9119
2025-03-24 16:19:12,976 - cloned_fsc_actor_policy.py:189 - Epoch 17000, Loss: 0.1693
2025-03-24 16:19:12,976 - cloned_fsc_actor_policy.py:190 - Epoch 17000, Accuracy: 0.9156
2025-03-24 16:19:16,430 - cloned_fsc_actor_policy.py:189 - Epoch 18000, Loss: 0.1686
2025-03-24 16:19:16,430 - cloned_fsc_actor_policy.py:190 - Epoch 18000, Accuracy: 0.9166
2025-03-24 16:19:19,749 - cloned_fsc_actor_policy.py:189 - Epoch 19000, Loss: 0.1689
2025-03-24 16:19:19,749 - cloned_fsc_actor_policy.py:190 - Epoch 19000, Accuracy: 0.9169
2025-03-24 16:19:23,156 - cloned_fsc_actor_policy.py:189 - Epoch 20000, Loss: 0.1682
2025-03-24 16:19:23,157 - cloned_fsc_actor_policy.py:190 - Epoch 20000, Accuracy: 0.9175
2025-03-24 16:19:23,186 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:19:41,093 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:19:41,094 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:19:41,094 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:19:41,094 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:19:41,094 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:19:41,094 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:19:41,094 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:19:44,513 - cloned_fsc_actor_policy.py:189 - Epoch 21000, Loss: 0.1680
2025-03-24 16:19:44,513 - cloned_fsc_actor_policy.py:190 - Epoch 21000, Accuracy: 0.9176
2025-03-24 16:19:47,936 - cloned_fsc_actor_policy.py:189 - Epoch 22000, Loss: 0.1677
2025-03-24 16:19:47,937 - cloned_fsc_actor_policy.py:190 - Epoch 22000, Accuracy: 0.9179
2025-03-24 16:19:51,337 - cloned_fsc_actor_policy.py:189 - Epoch 23000, Loss: 0.1687
2025-03-24 16:19:51,337 - cloned_fsc_actor_policy.py:190 - Epoch 23000, Accuracy: 0.9172
2025-03-24 16:19:54,871 - cloned_fsc_actor_policy.py:189 - Epoch 24000, Loss: 0.1672
2025-03-24 16:19:54,871 - cloned_fsc_actor_policy.py:190 - Epoch 24000, Accuracy: 0.9180
2025-03-24 16:19:58,221 - cloned_fsc_actor_policy.py:189 - Epoch 25000, Loss: 0.1672
2025-03-24 16:19:58,221 - cloned_fsc_actor_policy.py:190 - Epoch 25000, Accuracy: 0.9178
2025-03-24 16:19:58,454 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:20:16,043 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:20:16,043 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:20:16,043 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:20:16,043 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:20:16,043 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:20:16,043 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:20:16,043 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:20:19,481 - cloned_fsc_actor_policy.py:189 - Epoch 26000, Loss: 0.1676
2025-03-24 16:20:19,481 - cloned_fsc_actor_policy.py:190 - Epoch 26000, Accuracy: 0.9176
2025-03-24 16:20:22,930 - cloned_fsc_actor_policy.py:189 - Epoch 27000, Loss: 0.1678
2025-03-24 16:20:22,930 - cloned_fsc_actor_policy.py:190 - Epoch 27000, Accuracy: 0.9174
2025-03-24 16:20:26,485 - cloned_fsc_actor_policy.py:189 - Epoch 28000, Loss: 0.1684
2025-03-24 16:20:26,486 - cloned_fsc_actor_policy.py:190 - Epoch 28000, Accuracy: 0.9172
2025-03-24 16:20:29,829 - cloned_fsc_actor_policy.py:189 - Epoch 29000, Loss: 0.1672
2025-03-24 16:20:29,830 - cloned_fsc_actor_policy.py:190 - Epoch 29000, Accuracy: 0.9180
2025-03-24 16:20:33,254 - cloned_fsc_actor_policy.py:189 - Epoch 30000, Loss: 0.1682
2025-03-24 16:20:33,254 - cloned_fsc_actor_policy.py:190 - Epoch 30000, Accuracy: 0.9170
2025-03-24 16:20:33,285 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:20:50,273 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:20:50,274 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:20:50,274 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:20:50,274 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:20:50,274 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:20:50,274 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:20:50,274 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:20:54,249 - cloned_fsc_actor_policy.py:189 - Epoch 31000, Loss: 0.1682
2025-03-24 16:20:54,249 - cloned_fsc_actor_policy.py:190 - Epoch 31000, Accuracy: 0.9172
2025-03-24 16:20:58,097 - cloned_fsc_actor_policy.py:189 - Epoch 32000, Loss: 0.1672
2025-03-24 16:20:58,098 - cloned_fsc_actor_policy.py:190 - Epoch 32000, Accuracy: 0.9176
2025-03-24 16:21:01,538 - cloned_fsc_actor_policy.py:189 - Epoch 33000, Loss: 0.1674
2025-03-24 16:21:01,539 - cloned_fsc_actor_policy.py:190 - Epoch 33000, Accuracy: 0.9176
2025-03-24 16:21:04,952 - cloned_fsc_actor_policy.py:189 - Epoch 34000, Loss: 0.1680
2025-03-24 16:21:04,952 - cloned_fsc_actor_policy.py:190 - Epoch 34000, Accuracy: 0.9172
2025-03-24 16:21:08,437 - cloned_fsc_actor_policy.py:189 - Epoch 35000, Loss: 0.1675
2025-03-24 16:21:08,437 - cloned_fsc_actor_policy.py:190 - Epoch 35000, Accuracy: 0.9174
2025-03-24 16:21:08,491 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:21:25,184 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:21:25,184 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:21:25,184 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:21:25,184 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:21:25,184 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:21:25,184 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:21:25,184 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:21:28,700 - cloned_fsc_actor_policy.py:189 - Epoch 36000, Loss: 0.1680
2025-03-24 16:21:28,700 - cloned_fsc_actor_policy.py:190 - Epoch 36000, Accuracy: 0.9171
2025-03-24 16:21:32,962 - cloned_fsc_actor_policy.py:189 - Epoch 37000, Loss: 0.1673
2025-03-24 16:21:32,962 - cloned_fsc_actor_policy.py:190 - Epoch 37000, Accuracy: 0.9176
2025-03-24 16:21:36,343 - cloned_fsc_actor_policy.py:189 - Epoch 38000, Loss: 0.1675
2025-03-24 16:21:36,343 - cloned_fsc_actor_policy.py:190 - Epoch 38000, Accuracy: 0.9174
2025-03-24 16:21:39,746 - cloned_fsc_actor_policy.py:189 - Epoch 39000, Loss: 0.1675
2025-03-24 16:21:39,746 - cloned_fsc_actor_policy.py:190 - Epoch 39000, Accuracy: 0.9175
2025-03-24 16:21:43,119 - cloned_fsc_actor_policy.py:189 - Epoch 40000, Loss: 0.1672
2025-03-24 16:21:43,120 - cloned_fsc_actor_policy.py:190 - Epoch 40000, Accuracy: 0.9180
2025-03-24 16:21:43,153 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:22:00,019 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:22:00,019 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:22:00,019 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:22:00,019 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:22:00,019 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:22:00,019 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:22:00,019 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:22:03,409 - cloned_fsc_actor_policy.py:189 - Epoch 41000, Loss: 0.1670
2025-03-24 16:22:03,409 - cloned_fsc_actor_policy.py:190 - Epoch 41000, Accuracy: 0.9179
2025-03-24 16:22:06,818 - cloned_fsc_actor_policy.py:189 - Epoch 42000, Loss: 0.1679
2025-03-24 16:22:06,818 - cloned_fsc_actor_policy.py:190 - Epoch 42000, Accuracy: 0.9171
2025-03-24 16:22:10,168 - cloned_fsc_actor_policy.py:189 - Epoch 43000, Loss: 0.1675
2025-03-24 16:22:10,168 - cloned_fsc_actor_policy.py:190 - Epoch 43000, Accuracy: 0.9176
2025-03-24 16:22:13,506 - cloned_fsc_actor_policy.py:189 - Epoch 44000, Loss: 0.1670
2025-03-24 16:22:13,506 - cloned_fsc_actor_policy.py:190 - Epoch 44000, Accuracy: 0.9178
2025-03-24 16:22:16,974 - cloned_fsc_actor_policy.py:189 - Epoch 45000, Loss: 0.1671
2025-03-24 16:22:16,974 - cloned_fsc_actor_policy.py:190 - Epoch 45000, Accuracy: 0.9179
2025-03-24 16:22:17,009 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:22:33,489 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:22:33,489 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:22:33,489 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:22:33,489 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:22:33,489 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:22:33,489 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:22:33,489 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:22:36,896 - cloned_fsc_actor_policy.py:189 - Epoch 46000, Loss: 0.1674
2025-03-24 16:22:36,896 - cloned_fsc_actor_policy.py:190 - Epoch 46000, Accuracy: 0.9176
2025-03-24 16:22:40,246 - cloned_fsc_actor_policy.py:189 - Epoch 47000, Loss: 0.1674
2025-03-24 16:22:40,246 - cloned_fsc_actor_policy.py:190 - Epoch 47000, Accuracy: 0.9176
2025-03-24 16:22:43,598 - cloned_fsc_actor_policy.py:189 - Epoch 48000, Loss: 0.1662
2025-03-24 16:22:43,599 - cloned_fsc_actor_policy.py:190 - Epoch 48000, Accuracy: 0.9182
2025-03-24 16:22:47,066 - cloned_fsc_actor_policy.py:189 - Epoch 49000, Loss: 0.1671
2025-03-24 16:22:47,066 - cloned_fsc_actor_policy.py:190 - Epoch 49000, Accuracy: 0.9179
2025-03-24 16:22:50,494 - cloned_fsc_actor_policy.py:189 - Epoch 50000, Loss: 0.1667
2025-03-24 16:22:50,494 - cloned_fsc_actor_policy.py:190 - Epoch 50000, Accuracy: 0.9181
2025-03-24 16:22:50,526 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:23:08,197 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:23:08,198 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:23:08,198 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:23:08,198 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:23:08,198 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:23:08,198 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:23:08,198 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:23:11,604 - cloned_fsc_actor_policy.py:189 - Epoch 51000, Loss: 0.1673
2025-03-24 16:23:11,604 - cloned_fsc_actor_policy.py:190 - Epoch 51000, Accuracy: 0.9178
2025-03-24 16:23:15,010 - cloned_fsc_actor_policy.py:189 - Epoch 52000, Loss: 0.1665
2025-03-24 16:23:15,010 - cloned_fsc_actor_policy.py:190 - Epoch 52000, Accuracy: 0.9183
2025-03-24 16:23:18,501 - cloned_fsc_actor_policy.py:189 - Epoch 53000, Loss: 0.1667
2025-03-24 16:23:18,501 - cloned_fsc_actor_policy.py:190 - Epoch 53000, Accuracy: 0.9179
2025-03-24 16:23:21,899 - cloned_fsc_actor_policy.py:189 - Epoch 54000, Loss: 0.1673
2025-03-24 16:23:21,900 - cloned_fsc_actor_policy.py:190 - Epoch 54000, Accuracy: 0.9176
2025-03-24 16:23:25,266 - cloned_fsc_actor_policy.py:189 - Epoch 55000, Loss: 0.1671
2025-03-24 16:23:25,266 - cloned_fsc_actor_policy.py:190 - Epoch 55000, Accuracy: 0.9175
2025-03-24 16:23:25,298 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:23:43,448 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:23:43,448 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:23:43,448 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:23:43,448 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:23:43,448 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:23:43,448 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:23:43,448 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:23:46,847 - cloned_fsc_actor_policy.py:189 - Epoch 56000, Loss: 0.1669
2025-03-24 16:23:46,847 - cloned_fsc_actor_policy.py:190 - Epoch 56000, Accuracy: 0.9179
2025-03-24 16:23:50,294 - cloned_fsc_actor_policy.py:189 - Epoch 57000, Loss: 0.1671
2025-03-24 16:23:50,294 - cloned_fsc_actor_policy.py:190 - Epoch 57000, Accuracy: 0.9178
2025-03-24 16:23:53,684 - cloned_fsc_actor_policy.py:189 - Epoch 58000, Loss: 0.1669
2025-03-24 16:23:53,685 - cloned_fsc_actor_policy.py:190 - Epoch 58000, Accuracy: 0.9179
2025-03-24 16:23:57,093 - cloned_fsc_actor_policy.py:189 - Epoch 59000, Loss: 0.1670
2025-03-24 16:23:57,093 - cloned_fsc_actor_policy.py:190 - Epoch 59000, Accuracy: 0.9179
2025-03-24 16:24:00,466 - cloned_fsc_actor_policy.py:189 - Epoch 60000, Loss: 0.1669
2025-03-24 16:24:00,466 - cloned_fsc_actor_policy.py:190 - Epoch 60000, Accuracy: 0.9181
2025-03-24 16:24:00,500 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:24:16,976 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:24:16,976 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:24:16,976 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:24:16,976 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:24:16,976 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:24:16,976 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:24:16,976 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:24:17,812 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:24:34,694 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:24:34,694 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:24:34,694 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:24:34,694 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:24:34,694 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:24:34,694 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:24:34,694 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:24:35,301 - pomdp.py:349 - unfolding 5-FSC template into POMDP...
2025-03-24 16:24:36,088 - pomdp.py:355 - constructed quotient MDP having 33898 states and 617106 actions.
2025-03-24 16:24:37,166 - rl_family_extractor.py:329 - Building family from FFNN...
2025-03-24 16:24:37,248 - rl_family_extractor.py:357 - Number of misses: 1 out of 1000
2025-03-24 16:24:37,248 - rl_family_extractor.py:359 - Number of complete misses: 1 out of 1000
2025-03-24 16:24:37,249 - statistic.py:67 - synthesis initiated, design space: 4
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 0.04 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 100 %
DTMC stats: avg DTMC size: 383, iterations: 4

optimum: 0.140482
--------------------
2025-03-24 16:24:37,293 - statistic.py:67 - synthesis initiated, design space: 1e609
> progress 0.0%, elapsed 3 s, estimated 3004223 s (34 days), iters = {DTMC: 323}, opt = 0.1405
> progress 0.0%, elapsed 6 s, estimated 6008797 s (69 days), iters = {DTMC: 705}, opt = 0.1405
> progress 0.0%, elapsed 9 s, estimated 9010671 s (104 days), iters = {DTMC: 1077}, opt = 0.1405
> progress 0.0%, elapsed 12 s, estimated 12017092 s (139 days), iters = {DTMC: 1460}, opt = 0.1405
> progress 0.0%, elapsed 15 s, estimated 15017746 s (173 days), iters = {DTMC: 1835}, opt = 0.1405
> progress 0.0%, elapsed 18 s, estimated 18023126 s (208 days), iters = {DTMC: 2214}, opt = 0.1405
> progress 0.0%, elapsed 21 s, estimated 21027278 s (243 days), iters = {DTMC: 2595}, opt = 0.1405
> progress 0.0%, elapsed 24 s, estimated 24034181 s (278 days), iters = {DTMC: 2963}, opt = 0.1405
> progress 0.0%, elapsed 27 s, estimated 27042502 s (312 days), iters = {DTMC: 3306}, opt = 0.1405
> progress 0.0%, elapsed 30 s, estimated 30046108 s (347 days), iters = {DTMC: 3689}, opt = 0.1405
> progress 0.0%, elapsed 33 s, estimated 33052006 s (1 year), iters = {DTMC: 4069}, opt = 0.1405
> progress 0.0%, elapsed 36 s, estimated 36055018 s (1 year), iters = {DTMC: 4445}, opt = 0.1405
> progress 0.0%, elapsed 39 s, estimated 39056121 s (1 year), iters = {DTMC: 4823}, opt = 0.1405
> progress 0.0%, elapsed 42 s, estimated 42059957 s (1 year), iters = {DTMC: 5199}, opt = 0.1405
> progress 0.0%, elapsed 45 s, estimated 45060291 s (1 year), iters = {DTMC: 5582}, opt = 0.1405
> progress 0.0%, elapsed 48 s, estimated 48064098 s (1 year), iters = {DTMC: 5901}, opt = 0.1405
> progress 0.0%, elapsed 51 s, estimated 51068425 s (1 year), iters = {DTMC: 6280}, opt = 0.1405
> progress 0.0%, elapsed 54 s, estimated 54071404 s (1 year), iters = {DTMC: 6658}, opt = 0.1405
> progress 0.0%, elapsed 57 s, estimated 57074849 s (1 year), iters = {DTMC: 7039}, opt = 0.1405
2025-03-24 16:25:37,301 - synthesizer_onebyone.py:19 - Time limit reached
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.01 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 0 %
DTMC stats: avg DTMC size: 212, iterations: 7411

optimum: 0.140482
--------------------
2025-03-24 16:25:37,302 - synthesizer_rl_storm_paynt.py:280 - No improving assignment found.
2025-03-24 16:25:37,303 - storm_pomdp_control.py:246 - Interactive Storm resumed
2025-03-24 16:25:37,303 - storm_pomdp_control.py:291 - Updating FSC values in Storm
2025-03-24 16:26:08,334 - storm_pomdp_control.py:305 - Pausing Storm
-----------Storm-----------               
Value = 0.23461992224252154 | Time elapsed = 4608.5s | FSC size = 1636

2025-03-24 16:27:19,705 - synthesizer_pomdp.py:253 - Timeout for PAYNT started
2025-03-24 16:27:20,158 - synthesizer_ar_storm.py:136 - Resuming synthesis
2025-03-24 16:27:20,167 - synthesizer_ar_storm.py:147 - PAYNT's value is better. Prioritizing synthesis results
2025-03-24 16:27:49,897 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 16:27:49,965 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:27:59,168 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:27:59,169 - father_agent.py:564 - Average Virtual Goal Value = 3.053938627243042
2025-03-24 16:27:59,169 - father_agent.py:566 - Goal Reach Probability = 0.06107877376794722
2025-03-24 16:27:59,169 - father_agent.py:568 - Trap Reach Probability = 0.9389212262320528
2025-03-24 16:27:59,169 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:27:59,169 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:27:59,169 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:28:03,557 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:28:08,084 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:28:08,085 - evaluators.py:130 - Average Virtual Goal Value = 0.010115314275026321
2025-03-24 16:28:08,085 - evaluators.py:132 - Goal Reach Probability = 0.00020230629172567267
2025-03-24 16:28:08,085 - evaluators.py:134 - Trap Reach Probability = 0.9997976937082743
2025-03-24 16:28:08,085 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:28:08,085 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:28:08,085 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:28:08,085 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 2.1864230632781982
Epoch: 0, Critic Loss: 19.26367950439453
Epoch: 10, Actor Loss: 0.06433942168951035
Epoch: 10, Critic Loss: 9.894731521606445
Epoch: 20, Actor Loss: 0.029801929369568825
Epoch: 20, Critic Loss: 14.593172073364258
Epoch: 30, Actor Loss: 0.0313604474067688
Epoch: 30, Critic Loss: 5.050868988037109
Epoch: 40, Actor Loss: 0.024992704391479492
Epoch: 40, Critic Loss: 12.815431594848633
2025-03-24 16:28:41,469 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:28:45,302 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:28:45,302 - evaluators.py:130 - Average Virtual Goal Value = 9.245830535888672
2025-03-24 16:28:45,302 - evaluators.py:132 - Goal Reach Probability = 0.18491660623640319
2025-03-24 16:28:45,302 - evaluators.py:134 - Trap Reach Probability = 0.8150833937635968
2025-03-24 16:28:45,302 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:28:45,302 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:28:45,302 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:28:45,302 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.029856815934181213
Epoch: 50, Critic Loss: 6.477583885192871
Epoch: 60, Actor Loss: 0.025995109230279922
Epoch: 60, Critic Loss: 8.832342147827148
Epoch: 70, Actor Loss: 0.03146304935216904
Epoch: 70, Critic Loss: 9.939607620239258
Epoch: 80, Actor Loss: 0.029123526066541672
Epoch: 80, Critic Loss: 12.148746490478516
Epoch: 90, Actor Loss: 0.0356517992913723
Epoch: 90, Critic Loss: 9.617887496948242
2025-03-24 16:29:19,527 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:29:23,068 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:29:23,068 - evaluators.py:130 - Average Virtual Goal Value = 9.571742057800293
2025-03-24 16:29:23,068 - evaluators.py:132 - Goal Reach Probability = 0.19143484626647145
2025-03-24 16:29:23,068 - evaluators.py:134 - Trap Reach Probability = 0.8085651537335286
2025-03-24 16:29:23,068 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:29:23,068 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:29:23,068 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:29:23,068 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.022522950544953346
Epoch: 100, Critic Loss: 8.073046684265137
Epoch: 110, Actor Loss: 0.045152001082897186
Epoch: 110, Critic Loss: 7.499596118927002
Epoch: 120, Actor Loss: 0.022371487691998482
Epoch: 120, Critic Loss: 10.350786209106445
Epoch: 130, Actor Loss: 0.044173210859298706
Epoch: 130, Critic Loss: 11.015645980834961
Epoch: 140, Actor Loss: 0.023656439036130905
Epoch: 140, Critic Loss: 8.199996948242188
2025-03-24 16:29:57,380 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:30:00,703 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:30:00,703 - evaluators.py:130 - Average Virtual Goal Value = 9.869612693786621
2025-03-24 16:30:00,703 - evaluators.py:132 - Goal Reach Probability = 0.19739224918507786
2025-03-24 16:30:00,703 - evaluators.py:134 - Trap Reach Probability = 0.8026077508149221
2025-03-24 16:30:00,703 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:30:00,703 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:30:00,703 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:30:00,703 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.02876024879515171
Epoch: 150, Critic Loss: 10.144196510314941
Epoch: 160, Actor Loss: 0.017781822010874748
Epoch: 160, Critic Loss: 12.72628402709961
Epoch: 170, Actor Loss: 0.0447327084839344
Epoch: 170, Critic Loss: 11.444147109985352
Epoch: 180, Actor Loss: 0.019477970898151398
Epoch: 180, Critic Loss: 14.581233978271484
Epoch: 190, Actor Loss: 0.042785149067640305
Epoch: 190, Critic Loss: 14.758350372314453
2025-03-24 16:30:33,826 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:30:40,191 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:30:40,191 - evaluators.py:130 - Average Virtual Goal Value = 10.224719047546387
2025-03-24 16:30:40,191 - evaluators.py:132 - Goal Reach Probability = 0.20449438202247192
2025-03-24 16:30:40,191 - evaluators.py:134 - Trap Reach Probability = 0.7955056179775281
2025-03-24 16:30:40,191 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:30:40,191 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:30:40,191 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:30:40,191 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.03343038260936737
Epoch: 200, Critic Loss: 8.645062446594238
Epoch: 210, Actor Loss: 0.038030724972486496
Epoch: 210, Critic Loss: 10.889948844909668
Epoch: 220, Actor Loss: 0.015163916163146496
Epoch: 220, Critic Loss: 7.650547981262207
Epoch: 230, Actor Loss: 0.024298828095197678
Epoch: 230, Critic Loss: 9.844059944152832
Epoch: 240, Actor Loss: 0.019085746258497238
Epoch: 240, Critic Loss: 8.488117218017578
2025-03-24 16:31:13,010 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:31:16,840 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:31:16,840 - evaluators.py:130 - Average Virtual Goal Value = 9.90099048614502
2025-03-24 16:31:16,840 - evaluators.py:132 - Goal Reach Probability = 0.19801980198019803
2025-03-24 16:31:16,840 - evaluators.py:134 - Trap Reach Probability = 0.801980198019802
2025-03-24 16:31:16,840 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:31:16,840 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:31:16,840 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:31:16,840 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.024472178891301155
Epoch: 250, Critic Loss: 10.602293014526367
Epoch: 260, Actor Loss: 0.014622713439166546
Epoch: 260, Critic Loss: 12.41586971282959
Epoch: 270, Actor Loss: 0.028399424627423286
Epoch: 270, Critic Loss: 11.381101608276367
Epoch: 280, Actor Loss: 0.03293018788099289
Epoch: 280, Critic Loss: 3.6522903442382812
Epoch: 290, Actor Loss: 0.011747301556169987
Epoch: 290, Critic Loss: 7.848800182342529
2025-03-24 16:31:49,777 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:31:54,052 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:31:54,053 - evaluators.py:130 - Average Virtual Goal Value = 8.894143104553223
2025-03-24 16:31:54,053 - evaluators.py:132 - Goal Reach Probability = 0.17788286649690796
2025-03-24 16:31:54,053 - evaluators.py:134 - Trap Reach Probability = 0.822117133503092
2025-03-24 16:31:54,053 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:31:54,053 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:31:54,053 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:31:54,053 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.025221414864063263
Epoch: 300, Critic Loss: 13.436563491821289
Epoch: 310, Actor Loss: 0.04233172535896301
Epoch: 310, Critic Loss: 14.028557777404785
Epoch: 320, Actor Loss: 0.018751492723822594
Epoch: 320, Critic Loss: 5.033903121948242
Epoch: 330, Actor Loss: 0.015817372128367424
Epoch: 330, Critic Loss: 9.639628410339355
Epoch: 340, Actor Loss: 0.036909956485033035
Epoch: 340, Critic Loss: 14.42661190032959
2025-03-24 16:32:26,773 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:32:29,955 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:32:29,955 - evaluators.py:130 - Average Virtual Goal Value = 10.073260307312012
2025-03-24 16:32:29,956 - evaluators.py:132 - Goal Reach Probability = 0.20146520146520147
2025-03-24 16:32:29,956 - evaluators.py:134 - Trap Reach Probability = 0.7985347985347986
2025-03-24 16:32:29,956 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:32:29,956 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:32:29,956 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:32:29,956 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.02907181717455387
Epoch: 350, Critic Loss: 10.11097526550293
Epoch: 360, Actor Loss: 0.02342541702091694
Epoch: 360, Critic Loss: 6.131721496582031
Epoch: 370, Actor Loss: 0.030268728733062744
Epoch: 370, Critic Loss: 12.59596061706543
Epoch: 380, Actor Loss: 0.015837594866752625
Epoch: 380, Critic Loss: 9.407319068908691
Epoch: 390, Actor Loss: 0.021473564207553864
Epoch: 390, Critic Loss: 8.68429183959961
2025-03-24 16:33:03,015 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:33:06,146 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:33:06,146 - evaluators.py:130 - Average Virtual Goal Value = 10.839292526245117
2025-03-24 16:33:06,146 - evaluators.py:132 - Goal Reach Probability = 0.2167858487015431
2025-03-24 16:33:06,146 - evaluators.py:134 - Trap Reach Probability = 0.783214151298457
2025-03-24 16:33:06,146 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:33:06,146 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:33:06,146 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:33:06,146 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.03835548460483551
Epoch: 400, Critic Loss: 8.52800178527832
Epoch: 410, Actor Loss: 0.03421325609087944
Epoch: 410, Critic Loss: 5.148819923400879
Epoch: 420, Actor Loss: 0.017032576724886894
Epoch: 420, Critic Loss: 5.413656234741211
Epoch: 430, Actor Loss: 0.023530740290880203
Epoch: 430, Critic Loss: 9.751442909240723
Epoch: 440, Actor Loss: 0.02996106445789337
Epoch: 440, Critic Loss: 17.26754379272461
2025-03-24 16:33:39,229 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:33:42,518 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:33:42,518 - evaluators.py:130 - Average Virtual Goal Value = 9.467889785766602
2025-03-24 16:33:42,518 - evaluators.py:132 - Goal Reach Probability = 0.18935779816513762
2025-03-24 16:33:42,518 - evaluators.py:134 - Trap Reach Probability = 0.8106422018348624
2025-03-24 16:33:42,518 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:33:42,518 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:33:42,518 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:33:42,519 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.04013514518737793
Epoch: 450, Critic Loss: 14.068415641784668
Epoch: 460, Actor Loss: 0.027161354199051857
Epoch: 460, Critic Loss: 12.386859893798828
Epoch: 470, Actor Loss: 0.024222789332270622
Epoch: 470, Critic Loss: 7.921999454498291
Epoch: 480, Actor Loss: 0.019142096862196922
Epoch: 480, Critic Loss: 6.589792251586914
Epoch: 490, Actor Loss: 0.027804220095276833
Epoch: 490, Critic Loss: 9.425239562988281
2025-03-24 16:34:15,491 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:34:19,096 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:34:19,096 - evaluators.py:130 - Average Virtual Goal Value = 10.542398452758789
2025-03-24 16:34:19,096 - evaluators.py:132 - Goal Reach Probability = 0.2108479755538579
2025-03-24 16:34:19,096 - evaluators.py:134 - Trap Reach Probability = 0.7891520244461421
2025-03-24 16:34:19,096 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:34:19,096 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:34:19,096 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:34:19,096 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.025411037728190422
Epoch: 500, Critic Loss: 17.14609718322754
Epoch: 510, Actor Loss: 0.015910906717181206
Epoch: 510, Critic Loss: 11.557249069213867
Epoch: 520, Actor Loss: 0.0336981825530529
Epoch: 520, Critic Loss: 7.833335876464844
Epoch: 530, Actor Loss: 0.023731224238872528
Epoch: 530, Critic Loss: 15.899849891662598
Epoch: 540, Actor Loss: 0.02346394956111908
Epoch: 540, Critic Loss: 8.084256172180176
2025-03-24 16:34:51,850 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:34:55,311 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:34:55,311 - evaluators.py:130 - Average Virtual Goal Value = 10.476367950439453
2025-03-24 16:34:55,311 - evaluators.py:132 - Goal Reach Probability = 0.20952735392631186
2025-03-24 16:34:55,311 - evaluators.py:134 - Trap Reach Probability = 0.7904726460736882
2025-03-24 16:34:55,311 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:34:55,311 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:34:55,311 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:34:55,311 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.019971514120697975
Epoch: 550, Critic Loss: 12.529928207397461
Epoch: 560, Actor Loss: 0.020563466474413872
Epoch: 560, Critic Loss: 7.88395881652832
Epoch: 570, Actor Loss: 0.02361510507762432
Epoch: 570, Critic Loss: 6.676019191741943
Epoch: 580, Actor Loss: 0.018325980752706528
Epoch: 580, Critic Loss: 6.853672504425049
Epoch: 590, Actor Loss: 0.027220455929636955
Epoch: 590, Critic Loss: 9.593029975891113
2025-03-24 16:35:29,513 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:35:33,124 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:35:33,124 - evaluators.py:130 - Average Virtual Goal Value = 8.904847145080566
2025-03-24 16:35:33,124 - evaluators.py:132 - Goal Reach Probability = 0.17809694793536804
2025-03-24 16:35:33,124 - evaluators.py:134 - Trap Reach Probability = 0.821903052064632
2025-03-24 16:35:33,124 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:35:33,124 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:35:33,124 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:35:33,124 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.026741208508610725
Epoch: 600, Critic Loss: 11.804922103881836
Epoch: 610, Actor Loss: 0.010613343678414822
Epoch: 610, Critic Loss: 11.027490615844727
Epoch: 620, Actor Loss: 0.03621448203921318
Epoch: 620, Critic Loss: 9.356287002563477
Epoch: 630, Actor Loss: 0.049565400928258896
Epoch: 630, Critic Loss: 8.800992012023926
Epoch: 640, Actor Loss: 0.012226416729390621
Epoch: 640, Critic Loss: 9.634501457214355
2025-03-24 16:36:06,460 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:36:09,662 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:36:09,662 - evaluators.py:130 - Average Virtual Goal Value = 9.610437393188477
2025-03-24 16:36:09,662 - evaluators.py:132 - Goal Reach Probability = 0.1922087467842705
2025-03-24 16:36:09,662 - evaluators.py:134 - Trap Reach Probability = 0.8077912532157295
2025-03-24 16:36:09,662 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:36:09,662 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:36:09,662 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:36:09,662 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.024335786700248718
Epoch: 650, Critic Loss: 10.939458847045898
Epoch: 660, Actor Loss: 0.014864977449178696
Epoch: 660, Critic Loss: 14.505067825317383
Epoch: 670, Actor Loss: 0.02424551546573639
Epoch: 670, Critic Loss: 9.463186264038086
Epoch: 680, Actor Loss: 0.023319942876696587
Epoch: 680, Critic Loss: 17.543827056884766
Epoch: 690, Actor Loss: 0.027804842218756676
Epoch: 690, Critic Loss: 7.999725341796875
2025-03-24 16:36:42,552 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:36:45,745 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:36:45,745 - evaluators.py:130 - Average Virtual Goal Value = 10.813865661621094
2025-03-24 16:36:45,745 - evaluators.py:132 - Goal Reach Probability = 0.2162773172569706
2025-03-24 16:36:45,745 - evaluators.py:134 - Trap Reach Probability = 0.7837226827430294
2025-03-24 16:36:45,745 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:36:45,745 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:36:45,745 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:36:45,745 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.01458386518061161
Epoch: 700, Critic Loss: 10.096454620361328
2025-03-24 16:36:45,747 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 16:36:45,748 - father_agent.py:449 - Before training evaluation.
2025-03-24 16:36:45,748 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:36:53,462 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:36:53,462 - father_agent.py:564 - Average Virtual Goal Value = 10.404908180236816
2025-03-24 16:36:53,462 - father_agent.py:566 - Goal Reach Probability = 0.20809815950920246
2025-03-24 16:36:53,462 - father_agent.py:568 - Trap Reach Probability = 0.7919018404907976
2025-03-24 16:36:53,462 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:36:53,462 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:36:53,462 - father_agent.py:574 - Current Best Reach Probability = 0.2083181320685881
2025-03-24 16:36:53,501 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:36:53,603 - father_agent.py:359 - Training agent on-policy
2025-03-24 16:36:57,553 - father_agent.py:306 - Step: 0, Training loss: 95.15563201904297
2025-03-24 16:36:57,592 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:37:05,773 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:37:05,773 - father_agent.py:564 - Average Virtual Goal Value = 10.967302322387695
2025-03-24 16:37:05,773 - father_agent.py:566 - Goal Reach Probability = 0.21934604904632152
2025-03-24 16:37:05,773 - father_agent.py:568 - Trap Reach Probability = 0.7806539509536785
2025-03-24 16:37:05,773 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:37:05,773 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:37:05,773 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:37:05,816 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:37:08,011 - father_agent.py:306 - Step: 5, Training loss: 64.37385559082031
2025-03-24 16:37:09,111 - father_agent.py:306 - Step: 10, Training loss: 31.162769317626953
2025-03-24 16:37:10,291 - father_agent.py:306 - Step: 15, Training loss: 11.556986808776855
2025-03-24 16:37:11,456 - father_agent.py:306 - Step: 20, Training loss: 4.483834743499756
2025-03-24 16:37:12,649 - father_agent.py:306 - Step: 25, Training loss: 1.7938172817230225
2025-03-24 16:37:13,752 - father_agent.py:306 - Step: 30, Training loss: 1.7505217790603638
2025-03-24 16:37:15,073 - father_agent.py:306 - Step: 35, Training loss: 0.9096119403839111
2025-03-24 16:37:19,579 - father_agent.py:306 - Step: 40, Training loss: 0.6124523282051086
2025-03-24 16:37:20,814 - father_agent.py:306 - Step: 45, Training loss: 0.5395309925079346
2025-03-24 16:37:22,114 - father_agent.py:306 - Step: 50, Training loss: 0.5454785823822021
2025-03-24 16:37:23,530 - father_agent.py:306 - Step: 55, Training loss: 0.30364614725112915
2025-03-24 16:37:25,141 - father_agent.py:306 - Step: 60, Training loss: 0.525191068649292
2025-03-24 16:37:27,304 - father_agent.py:306 - Step: 65, Training loss: 0.24623779952526093
2025-03-24 16:37:28,768 - father_agent.py:306 - Step: 70, Training loss: 0.3752095401287079
2025-03-24 16:37:30,099 - father_agent.py:306 - Step: 75, Training loss: 0.20321378111839294
2025-03-24 16:37:31,446 - father_agent.py:306 - Step: 80, Training loss: 0.49864664673805237
2025-03-24 16:37:32,869 - father_agent.py:306 - Step: 85, Training loss: 0.24256449937820435
2025-03-24 16:37:34,288 - father_agent.py:306 - Step: 90, Training loss: 0.3028568625450134
2025-03-24 16:37:35,672 - father_agent.py:306 - Step: 95, Training loss: 0.4792027473449707
2025-03-24 16:37:37,057 - father_agent.py:306 - Step: 100, Training loss: 0.5269673466682434
2025-03-24 16:37:37,097 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:37:46,328 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:37:46,328 - father_agent.py:564 - Average Virtual Goal Value = 0.7274919748306274
2025-03-24 16:37:46,328 - father_agent.py:566 - Goal Reach Probability = 0.01454983922829582
2025-03-24 16:37:46,328 - father_agent.py:568 - Trap Reach Probability = 0.9854501607717042
2025-03-24 16:37:46,328 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:37:46,328 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:37:46,328 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:37:46,366 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:37:48,446 - father_agent.py:306 - Step: 105, Training loss: 0.2155669629573822
2025-03-24 16:37:49,694 - father_agent.py:306 - Step: 110, Training loss: 0.4939773976802826
2025-03-24 16:37:50,947 - father_agent.py:306 - Step: 115, Training loss: 0.4025241434574127
2025-03-24 16:37:52,309 - father_agent.py:306 - Step: 120, Training loss: 0.45438945293426514
2025-03-24 16:37:53,592 - father_agent.py:306 - Step: 125, Training loss: 0.2024925947189331
2025-03-24 16:37:54,886 - father_agent.py:306 - Step: 130, Training loss: 0.3000081777572632
2025-03-24 16:37:56,291 - father_agent.py:306 - Step: 135, Training loss: 0.47296029329299927
2025-03-24 16:37:57,573 - father_agent.py:306 - Step: 140, Training loss: 0.4019738733768463
2025-03-24 16:37:58,862 - father_agent.py:306 - Step: 145, Training loss: 0.2906443476676941
2025-03-24 16:38:00,323 - father_agent.py:306 - Step: 150, Training loss: 0.14785706996917725
2025-03-24 16:38:01,777 - father_agent.py:306 - Step: 155, Training loss: 0.286695659160614
2025-03-24 16:38:03,527 - father_agent.py:306 - Step: 160, Training loss: 0.23903697729110718
2025-03-24 16:38:04,906 - father_agent.py:306 - Step: 165, Training loss: 0.22116510570049286
2025-03-24 16:38:06,245 - father_agent.py:306 - Step: 170, Training loss: 0.6030764579772949
2025-03-24 16:38:07,616 - father_agent.py:306 - Step: 175, Training loss: 0.571311891078949
2025-03-24 16:38:08,882 - father_agent.py:306 - Step: 180, Training loss: 0.2333855926990509
2025-03-24 16:38:10,278 - father_agent.py:306 - Step: 185, Training loss: 0.4842056334018707
2025-03-24 16:38:11,684 - father_agent.py:306 - Step: 190, Training loss: 0.5066676139831543
2025-03-24 16:38:13,041 - father_agent.py:306 - Step: 195, Training loss: 0.3311253488063812
2025-03-24 16:38:14,395 - father_agent.py:306 - Step: 200, Training loss: 0.5829740762710571
2025-03-24 16:38:14,510 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:38:23,276 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:38:23,277 - father_agent.py:564 - Average Virtual Goal Value = 3.19606876373291
2025-03-24 16:38:23,277 - father_agent.py:566 - Goal Reach Probability = 0.06392137751786875
2025-03-24 16:38:23,277 - father_agent.py:568 - Trap Reach Probability = 0.9360786224821313
2025-03-24 16:38:23,277 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:38:23,277 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:38:23,277 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:38:23,318 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:38:25,260 - father_agent.py:306 - Step: 205, Training loss: 0.5405963659286499
2025-03-24 16:38:26,577 - father_agent.py:306 - Step: 210, Training loss: 0.6751850843429565
2025-03-24 16:38:27,763 - father_agent.py:306 - Step: 215, Training loss: 0.7421979904174805
2025-03-24 16:38:29,132 - father_agent.py:306 - Step: 220, Training loss: 0.7046209573745728
2025-03-24 16:38:30,444 - father_agent.py:306 - Step: 225, Training loss: 1.112727165222168
2025-03-24 16:38:31,599 - father_agent.py:306 - Step: 230, Training loss: 1.1935985088348389
2025-03-24 16:38:32,814 - father_agent.py:306 - Step: 235, Training loss: 1.1836727857589722
2025-03-24 16:38:33,930 - father_agent.py:306 - Step: 240, Training loss: 0.9324511885643005
2025-03-24 16:38:35,224 - father_agent.py:306 - Step: 245, Training loss: 0.6478917598724365
2025-03-24 16:38:36,588 - father_agent.py:306 - Step: 250, Training loss: 0.8159160614013672
2025-03-24 16:38:37,768 - father_agent.py:306 - Step: 255, Training loss: 0.6267973184585571
2025-03-24 16:38:39,131 - father_agent.py:306 - Step: 260, Training loss: 0.5204722881317139
2025-03-24 16:38:40,440 - father_agent.py:306 - Step: 265, Training loss: 0.5235041379928589
2025-03-24 16:38:41,665 - father_agent.py:306 - Step: 270, Training loss: 0.3645930290222168
2025-03-24 16:38:43,050 - father_agent.py:306 - Step: 275, Training loss: 0.25777679681777954
2025-03-24 16:38:44,378 - father_agent.py:306 - Step: 280, Training loss: 0.36894118785858154
2025-03-24 16:38:45,841 - father_agent.py:306 - Step: 285, Training loss: 0.3631685972213745
2025-03-24 16:38:47,318 - father_agent.py:306 - Step: 290, Training loss: 0.38360732793807983
2025-03-24 16:38:48,915 - father_agent.py:306 - Step: 295, Training loss: 0.5491503477096558
2025-03-24 16:38:50,154 - father_agent.py:306 - Step: 300, Training loss: 0.7574359178543091
2025-03-24 16:38:50,197 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:38:59,754 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:38:59,754 - father_agent.py:564 - Average Virtual Goal Value = 1.5326710939407349
2025-03-24 16:38:59,754 - father_agent.py:566 - Goal Reach Probability = 0.030653422318795594
2025-03-24 16:38:59,754 - father_agent.py:568 - Trap Reach Probability = 0.9693465776812044
2025-03-24 16:38:59,754 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:38:59,754 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:38:59,754 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:38:59,795 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:39:01,747 - father_agent.py:306 - Step: 305, Training loss: 0.792549729347229
2025-03-24 16:39:03,066 - father_agent.py:306 - Step: 310, Training loss: 0.8089694380760193
2025-03-24 16:39:04,344 - father_agent.py:306 - Step: 315, Training loss: 0.76781165599823
2025-03-24 16:39:05,665 - father_agent.py:306 - Step: 320, Training loss: 0.8170924186706543
2025-03-24 16:39:06,852 - father_agent.py:306 - Step: 325, Training loss: 0.9860988855361938
2025-03-24 16:39:08,099 - father_agent.py:306 - Step: 330, Training loss: 0.8375731706619263
2025-03-24 16:39:09,433 - father_agent.py:306 - Step: 335, Training loss: 0.6646051406860352
2025-03-24 16:39:10,619 - father_agent.py:306 - Step: 340, Training loss: 1.1114914417266846
2025-03-24 16:39:11,801 - father_agent.py:306 - Step: 345, Training loss: 0.9301134943962097
2025-03-24 16:39:13,218 - father_agent.py:306 - Step: 350, Training loss: 1.0370562076568604
2025-03-24 16:39:14,546 - father_agent.py:306 - Step: 355, Training loss: 0.9116783142089844
2025-03-24 16:39:15,750 - father_agent.py:306 - Step: 360, Training loss: 0.7083114981651306
2025-03-24 16:39:17,009 - father_agent.py:306 - Step: 365, Training loss: 0.9344159364700317
2025-03-24 16:39:18,265 - father_agent.py:306 - Step: 370, Training loss: 0.8833974003791809
2025-03-24 16:39:19,515 - father_agent.py:306 - Step: 375, Training loss: 0.7698913216590881
2025-03-24 16:39:20,866 - father_agent.py:306 - Step: 380, Training loss: 0.8730487823486328
2025-03-24 16:39:22,087 - father_agent.py:306 - Step: 385, Training loss: 0.8651078939437866
2025-03-24 16:39:23,277 - father_agent.py:306 - Step: 390, Training loss: 0.7791060209274292
2025-03-24 16:39:24,852 - father_agent.py:306 - Step: 395, Training loss: 0.8231654167175293
2025-03-24 16:39:26,163 - father_agent.py:306 - Step: 400, Training loss: 0.7678186297416687
2025-03-24 16:39:26,223 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:39:35,765 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:39:35,765 - father_agent.py:564 - Average Virtual Goal Value = 2.714949607849121
2025-03-24 16:39:35,765 - father_agent.py:566 - Goal Reach Probability = 0.05429899302865995
2025-03-24 16:39:35,765 - father_agent.py:568 - Trap Reach Probability = 0.9457010069713401
2025-03-24 16:39:35,765 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:39:35,765 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:39:35,765 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:39:35,807 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:39:37,920 - father_agent.py:306 - Step: 405, Training loss: 0.9027321338653564
2025-03-24 16:39:39,116 - father_agent.py:306 - Step: 410, Training loss: 0.8627015948295593
2025-03-24 16:39:40,409 - father_agent.py:306 - Step: 415, Training loss: 1.0778093338012695
2025-03-24 16:39:41,672 - father_agent.py:306 - Step: 420, Training loss: 0.8189964890480042
2025-03-24 16:39:42,833 - father_agent.py:306 - Step: 425, Training loss: 0.874472975730896
2025-03-24 16:39:44,051 - father_agent.py:306 - Step: 430, Training loss: 0.530768871307373
2025-03-24 16:39:45,403 - father_agent.py:306 - Step: 435, Training loss: 0.9332090616226196
2025-03-24 16:39:46,610 - father_agent.py:306 - Step: 440, Training loss: 1.1261762380599976
2025-03-24 16:39:47,816 - father_agent.py:306 - Step: 445, Training loss: 0.5797054767608643
2025-03-24 16:39:49,224 - father_agent.py:306 - Step: 450, Training loss: 1.2233999967575073
2025-03-24 16:39:50,536 - father_agent.py:306 - Step: 455, Training loss: 0.8033031225204468
2025-03-24 16:39:51,698 - father_agent.py:306 - Step: 460, Training loss: 0.9002600312232971
2025-03-24 16:39:52,938 - father_agent.py:306 - Step: 465, Training loss: 0.9260164499282837
2025-03-24 16:39:54,208 - father_agent.py:306 - Step: 470, Training loss: 0.9418166875839233
2025-03-24 16:39:55,509 - father_agent.py:306 - Step: 475, Training loss: 0.7467305064201355
2025-03-24 16:39:56,887 - father_agent.py:306 - Step: 480, Training loss: 0.7027159929275513
2025-03-24 16:39:58,174 - father_agent.py:306 - Step: 485, Training loss: 0.7266760468482971
2025-03-24 16:39:59,436 - father_agent.py:306 - Step: 490, Training loss: 0.7150784134864807
2025-03-24 16:40:00,687 - father_agent.py:306 - Step: 495, Training loss: 0.8771553635597229
2025-03-24 16:40:01,898 - father_agent.py:306 - Step: 500, Training loss: 0.8309924006462097
2025-03-24 16:40:02,452 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:40:11,841 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:40:11,841 - father_agent.py:564 - Average Virtual Goal Value = 2.706772804260254
2025-03-24 16:40:11,841 - father_agent.py:566 - Goal Reach Probability = 0.054135454475249055
2025-03-24 16:40:11,841 - father_agent.py:568 - Trap Reach Probability = 0.945864545524751
2025-03-24 16:40:11,841 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:40:11,841 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:40:11,841 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:40:11,883 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:40:14,240 - father_agent.py:306 - Step: 505, Training loss: 0.8652085065841675
2025-03-24 16:40:15,488 - father_agent.py:306 - Step: 510, Training loss: 0.9859815835952759
2025-03-24 16:40:16,857 - father_agent.py:306 - Step: 515, Training loss: 1.0810960531234741
2025-03-24 16:40:18,183 - father_agent.py:306 - Step: 520, Training loss: 0.7700492143630981
2025-03-24 16:40:19,435 - father_agent.py:306 - Step: 525, Training loss: 0.8349625468254089
2025-03-24 16:40:20,647 - father_agent.py:306 - Step: 530, Training loss: 1.160019874572754
2025-03-24 16:40:21,895 - father_agent.py:306 - Step: 535, Training loss: 0.8059255480766296
2025-03-24 16:40:23,247 - father_agent.py:306 - Step: 540, Training loss: 0.8123019933700562
2025-03-24 16:40:24,457 - father_agent.py:306 - Step: 545, Training loss: 0.8769493699073792
2025-03-24 16:40:25,728 - father_agent.py:306 - Step: 550, Training loss: 0.744534969329834
2025-03-24 16:40:27,070 - father_agent.py:306 - Step: 555, Training loss: 0.7838616371154785
2025-03-24 16:40:28,246 - father_agent.py:306 - Step: 560, Training loss: 0.8140177726745605
2025-03-24 16:40:29,494 - father_agent.py:306 - Step: 565, Training loss: 0.546521782875061
2025-03-24 16:40:30,796 - father_agent.py:306 - Step: 570, Training loss: 1.1472517251968384
2025-03-24 16:40:32,037 - father_agent.py:306 - Step: 575, Training loss: 0.9283026456832886
2025-03-24 16:40:33,369 - father_agent.py:306 - Step: 580, Training loss: 0.9117929935455322
2025-03-24 16:40:34,554 - father_agent.py:306 - Step: 585, Training loss: 0.6502454280853271
2025-03-24 16:40:35,732 - father_agent.py:306 - Step: 590, Training loss: 0.9011045098304749
2025-03-24 16:40:37,094 - father_agent.py:306 - Step: 595, Training loss: 0.8420352339744568
2025-03-24 16:40:38,401 - father_agent.py:306 - Step: 600, Training loss: 0.7077869772911072
2025-03-24 16:40:38,501 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:40:47,324 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:40:47,324 - father_agent.py:564 - Average Virtual Goal Value = 2.6555793285369873
2025-03-24 16:40:47,324 - father_agent.py:566 - Goal Reach Probability = 0.053111587982832616
2025-03-24 16:40:47,324 - father_agent.py:568 - Trap Reach Probability = 0.9468884120171673
2025-03-24 16:40:47,324 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:40:47,324 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:40:47,324 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:40:47,365 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:40:49,618 - father_agent.py:306 - Step: 605, Training loss: 0.7013003826141357
2025-03-24 16:40:50,841 - father_agent.py:306 - Step: 610, Training loss: 0.6447659134864807
2025-03-24 16:40:52,055 - father_agent.py:306 - Step: 615, Training loss: 0.5503114461898804
2025-03-24 16:40:53,387 - father_agent.py:306 - Step: 620, Training loss: 0.5885007977485657
2025-03-24 16:40:54,599 - father_agent.py:306 - Step: 625, Training loss: 0.7173327803611755
2025-03-24 16:40:55,778 - father_agent.py:306 - Step: 630, Training loss: 0.6687159538269043
2025-03-24 16:40:57,154 - father_agent.py:306 - Step: 635, Training loss: 0.6686968803405762
2025-03-24 16:40:58,339 - father_agent.py:306 - Step: 640, Training loss: 0.6765303611755371
2025-03-24 16:40:59,528 - father_agent.py:306 - Step: 645, Training loss: 0.8360131978988647
2025-03-24 16:41:00,921 - father_agent.py:306 - Step: 650, Training loss: 0.6646110415458679
2025-03-24 16:41:02,187 - father_agent.py:306 - Step: 655, Training loss: 0.6361088156700134
2025-03-24 16:41:03,431 - father_agent.py:306 - Step: 660, Training loss: 0.8194718956947327
2025-03-24 16:41:04,706 - father_agent.py:306 - Step: 665, Training loss: 0.7177425622940063
2025-03-24 16:41:05,910 - father_agent.py:306 - Step: 670, Training loss: 0.6226637959480286
2025-03-24 16:41:07,244 - father_agent.py:306 - Step: 675, Training loss: 0.9453478455543518
2025-03-24 16:41:08,478 - father_agent.py:306 - Step: 680, Training loss: 0.7984182238578796
2025-03-24 16:41:09,704 - father_agent.py:306 - Step: 685, Training loss: 0.70505690574646
2025-03-24 16:41:11,030 - father_agent.py:306 - Step: 690, Training loss: 0.9642316699028015
2025-03-24 16:41:12,281 - father_agent.py:306 - Step: 695, Training loss: 0.7678660154342651
2025-03-24 16:41:13,481 - father_agent.py:306 - Step: 700, Training loss: 0.7626821398735046
2025-03-24 16:41:13,523 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:41:22,720 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:41:22,720 - father_agent.py:564 - Average Virtual Goal Value = 1.9627788066864014
2025-03-24 16:41:22,720 - father_agent.py:566 - Goal Reach Probability = 0.03925557564798071
2025-03-24 16:41:22,720 - father_agent.py:568 - Trap Reach Probability = 0.9607444243520193
2025-03-24 16:41:22,720 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:41:22,720 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:41:22,720 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:41:22,761 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:41:22,870 - father_agent.py:455 - Training finished.
2025-03-24 16:41:22,876 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:41:22,968 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 16:41:31,930 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:41:31,930 - father_agent.py:564 - Average Virtual Goal Value = 2.1382076740264893
2025-03-24 16:41:31,930 - father_agent.py:566 - Goal Reach Probability = 0.04276415379957614
2025-03-24 16:41:31,930 - father_agent.py:568 - Trap Reach Probability = 0.9572358462004239
2025-03-24 16:41:31,930 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:41:31,930 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:41:31,930 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:41:32,521 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:41:51,202 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:41:51,203 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:41:51,203 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:41:51,203 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:41:51,203 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:41:51,203 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:41:51,203 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:41:51,218 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:42:35,529 - cloned_fsc_actor_policy.py:189 - Epoch 0, Loss: 2.0608
2025-03-24 16:42:35,529 - cloned_fsc_actor_policy.py:190 - Epoch 0, Accuracy: 0.0769
2025-03-24 16:42:35,571 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:42:52,486 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:42:52,487 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:42:52,487 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:42:52,487 - evaluators.py:134 - Trap Reach Probability = 0.0
2025-03-24 16:42:52,487 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:42:52,487 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:42:52,487 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:42:55,913 - cloned_fsc_actor_policy.py:189 - Epoch 1000, Loss: 0.7635
2025-03-24 16:42:55,913 - cloned_fsc_actor_policy.py:190 - Epoch 1000, Accuracy: 0.7401
2025-03-24 16:42:59,279 - cloned_fsc_actor_policy.py:189 - Epoch 2000, Loss: 0.2095
2025-03-24 16:42:59,279 - cloned_fsc_actor_policy.py:190 - Epoch 2000, Accuracy: 0.9604
2025-03-24 16:43:02,596 - cloned_fsc_actor_policy.py:189 - Epoch 3000, Loss: 0.0457
2025-03-24 16:43:02,596 - cloned_fsc_actor_policy.py:190 - Epoch 3000, Accuracy: 0.9970
2025-03-24 16:43:05,911 - cloned_fsc_actor_policy.py:189 - Epoch 4000, Loss: 0.0171
2025-03-24 16:43:05,911 - cloned_fsc_actor_policy.py:190 - Epoch 4000, Accuracy: 0.9980
2025-03-24 16:43:09,206 - cloned_fsc_actor_policy.py:189 - Epoch 5000, Loss: 0.0092
2025-03-24 16:43:09,206 - cloned_fsc_actor_policy.py:190 - Epoch 5000, Accuracy: 0.9984
2025-03-24 16:43:09,253 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:43:26,195 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:43:26,195 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:43:26,195 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:43:26,195 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:43:26,195 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:43:26,195 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:43:26,195 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:43:29,452 - cloned_fsc_actor_policy.py:189 - Epoch 6000, Loss: 0.0046
2025-03-24 16:43:29,452 - cloned_fsc_actor_policy.py:190 - Epoch 6000, Accuracy: 0.9990
2025-03-24 16:43:32,776 - cloned_fsc_actor_policy.py:189 - Epoch 7000, Loss: 0.0010
2025-03-24 16:43:32,776 - cloned_fsc_actor_policy.py:190 - Epoch 7000, Accuracy: 1.0000
2025-03-24 16:43:36,402 - cloned_fsc_actor_policy.py:189 - Epoch 8000, Loss: 0.0004
2025-03-24 16:43:36,402 - cloned_fsc_actor_policy.py:190 - Epoch 8000, Accuracy: 1.0000
2025-03-24 16:43:40,375 - cloned_fsc_actor_policy.py:189 - Epoch 9000, Loss: 0.0002
2025-03-24 16:43:40,376 - cloned_fsc_actor_policy.py:190 - Epoch 9000, Accuracy: 1.0000
2025-03-24 16:43:43,702 - cloned_fsc_actor_policy.py:189 - Epoch 10000, Loss: 0.0001
2025-03-24 16:43:43,703 - cloned_fsc_actor_policy.py:190 - Epoch 10000, Accuracy: 1.0000
2025-03-24 16:43:43,740 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:44:00,402 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:44:00,403 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:44:00,403 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:44:00,403 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:44:00,403 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:44:00,403 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:44:00,403 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:44:03,742 - cloned_fsc_actor_policy.py:189 - Epoch 11000, Loss: 0.0001
2025-03-24 16:44:03,742 - cloned_fsc_actor_policy.py:190 - Epoch 11000, Accuracy: 1.0000
2025-03-24 16:44:07,093 - cloned_fsc_actor_policy.py:189 - Epoch 12000, Loss: 0.0000
2025-03-24 16:44:07,093 - cloned_fsc_actor_policy.py:190 - Epoch 12000, Accuracy: 1.0000
2025-03-24 16:44:10,512 - cloned_fsc_actor_policy.py:189 - Epoch 13000, Loss: 0.0000
2025-03-24 16:44:10,512 - cloned_fsc_actor_policy.py:190 - Epoch 13000, Accuracy: 1.0000
2025-03-24 16:44:14,078 - cloned_fsc_actor_policy.py:189 - Epoch 14000, Loss: 0.0000
2025-03-24 16:44:14,078 - cloned_fsc_actor_policy.py:190 - Epoch 14000, Accuracy: 1.0000
2025-03-24 16:44:17,610 - cloned_fsc_actor_policy.py:189 - Epoch 15000, Loss: 0.0000
2025-03-24 16:44:17,611 - cloned_fsc_actor_policy.py:190 - Epoch 15000, Accuracy: 1.0000
2025-03-24 16:44:17,649 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:44:35,123 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:44:35,123 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:44:35,123 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:44:35,123 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:44:35,123 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:44:35,123 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:44:35,123 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:44:38,443 - cloned_fsc_actor_policy.py:189 - Epoch 16000, Loss: 0.0000
2025-03-24 16:44:38,443 - cloned_fsc_actor_policy.py:190 - Epoch 16000, Accuracy: 1.0000
2025-03-24 16:44:41,821 - cloned_fsc_actor_policy.py:189 - Epoch 17000, Loss: 0.0000
2025-03-24 16:44:41,821 - cloned_fsc_actor_policy.py:190 - Epoch 17000, Accuracy: 1.0000
2025-03-24 16:44:45,149 - cloned_fsc_actor_policy.py:189 - Epoch 18000, Loss: 0.0000
2025-03-24 16:44:45,149 - cloned_fsc_actor_policy.py:190 - Epoch 18000, Accuracy: 1.0000
2025-03-24 16:44:48,548 - cloned_fsc_actor_policy.py:189 - Epoch 19000, Loss: 0.0000
2025-03-24 16:44:48,548 - cloned_fsc_actor_policy.py:190 - Epoch 19000, Accuracy: 1.0000
2025-03-24 16:44:51,817 - cloned_fsc_actor_policy.py:189 - Epoch 20000, Loss: 0.0000
2025-03-24 16:44:51,817 - cloned_fsc_actor_policy.py:190 - Epoch 20000, Accuracy: 1.0000
2025-03-24 16:44:51,855 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:45:09,104 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:45:09,104 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:45:09,104 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:45:09,104 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:45:09,104 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:45:09,104 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:45:09,104 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:45:12,392 - cloned_fsc_actor_policy.py:189 - Epoch 21000, Loss: 0.0000
2025-03-24 16:45:12,392 - cloned_fsc_actor_policy.py:190 - Epoch 21000, Accuracy: 1.0000
2025-03-24 16:45:15,692 - cloned_fsc_actor_policy.py:189 - Epoch 22000, Loss: 0.0000
2025-03-24 16:45:15,692 - cloned_fsc_actor_policy.py:190 - Epoch 22000, Accuracy: 1.0000
2025-03-24 16:45:19,078 - cloned_fsc_actor_policy.py:189 - Epoch 23000, Loss: 0.0000
2025-03-24 16:45:19,078 - cloned_fsc_actor_policy.py:190 - Epoch 23000, Accuracy: 1.0000
2025-03-24 16:45:22,774 - cloned_fsc_actor_policy.py:189 - Epoch 24000, Loss: 0.0000
2025-03-24 16:45:22,774 - cloned_fsc_actor_policy.py:190 - Epoch 24000, Accuracy: 1.0000
2025-03-24 16:45:26,052 - cloned_fsc_actor_policy.py:189 - Epoch 25000, Loss: 0.0000
2025-03-24 16:45:26,052 - cloned_fsc_actor_policy.py:190 - Epoch 25000, Accuracy: 1.0000
2025-03-24 16:45:26,087 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:45:43,189 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:45:43,189 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:45:43,190 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:45:43,190 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:45:43,190 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:45:43,190 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:45:43,190 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:45:46,492 - cloned_fsc_actor_policy.py:189 - Epoch 26000, Loss: 0.0000
2025-03-24 16:45:46,492 - cloned_fsc_actor_policy.py:190 - Epoch 26000, Accuracy: 1.0000
2025-03-24 16:45:49,818 - cloned_fsc_actor_policy.py:189 - Epoch 27000, Loss: 0.0000
2025-03-24 16:45:49,818 - cloned_fsc_actor_policy.py:190 - Epoch 27000, Accuracy: 1.0000
2025-03-24 16:45:53,118 - cloned_fsc_actor_policy.py:189 - Epoch 28000, Loss: 0.0000
2025-03-24 16:45:53,119 - cloned_fsc_actor_policy.py:190 - Epoch 28000, Accuracy: 1.0000
2025-03-24 16:45:56,347 - cloned_fsc_actor_policy.py:189 - Epoch 29000, Loss: 0.0000
2025-03-24 16:45:56,348 - cloned_fsc_actor_policy.py:190 - Epoch 29000, Accuracy: 1.0000
2025-03-24 16:45:59,546 - cloned_fsc_actor_policy.py:189 - Epoch 30000, Loss: 0.0000
2025-03-24 16:45:59,546 - cloned_fsc_actor_policy.py:190 - Epoch 30000, Accuracy: 1.0000
2025-03-24 16:45:59,581 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:46:16,459 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:46:16,459 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:46:16,459 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:46:16,459 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:46:16,460 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:46:16,460 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:46:16,460 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:46:20,499 - cloned_fsc_actor_policy.py:189 - Epoch 31000, Loss: 0.0000
2025-03-24 16:46:20,499 - cloned_fsc_actor_policy.py:190 - Epoch 31000, Accuracy: 1.0000
2025-03-24 16:46:23,731 - cloned_fsc_actor_policy.py:189 - Epoch 32000, Loss: 0.0000
2025-03-24 16:46:23,731 - cloned_fsc_actor_policy.py:190 - Epoch 32000, Accuracy: 1.0000
2025-03-24 16:46:27,164 - cloned_fsc_actor_policy.py:189 - Epoch 33000, Loss: 0.0000
2025-03-24 16:46:27,164 - cloned_fsc_actor_policy.py:190 - Epoch 33000, Accuracy: 1.0000
2025-03-24 16:46:30,492 - cloned_fsc_actor_policy.py:189 - Epoch 34000, Loss: 0.0000
2025-03-24 16:46:30,492 - cloned_fsc_actor_policy.py:190 - Epoch 34000, Accuracy: 1.0000
2025-03-24 16:46:33,817 - cloned_fsc_actor_policy.py:189 - Epoch 35000, Loss: 0.0000
2025-03-24 16:46:33,817 - cloned_fsc_actor_policy.py:190 - Epoch 35000, Accuracy: 1.0000
2025-03-24 16:46:33,848 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:46:50,800 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:46:50,800 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:46:50,801 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:46:50,801 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:46:50,801 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:46:50,801 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:46:50,801 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:46:54,306 - cloned_fsc_actor_policy.py:189 - Epoch 36000, Loss: 0.0000
2025-03-24 16:46:54,306 - cloned_fsc_actor_policy.py:190 - Epoch 36000, Accuracy: 1.0000
2025-03-24 16:46:57,547 - cloned_fsc_actor_policy.py:189 - Epoch 37000, Loss: 0.0000
2025-03-24 16:46:57,547 - cloned_fsc_actor_policy.py:190 - Epoch 37000, Accuracy: 1.0000
2025-03-24 16:47:00,878 - cloned_fsc_actor_policy.py:189 - Epoch 38000, Loss: 0.0000
2025-03-24 16:47:00,878 - cloned_fsc_actor_policy.py:190 - Epoch 38000, Accuracy: 1.0000
2025-03-24 16:47:04,190 - cloned_fsc_actor_policy.py:189 - Epoch 39000, Loss: 0.0000
2025-03-24 16:47:04,190 - cloned_fsc_actor_policy.py:190 - Epoch 39000, Accuracy: 1.0000
2025-03-24 16:47:07,472 - cloned_fsc_actor_policy.py:189 - Epoch 40000, Loss: 0.0000
2025-03-24 16:47:07,472 - cloned_fsc_actor_policy.py:190 - Epoch 40000, Accuracy: 1.0000
2025-03-24 16:47:07,505 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:47:25,116 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:47:25,116 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:47:25,116 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:47:25,116 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:47:25,116 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:47:25,116 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:47:25,116 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:47:28,462 - cloned_fsc_actor_policy.py:189 - Epoch 41000, Loss: 0.0000
2025-03-24 16:47:28,463 - cloned_fsc_actor_policy.py:190 - Epoch 41000, Accuracy: 1.0000
2025-03-24 16:47:31,660 - cloned_fsc_actor_policy.py:189 - Epoch 42000, Loss: 0.0000
2025-03-24 16:47:31,661 - cloned_fsc_actor_policy.py:190 - Epoch 42000, Accuracy: 1.0000
2025-03-24 16:47:35,049 - cloned_fsc_actor_policy.py:189 - Epoch 43000, Loss: 0.0000
2025-03-24 16:47:35,050 - cloned_fsc_actor_policy.py:190 - Epoch 43000, Accuracy: 1.0000
2025-03-24 16:47:38,384 - cloned_fsc_actor_policy.py:189 - Epoch 44000, Loss: 0.0000
2025-03-24 16:47:38,384 - cloned_fsc_actor_policy.py:190 - Epoch 44000, Accuracy: 1.0000
2025-03-24 16:47:41,528 - cloned_fsc_actor_policy.py:189 - Epoch 45000, Loss: 0.0000
2025-03-24 16:47:41,528 - cloned_fsc_actor_policy.py:190 - Epoch 45000, Accuracy: 1.0000
2025-03-24 16:47:41,567 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:47:59,403 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:47:59,404 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:47:59,404 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:47:59,404 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:47:59,404 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:47:59,404 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:47:59,404 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:48:02,804 - cloned_fsc_actor_policy.py:189 - Epoch 46000, Loss: 0.0000
2025-03-24 16:48:02,804 - cloned_fsc_actor_policy.py:190 - Epoch 46000, Accuracy: 1.0000
2025-03-24 16:48:06,116 - cloned_fsc_actor_policy.py:189 - Epoch 47000, Loss: 0.0000
2025-03-24 16:48:06,116 - cloned_fsc_actor_policy.py:190 - Epoch 47000, Accuracy: 1.0000
2025-03-24 16:48:09,397 - cloned_fsc_actor_policy.py:189 - Epoch 48000, Loss: 0.0000
2025-03-24 16:48:09,397 - cloned_fsc_actor_policy.py:190 - Epoch 48000, Accuracy: 1.0000
2025-03-24 16:48:12,791 - cloned_fsc_actor_policy.py:189 - Epoch 49000, Loss: 0.0000
2025-03-24 16:48:12,792 - cloned_fsc_actor_policy.py:190 - Epoch 49000, Accuracy: 1.0000
2025-03-24 16:48:16,087 - cloned_fsc_actor_policy.py:189 - Epoch 50000, Loss: 0.0000
2025-03-24 16:48:16,087 - cloned_fsc_actor_policy.py:190 - Epoch 50000, Accuracy: 1.0000
2025-03-24 16:48:16,128 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:48:33,564 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:48:33,564 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:48:33,564 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:48:33,564 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:48:33,564 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:48:33,564 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:48:33,564 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:48:36,891 - cloned_fsc_actor_policy.py:189 - Epoch 51000, Loss: 0.0000
2025-03-24 16:48:36,891 - cloned_fsc_actor_policy.py:190 - Epoch 51000, Accuracy: 1.0000
2025-03-24 16:48:40,203 - cloned_fsc_actor_policy.py:189 - Epoch 52000, Loss: 0.0000
2025-03-24 16:48:40,203 - cloned_fsc_actor_policy.py:190 - Epoch 52000, Accuracy: 1.0000
2025-03-24 16:48:43,499 - cloned_fsc_actor_policy.py:189 - Epoch 53000, Loss: 0.0000
2025-03-24 16:48:43,499 - cloned_fsc_actor_policy.py:190 - Epoch 53000, Accuracy: 1.0000
2025-03-24 16:48:46,789 - cloned_fsc_actor_policy.py:189 - Epoch 54000, Loss: 0.0000
2025-03-24 16:48:46,789 - cloned_fsc_actor_policy.py:190 - Epoch 54000, Accuracy: 1.0000
2025-03-24 16:48:50,165 - cloned_fsc_actor_policy.py:189 - Epoch 55000, Loss: 0.0000
2025-03-24 16:48:50,165 - cloned_fsc_actor_policy.py:190 - Epoch 55000, Accuracy: 1.0000
2025-03-24 16:48:50,486 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:49:06,974 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:49:06,974 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:49:06,974 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:49:06,974 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:49:06,974 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:49:06,975 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:49:06,975 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:49:10,297 - cloned_fsc_actor_policy.py:189 - Epoch 56000, Loss: 0.0000
2025-03-24 16:49:10,297 - cloned_fsc_actor_policy.py:190 - Epoch 56000, Accuracy: 1.0000
2025-03-24 16:49:13,658 - cloned_fsc_actor_policy.py:189 - Epoch 57000, Loss: 0.0000
2025-03-24 16:49:13,658 - cloned_fsc_actor_policy.py:190 - Epoch 57000, Accuracy: 1.0000
2025-03-24 16:49:16,953 - cloned_fsc_actor_policy.py:189 - Epoch 58000, Loss: 0.0000
2025-03-24 16:49:16,953 - cloned_fsc_actor_policy.py:190 - Epoch 58000, Accuracy: 1.0000
2025-03-24 16:49:20,246 - cloned_fsc_actor_policy.py:189 - Epoch 59000, Loss: 0.0000
2025-03-24 16:49:20,246 - cloned_fsc_actor_policy.py:190 - Epoch 59000, Accuracy: 1.0000
2025-03-24 16:49:23,519 - cloned_fsc_actor_policy.py:189 - Epoch 60000, Loss: 0.0000
2025-03-24 16:49:23,519 - cloned_fsc_actor_policy.py:190 - Epoch 60000, Accuracy: 1.0000
2025-03-24 16:49:23,557 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:49:40,426 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:49:40,426 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:49:40,426 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:49:40,426 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:49:40,426 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:49:40,426 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:49:40,426 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:49:41,269 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:49:58,050 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:49:58,050 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 16:49:58,050 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 16:49:58,050 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 16:49:58,050 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:49:58,050 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:49:58,050 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 16:49:58,677 - pomdp.py:349 - unfolding 5-FSC template into POMDP...
2025-03-24 16:50:00,040 - pomdp.py:355 - constructed quotient MDP having 33898 states and 617106 actions.
2025-03-24 16:50:01,305 - rl_family_extractor.py:329 - Building family from FFNN...
2025-03-24 16:50:01,390 - rl_family_extractor.py:357 - Number of misses: 1 out of 1000
2025-03-24 16:50:01,390 - rl_family_extractor.py:359 - Number of complete misses: 1 out of 1000
2025-03-24 16:50:01,391 - statistic.py:67 - synthesis initiated, design space: 4
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 0.05 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 100 %
DTMC stats: avg DTMC size: 632, iterations: 4

optimum: 0.140482
--------------------
2025-03-24 16:50:01,439 - statistic.py:67 - synthesis initiated, design space: 1e609
> progress 0.0%, elapsed 3 s, estimated 3008079 s (34 days), iters = {DTMC: 363}, opt = 0.1405
> progress 0.0%, elapsed 6 s, estimated 6010418 s (69 days), iters = {DTMC: 724}, opt = 0.1405
> progress 0.0%, elapsed 9 s, estimated 9015922 s (104 days), iters = {DTMC: 1087}, opt = 0.1405
> progress 0.0%, elapsed 12 s, estimated 12019688 s (139 days), iters = {DTMC: 1451}, opt = 0.1405
> progress 0.0%, elapsed 15 s, estimated 15022460 s (173 days), iters = {DTMC: 1814}, opt = 0.1405
> progress 0.0%, elapsed 18 s, estimated 18023922 s (208 days), iters = {DTMC: 2181}, opt = 0.1405
> progress 0.0%, elapsed 21 s, estimated 21024943 s (243 days), iters = {DTMC: 2546}, opt = 0.1405
> progress 0.0%, elapsed 24 s, estimated 24032124 s (278 days), iters = {DTMC: 2903}, opt = 0.1405
> progress 0.0%, elapsed 27 s, estimated 27033793 s (312 days), iters = {DTMC: 3226}, opt = 0.1405
> progress 0.0%, elapsed 30 s, estimated 30038115 s (347 days), iters = {DTMC: 3593}, opt = 0.1405
> progress 0.0%, elapsed 33 s, estimated 33039601 s (1 year), iters = {DTMC: 3952}, opt = 0.1405
> progress 0.0%, elapsed 36 s, estimated 36043615 s (1 year), iters = {DTMC: 4259}, opt = 0.1405
> progress 0.0%, elapsed 39 s, estimated 39046083 s (1 year), iters = {DTMC: 4622}, opt = 0.1405
> progress 0.0%, elapsed 42 s, estimated 42054443 s (1 year), iters = {DTMC: 4986}, opt = 0.1405
> progress 0.0%, elapsed 45 s, estimated 45056247 s (1 year), iters = {DTMC: 5346}, opt = 0.1405
> progress 0.0%, elapsed 48 s, estimated 48056925 s (1 year), iters = {DTMC: 5710}, opt = 0.1405
> progress 0.0%, elapsed 51 s, estimated 51064951 s (1 year), iters = {DTMC: 6068}, opt = 0.1405
> progress 0.0%, elapsed 54 s, estimated 54067277 s (1 year), iters = {DTMC: 6431}, opt = 0.1405
> progress 0.0%, elapsed 57 s, estimated 57072642 s (1 year), iters = {DTMC: 6794}, opt = 0.1405
2025-03-24 16:51:01,447 - synthesizer_onebyone.py:19 - Time limit reached
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.01 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 0 %
DTMC stats: avg DTMC size: 212, iterations: 7152

optimum: 0.140482
--------------------
2025-03-24 16:51:01,448 - synthesizer_rl_storm_paynt.py:280 - No improving assignment found.
2025-03-24 16:51:01,449 - storm_pomdp_control.py:246 - Interactive Storm resumed
2025-03-24 16:51:01,449 - storm_pomdp_control.py:291 - Updating FSC values in Storm
2025-03-24 16:51:32,481 - storm_pomdp_control.py:305 - Pausing Storm
-----------Storm-----------               
Value = 0.23461992224252154 | Time elapsed = 6146.7s | FSC size = 1636

2025-03-24 16:52:57,774 - synthesizer_pomdp.py:253 - Timeout for PAYNT started
2025-03-24 16:52:58,718 - synthesizer_ar_storm.py:136 - Resuming synthesis
2025-03-24 16:52:58,725 - synthesizer_ar_storm.py:147 - PAYNT's value is better. Prioritizing synthesis results
2025-03-24 16:53:27,991 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 16:53:28,034 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:53:38,625 - father_agent.py:562 - Average Return = 0.0
2025-03-24 16:53:38,626 - father_agent.py:564 - Average Virtual Goal Value = 2.067173719406128
2025-03-24 16:53:38,626 - father_agent.py:566 - Goal Reach Probability = 0.041343474659236386
2025-03-24 16:53:38,626 - father_agent.py:568 - Trap Reach Probability = 0.9586565253407636
2025-03-24 16:53:38,626 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 16:53:38,626 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 16:53:38,626 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 16:53:43,148 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:53:47,512 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:53:47,513 - evaluators.py:130 - Average Virtual Goal Value = 0.26954177021980286
2025-03-24 16:53:47,513 - evaluators.py:132 - Goal Reach Probability = 0.005390835579514825
2025-03-24 16:53:47,513 - evaluators.py:134 - Trap Reach Probability = 0.9946091644204852
2025-03-24 16:53:47,513 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:53:47,513 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:53:47,513 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:53:47,513 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 1.2998459339141846
Epoch: 0, Critic Loss: 15.887322425842285
Epoch: 10, Actor Loss: 0.13562580943107605
Epoch: 10, Critic Loss: 18.404029846191406
Epoch: 20, Actor Loss: 0.13003018498420715
Epoch: 20, Critic Loss: 16.88302230834961
Epoch: 30, Actor Loss: 0.0889960452914238
Epoch: 30, Critic Loss: 15.601097106933594
Epoch: 40, Actor Loss: 0.10964053869247437
Epoch: 40, Critic Loss: 20.452106475830078
2025-03-24 16:54:20,766 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:54:24,172 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:54:24,173 - evaluators.py:130 - Average Virtual Goal Value = 5.694227695465088
2025-03-24 16:54:24,173 - evaluators.py:132 - Goal Reach Probability = 0.11388455538221529
2025-03-24 16:54:24,173 - evaluators.py:134 - Trap Reach Probability = 0.8861154446177847
2025-03-24 16:54:24,173 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:54:24,173 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:54:24,173 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:54:24,173 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.0605766586959362
Epoch: 50, Critic Loss: 8.103944778442383
Epoch: 60, Actor Loss: 0.04648391902446747
Epoch: 60, Critic Loss: 18.79159164428711
Epoch: 70, Actor Loss: 0.045285776257514954
Epoch: 70, Critic Loss: 15.153971672058105
Epoch: 80, Actor Loss: 0.030137838795781136
Epoch: 80, Critic Loss: 12.405851364135742
Epoch: 90, Actor Loss: 0.037452004849910736
Epoch: 90, Critic Loss: 17.260969161987305
2025-03-24 16:54:57,490 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:55:03,620 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:55:03,621 - evaluators.py:130 - Average Virtual Goal Value = 8.442453384399414
2025-03-24 16:55:03,621 - evaluators.py:132 - Goal Reach Probability = 0.16884906960716747
2025-03-24 16:55:03,621 - evaluators.py:134 - Trap Reach Probability = 0.8311509303928325
2025-03-24 16:55:03,621 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:55:03,621 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:55:03,621 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:55:03,621 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.04559475928544998
Epoch: 100, Critic Loss: 10.905817031860352
Epoch: 110, Actor Loss: 0.03735816851258278
Epoch: 110, Critic Loss: 10.983989715576172
Epoch: 120, Actor Loss: 0.03803879767656326
Epoch: 120, Critic Loss: 17.525543212890625
Epoch: 130, Actor Loss: 0.029135284945368767
Epoch: 130, Critic Loss: 11.41913890838623
Epoch: 140, Actor Loss: 0.03063632734119892
Epoch: 140, Critic Loss: 8.3399658203125
2025-03-24 16:55:36,695 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:55:39,889 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:55:39,889 - evaluators.py:130 - Average Virtual Goal Value = 9.049295425415039
2025-03-24 16:55:39,889 - evaluators.py:132 - Goal Reach Probability = 0.18098591549295776
2025-03-24 16:55:39,889 - evaluators.py:134 - Trap Reach Probability = 0.8190140845070423
2025-03-24 16:55:39,889 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:55:39,889 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:55:39,889 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:55:39,890 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.03742631897330284
Epoch: 150, Critic Loss: 4.856035232543945
Epoch: 160, Actor Loss: 0.023471031337976456
Epoch: 160, Critic Loss: 12.996376037597656
Epoch: 170, Actor Loss: 0.023513708263635635
Epoch: 170, Critic Loss: 12.439519882202148
Epoch: 180, Actor Loss: 0.0533786304295063
Epoch: 180, Critic Loss: 11.128944396972656
Epoch: 190, Actor Loss: 0.016213594004511833
Epoch: 190, Critic Loss: 11.821189880371094
2025-03-24 16:56:12,930 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:56:16,035 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:56:16,036 - evaluators.py:130 - Average Virtual Goal Value = 9.541284561157227
2025-03-24 16:56:16,036 - evaluators.py:132 - Goal Reach Probability = 0.1908256880733945
2025-03-24 16:56:16,036 - evaluators.py:134 - Trap Reach Probability = 0.8091743119266055
2025-03-24 16:56:16,036 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:56:16,036 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:56:16,036 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:56:16,036 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.02053414285182953
Epoch: 200, Critic Loss: 9.545666694641113
Epoch: 210, Actor Loss: 0.04465616121888161
Epoch: 210, Critic Loss: 15.075162887573242
Epoch: 220, Actor Loss: 0.03584709018468857
Epoch: 220, Critic Loss: 12.87890338897705
Epoch: 230, Actor Loss: 0.021853286772966385
Epoch: 230, Critic Loss: 4.474995136260986
Epoch: 240, Actor Loss: 0.03369671851396561
Epoch: 240, Critic Loss: 13.566280364990234
2025-03-24 16:56:49,790 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:56:53,093 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:56:53,093 - evaluators.py:130 - Average Virtual Goal Value = 10.018656730651855
2025-03-24 16:56:53,094 - evaluators.py:132 - Goal Reach Probability = 0.2003731343283582
2025-03-24 16:56:53,094 - evaluators.py:134 - Trap Reach Probability = 0.7996268656716418
2025-03-24 16:56:53,094 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:56:53,094 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:56:53,094 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:56:53,094 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.028910158202052116
Epoch: 250, Critic Loss: 13.619842529296875
Epoch: 260, Actor Loss: 0.03493461012840271
Epoch: 260, Critic Loss: 8.49584674835205
Epoch: 270, Actor Loss: 0.03529539331793785
Epoch: 270, Critic Loss: 13.024394989013672
Epoch: 280, Actor Loss: 0.02460324764251709
Epoch: 280, Critic Loss: 9.356757164001465
Epoch: 290, Actor Loss: 0.03301960229873657
Epoch: 290, Critic Loss: 12.952981948852539
2025-03-24 16:57:25,743 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:57:29,262 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:57:29,262 - evaluators.py:130 - Average Virtual Goal Value = 9.30402946472168
2025-03-24 16:57:29,262 - evaluators.py:132 - Goal Reach Probability = 0.18608058608058609
2025-03-24 16:57:29,262 - evaluators.py:134 - Trap Reach Probability = 0.8139194139194139
2025-03-24 16:57:29,262 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:57:29,262 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:57:29,262 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:57:29,262 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.017484981566667557
Epoch: 300, Critic Loss: 9.86980152130127
Epoch: 310, Actor Loss: 0.026657195761799812
Epoch: 310, Critic Loss: 9.036328315734863
Epoch: 320, Actor Loss: 0.02296523004770279
Epoch: 320, Critic Loss: 12.220783233642578
Epoch: 330, Actor Loss: 0.016551341861486435
Epoch: 330, Critic Loss: 9.446775436401367
Epoch: 340, Actor Loss: 0.04168118163943291
Epoch: 340, Critic Loss: 13.29721450805664
2025-03-24 16:58:02,521 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:58:06,033 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:58:06,033 - evaluators.py:130 - Average Virtual Goal Value = 9.50473403930664
2025-03-24 16:58:06,034 - evaluators.py:132 - Goal Reach Probability = 0.19009468317552805
2025-03-24 16:58:06,034 - evaluators.py:134 - Trap Reach Probability = 0.809905316824472
2025-03-24 16:58:06,034 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:58:06,034 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:58:06,034 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:58:06,034 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.020998897030949593
Epoch: 350, Critic Loss: 12.209209442138672
Epoch: 360, Actor Loss: 0.03432100638747215
Epoch: 360, Critic Loss: 8.66372299194336
Epoch: 370, Actor Loss: 0.025751622393727303
Epoch: 370, Critic Loss: 9.281904220581055
Epoch: 380, Actor Loss: 0.02887318655848503
Epoch: 380, Critic Loss: 10.437080383300781
Epoch: 390, Actor Loss: 0.032740235328674316
Epoch: 390, Critic Loss: 4.413476467132568
2025-03-24 16:58:38,509 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:58:42,782 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:58:42,782 - evaluators.py:130 - Average Virtual Goal Value = 10.474090576171875
2025-03-24 16:58:42,782 - evaluators.py:132 - Goal Reach Probability = 0.20948180815876516
2025-03-24 16:58:42,782 - evaluators.py:134 - Trap Reach Probability = 0.7905181918412348
2025-03-24 16:58:42,782 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:58:42,782 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:58:42,782 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:58:42,782 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.027705661952495575
Epoch: 400, Critic Loss: 9.800969123840332
Epoch: 410, Actor Loss: 0.033349763602018356
Epoch: 410, Critic Loss: 10.452714920043945
Epoch: 420, Actor Loss: 0.02214866131544113
Epoch: 420, Critic Loss: 11.580127716064453
Epoch: 430, Actor Loss: 0.01586821861565113
Epoch: 430, Critic Loss: 8.833373069763184
Epoch: 440, Actor Loss: 0.01855955272912979
Epoch: 440, Critic Loss: 10.252490997314453
2025-03-24 16:59:15,338 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:59:19,027 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:59:19,027 - evaluators.py:130 - Average Virtual Goal Value = 8.987751960754395
2025-03-24 16:59:19,027 - evaluators.py:132 - Goal Reach Probability = 0.1797550432276657
2025-03-24 16:59:19,027 - evaluators.py:134 - Trap Reach Probability = 0.8202449567723343
2025-03-24 16:59:19,027 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:59:19,027 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:59:19,027 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:59:19,027 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.018753116950392723
Epoch: 450, Critic Loss: 7.852598190307617
Epoch: 460, Actor Loss: 0.019741039723157883
Epoch: 460, Critic Loss: 12.912176132202148
Epoch: 470, Actor Loss: 0.01759122684597969
Epoch: 470, Critic Loss: 11.194377899169922
Epoch: 480, Actor Loss: 0.02743895724415779
Epoch: 480, Critic Loss: 13.323612213134766
Epoch: 490, Actor Loss: 0.024066217243671417
Epoch: 490, Critic Loss: 8.357341766357422
2025-03-24 16:59:52,140 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 16:59:55,532 - evaluators.py:128 - Average Return = 0.0
2025-03-24 16:59:55,532 - evaluators.py:130 - Average Virtual Goal Value = 9.494585037231445
2025-03-24 16:59:55,532 - evaluators.py:132 - Goal Reach Probability = 0.18989169675090253
2025-03-24 16:59:55,532 - evaluators.py:134 - Trap Reach Probability = 0.8101083032490974
2025-03-24 16:59:55,532 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 16:59:55,532 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 16:59:55,532 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 16:59:55,532 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.021451666951179504
Epoch: 500, Critic Loss: 12.027183532714844
Epoch: 510, Actor Loss: 0.032031573355197906
Epoch: 510, Critic Loss: 14.537240982055664
Epoch: 520, Actor Loss: 0.017729924991726875
Epoch: 520, Critic Loss: 4.597935676574707
Epoch: 530, Actor Loss: 0.014259535819292068
Epoch: 530, Critic Loss: 11.129277229309082
Epoch: 540, Actor Loss: 0.023861879482865334
Epoch: 540, Critic Loss: 9.677412033081055
2025-03-24 17:00:29,222 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:00:32,396 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:00:32,396 - evaluators.py:130 - Average Virtual Goal Value = 10.383447647094727
2025-03-24 17:00:32,396 - evaluators.py:132 - Goal Reach Probability = 0.2076689445709947
2025-03-24 17:00:32,396 - evaluators.py:134 - Trap Reach Probability = 0.7923310554290053
2025-03-24 17:00:32,396 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:00:32,396 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:00:32,397 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 17:00:32,397 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.019618656486272812
Epoch: 550, Critic Loss: 8.500396728515625
Epoch: 560, Actor Loss: 0.029801787808537483
Epoch: 560, Critic Loss: 8.913331985473633
Epoch: 570, Actor Loss: 0.014769833534955978
Epoch: 570, Critic Loss: 13.426512718200684
Epoch: 580, Actor Loss: 0.01747896708548069
Epoch: 580, Critic Loss: 12.013870239257812
Epoch: 590, Actor Loss: 0.01967557892203331
Epoch: 590, Critic Loss: 19.499935150146484
2025-03-24 17:01:05,291 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:01:08,558 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:01:08,558 - evaluators.py:130 - Average Virtual Goal Value = 9.908759117126465
2025-03-24 17:01:08,558 - evaluators.py:132 - Goal Reach Probability = 0.19817518248175184
2025-03-24 17:01:08,558 - evaluators.py:134 - Trap Reach Probability = 0.8018248175182482
2025-03-24 17:01:08,558 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:01:08,558 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:01:08,558 - evaluators.py:140 - Current Best Reach Probability = 0.21836040800906686
2025-03-24 17:01:08,558 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.026897763833403587
Epoch: 600, Critic Loss: 12.494140625
Epoch: 610, Actor Loss: 0.017054744064807892
Epoch: 610, Critic Loss: 6.940617084503174
Epoch: 620, Actor Loss: 0.020062532275915146
Epoch: 620, Critic Loss: 12.60078239440918
Epoch: 630, Actor Loss: 0.03551430627703667
Epoch: 630, Critic Loss: 7.869385719299316
Epoch: 640, Actor Loss: 0.0371624119579792
Epoch: 640, Critic Loss: 13.67959213256836
2025-03-24 17:01:40,956 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:01:44,254 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:01:44,254 - evaluators.py:130 - Average Virtual Goal Value = 11.023470878601074
2025-03-24 17:01:44,254 - evaluators.py:132 - Goal Reach Probability = 0.2204694113120431
2025-03-24 17:01:44,254 - evaluators.py:134 - Trap Reach Probability = 0.7795305886879569
2025-03-24 17:01:44,254 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:01:44,254 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:01:44,254 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:01:44,254 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.029724130406975746
Epoch: 650, Critic Loss: 9.679720878601074
Epoch: 660, Actor Loss: 0.02915997803211212
Epoch: 660, Critic Loss: 9.431737899780273
Epoch: 670, Actor Loss: 0.028187723830342293
Epoch: 670, Critic Loss: 9.960638046264648
Epoch: 680, Actor Loss: 0.02412221021950245
Epoch: 680, Critic Loss: 8.556984901428223
Epoch: 690, Actor Loss: 0.026239866390824318
Epoch: 690, Critic Loss: 9.546425819396973
2025-03-24 17:02:17,424 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:02:20,868 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:02:20,868 - evaluators.py:130 - Average Virtual Goal Value = 10.022067070007324
2025-03-24 17:02:20,868 - evaluators.py:132 - Goal Reach Probability = 0.20044133872747333
2025-03-24 17:02:20,868 - evaluators.py:134 - Trap Reach Probability = 0.7995586612725266
2025-03-24 17:02:20,868 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:02:20,868 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:02:20,868 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:02:20,868 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.0371212512254715
Epoch: 700, Critic Loss: 9.089768409729004
2025-03-24 17:02:20,870 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 17:02:20,871 - father_agent.py:449 - Before training evaluation.
2025-03-24 17:02:20,871 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:02:28,029 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:02:28,030 - father_agent.py:564 - Average Virtual Goal Value = 10.21374797821045
2025-03-24 17:02:28,030 - father_agent.py:566 - Goal Reach Probability = 0.2042749574933204
2025-03-24 17:02:28,030 - father_agent.py:568 - Trap Reach Probability = 0.7957250425066796
2025-03-24 17:02:28,030 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:02:28,030 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:02:28,030 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:02:28,461 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:02:28,572 - father_agent.py:359 - Training agent on-policy
2025-03-24 17:02:32,633 - father_agent.py:306 - Step: 0, Training loss: 84.51068115234375
2025-03-24 17:02:32,684 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:02:40,713 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:02:40,714 - father_agent.py:564 - Average Virtual Goal Value = 10.235595703125
2025-03-24 17:02:40,714 - father_agent.py:566 - Goal Reach Probability = 0.2047119140625
2025-03-24 17:02:40,714 - father_agent.py:568 - Trap Reach Probability = 0.7952880859375
2025-03-24 17:02:40,714 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:02:40,714 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:02:40,714 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:02:40,751 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:02:42,706 - father_agent.py:306 - Step: 5, Training loss: 57.49220275878906
2025-03-24 17:02:43,793 - father_agent.py:306 - Step: 10, Training loss: 35.2355842590332
2025-03-24 17:02:49,727 - father_agent.py:306 - Step: 15, Training loss: 14.626727104187012
2025-03-24 17:02:50,979 - father_agent.py:306 - Step: 20, Training loss: 6.579671859741211
2025-03-24 17:02:52,312 - father_agent.py:306 - Step: 25, Training loss: 2.735753297805786
2025-03-24 17:02:53,574 - father_agent.py:306 - Step: 30, Training loss: 1.2483004331588745
2025-03-24 17:02:54,876 - father_agent.py:306 - Step: 35, Training loss: 0.69080650806427
2025-03-24 17:02:56,258 - father_agent.py:306 - Step: 40, Training loss: 0.45597049593925476
2025-03-24 17:02:57,611 - father_agent.py:306 - Step: 45, Training loss: 0.17783452570438385
2025-03-24 17:02:58,996 - father_agent.py:306 - Step: 50, Training loss: 0.26143789291381836
2025-03-24 17:03:00,395 - father_agent.py:306 - Step: 55, Training loss: 0.43467316031455994
2025-03-24 17:03:01,667 - father_agent.py:306 - Step: 60, Training loss: 0.37105774879455566
2025-03-24 17:03:03,093 - father_agent.py:306 - Step: 65, Training loss: 0.4010992646217346
2025-03-24 17:03:04,495 - father_agent.py:306 - Step: 70, Training loss: 0.25548243522644043
2025-03-24 17:03:05,796 - father_agent.py:306 - Step: 75, Training loss: 0.2157505750656128
2025-03-24 17:03:07,164 - father_agent.py:306 - Step: 80, Training loss: 0.28726181387901306
2025-03-24 17:03:08,549 - father_agent.py:306 - Step: 85, Training loss: 0.3043641746044159
2025-03-24 17:03:09,813 - father_agent.py:306 - Step: 90, Training loss: 0.4907054007053375
2025-03-24 17:03:11,151 - father_agent.py:306 - Step: 95, Training loss: 0.5103245377540588
2025-03-24 17:03:12,514 - father_agent.py:306 - Step: 100, Training loss: 0.4542675316333771
2025-03-24 17:03:12,556 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:03:21,705 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:03:21,705 - father_agent.py:564 - Average Virtual Goal Value = 0.5936407446861267
2025-03-24 17:03:21,705 - father_agent.py:566 - Goal Reach Probability = 0.01187281450760348
2025-03-24 17:03:21,705 - father_agent.py:568 - Trap Reach Probability = 0.9881271854923965
2025-03-24 17:03:21,705 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:03:21,705 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:03:21,705 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:03:21,744 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:03:24,350 - father_agent.py:306 - Step: 105, Training loss: 0.4167095720767975
2025-03-24 17:03:25,600 - father_agent.py:306 - Step: 110, Training loss: 0.8610892295837402
2025-03-24 17:03:26,837 - father_agent.py:306 - Step: 115, Training loss: 0.3583856523036957
2025-03-24 17:03:28,199 - father_agent.py:306 - Step: 120, Training loss: 0.4184083938598633
2025-03-24 17:03:29,464 - father_agent.py:306 - Step: 125, Training loss: 0.39276450872421265
2025-03-24 17:03:30,710 - father_agent.py:306 - Step: 130, Training loss: 0.4522552192211151
2025-03-24 17:03:31,990 - father_agent.py:306 - Step: 135, Training loss: 0.35372504591941833
2025-03-24 17:03:33,334 - father_agent.py:306 - Step: 140, Training loss: 0.5172601938247681
2025-03-24 17:03:34,699 - father_agent.py:306 - Step: 145, Training loss: 0.255560964345932
2025-03-24 17:03:36,008 - father_agent.py:306 - Step: 150, Training loss: 0.3956131935119629
2025-03-24 17:03:37,260 - father_agent.py:306 - Step: 155, Training loss: 0.36874696612358093
2025-03-24 17:03:38,688 - father_agent.py:306 - Step: 160, Training loss: 0.6565017700195312
2025-03-24 17:03:40,072 - father_agent.py:306 - Step: 165, Training loss: 0.2836376130580902
2025-03-24 17:03:41,458 - father_agent.py:306 - Step: 170, Training loss: 0.34490036964416504
2025-03-24 17:03:42,853 - father_agent.py:306 - Step: 175, Training loss: 0.4525904655456543
2025-03-24 17:03:44,093 - father_agent.py:306 - Step: 180, Training loss: 0.18941713869571686
2025-03-24 17:03:45,286 - father_agent.py:306 - Step: 185, Training loss: 0.5172660946846008
2025-03-24 17:03:46,672 - father_agent.py:306 - Step: 190, Training loss: 0.3426549434661865
2025-03-24 17:03:47,885 - father_agent.py:306 - Step: 195, Training loss: 0.21853099763393402
2025-03-24 17:03:49,247 - father_agent.py:306 - Step: 200, Training loss: 0.21575111150741577
2025-03-24 17:03:49,290 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:03:58,585 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:03:58,585 - father_agent.py:564 - Average Virtual Goal Value = 0.4843304753303528
2025-03-24 17:03:58,586 - father_agent.py:566 - Goal Reach Probability = 0.009686609686609686
2025-03-24 17:03:58,586 - father_agent.py:568 - Trap Reach Probability = 0.9903133903133903
2025-03-24 17:03:58,586 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:03:58,586 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:03:58,586 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:03:58,627 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:04:00,618 - father_agent.py:306 - Step: 205, Training loss: 0.6104930639266968
2025-03-24 17:04:01,850 - father_agent.py:306 - Step: 210, Training loss: 0.4599381685256958
2025-03-24 17:04:03,212 - father_agent.py:306 - Step: 215, Training loss: 0.30016836524009705
2025-03-24 17:04:04,512 - father_agent.py:306 - Step: 220, Training loss: 0.34040096402168274
2025-03-24 17:04:05,764 - father_agent.py:306 - Step: 225, Training loss: 0.4311867952346802
2025-03-24 17:04:07,136 - father_agent.py:306 - Step: 230, Training loss: 0.42637360095977783
2025-03-24 17:04:08,440 - father_agent.py:306 - Step: 235, Training loss: 0.4192524552345276
2025-03-24 17:04:09,705 - father_agent.py:306 - Step: 240, Training loss: 0.4548161029815674
2025-03-24 17:04:11,088 - father_agent.py:306 - Step: 245, Training loss: 0.3923680782318115
2025-03-24 17:04:12,379 - father_agent.py:306 - Step: 250, Training loss: 0.615251362323761
2025-03-24 17:04:13,723 - father_agent.py:306 - Step: 255, Training loss: 0.31429800391197205
2025-03-24 17:04:15,397 - father_agent.py:306 - Step: 260, Training loss: 0.2930520176887512
2025-03-24 17:04:16,940 - father_agent.py:306 - Step: 265, Training loss: 0.42143699526786804
2025-03-24 17:04:18,248 - father_agent.py:306 - Step: 270, Training loss: 0.2271817922592163
2025-03-24 17:04:19,621 - father_agent.py:306 - Step: 275, Training loss: 0.27086377143859863
2025-03-24 17:04:20,972 - father_agent.py:306 - Step: 280, Training loss: 0.487875759601593
2025-03-24 17:04:22,217 - father_agent.py:306 - Step: 285, Training loss: 0.38292306661605835
2025-03-24 17:04:23,505 - father_agent.py:306 - Step: 290, Training loss: 0.4486432671546936
2025-03-24 17:04:24,931 - father_agent.py:306 - Step: 295, Training loss: 0.3509826958179474
2025-03-24 17:04:26,219 - father_agent.py:306 - Step: 300, Training loss: 0.4381249248981476
2025-03-24 17:04:26,263 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:04:35,567 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:04:35,567 - father_agent.py:564 - Average Virtual Goal Value = 0.376091867685318
2025-03-24 17:04:35,567 - father_agent.py:566 - Goal Reach Probability = 0.00752183759301197
2025-03-24 17:04:35,567 - father_agent.py:568 - Trap Reach Probability = 0.992478162406988
2025-03-24 17:04:35,567 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:04:35,567 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:04:35,567 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:04:35,608 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:04:37,886 - father_agent.py:306 - Step: 305, Training loss: 0.4348043203353882
2025-03-24 17:04:39,262 - father_agent.py:306 - Step: 310, Training loss: 0.30092066526412964
2025-03-24 17:04:40,659 - father_agent.py:306 - Step: 315, Training loss: 0.22555148601531982
2025-03-24 17:04:41,897 - father_agent.py:306 - Step: 320, Training loss: 0.2789204716682434
2025-03-24 17:04:43,127 - father_agent.py:306 - Step: 325, Training loss: 0.4679211676120758
2025-03-24 17:04:44,531 - father_agent.py:306 - Step: 330, Training loss: 0.257256418466568
2025-03-24 17:04:45,793 - father_agent.py:306 - Step: 335, Training loss: 0.5735167264938354
2025-03-24 17:04:47,054 - father_agent.py:306 - Step: 340, Training loss: 0.3935773968696594
2025-03-24 17:04:48,439 - father_agent.py:306 - Step: 345, Training loss: 0.32740089297294617
2025-03-24 17:04:49,730 - father_agent.py:306 - Step: 350, Training loss: 0.32369673252105713
2025-03-24 17:04:51,086 - father_agent.py:306 - Step: 355, Training loss: 0.2926522493362427
2025-03-24 17:04:52,820 - father_agent.py:306 - Step: 360, Training loss: 0.4879711866378784
2025-03-24 17:04:54,245 - father_agent.py:306 - Step: 365, Training loss: 0.43082237243652344
2025-03-24 17:04:55,585 - father_agent.py:306 - Step: 370, Training loss: 0.3390537202358246
2025-03-24 17:04:56,912 - father_agent.py:306 - Step: 375, Training loss: 0.26454925537109375
2025-03-24 17:04:58,246 - father_agent.py:306 - Step: 380, Training loss: 0.19953468441963196
2025-03-24 17:04:59,557 - father_agent.py:306 - Step: 385, Training loss: 0.1935713291168213
2025-03-24 17:05:00,858 - father_agent.py:306 - Step: 390, Training loss: 0.19530148804187775
2025-03-24 17:05:02,189 - father_agent.py:306 - Step: 395, Training loss: 0.4966104328632355
2025-03-24 17:05:03,510 - father_agent.py:306 - Step: 400, Training loss: 0.13647842407226562
2025-03-24 17:05:03,557 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:05:13,161 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:05:13,161 - father_agent.py:564 - Average Virtual Goal Value = 0.6619184017181396
2025-03-24 17:05:13,161 - father_agent.py:566 - Goal Reach Probability = 0.013238368042115043
2025-03-24 17:05:13,161 - father_agent.py:568 - Trap Reach Probability = 0.9867616319578849
2025-03-24 17:05:13,161 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:05:13,161 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:05:13,161 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:05:13,203 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:05:15,200 - father_agent.py:306 - Step: 405, Training loss: 0.12569543719291687
2025-03-24 17:05:16,537 - father_agent.py:306 - Step: 410, Training loss: 0.3855103850364685
2025-03-24 17:05:17,749 - father_agent.py:306 - Step: 415, Training loss: 0.39025235176086426
2025-03-24 17:05:19,186 - father_agent.py:306 - Step: 420, Training loss: 0.5053964257240295
2025-03-24 17:05:20,412 - father_agent.py:306 - Step: 425, Training loss: 0.40998438000679016
2025-03-24 17:05:21,607 - father_agent.py:306 - Step: 430, Training loss: 0.27461498975753784
2025-03-24 17:05:23,003 - father_agent.py:306 - Step: 435, Training loss: 0.513909101486206
2025-03-24 17:05:24,343 - father_agent.py:306 - Step: 440, Training loss: 0.2878235876560211
2025-03-24 17:05:25,602 - father_agent.py:306 - Step: 445, Training loss: 0.4213443100452423
2025-03-24 17:05:26,898 - father_agent.py:306 - Step: 450, Training loss: 0.5689665079116821
2025-03-24 17:05:28,193 - father_agent.py:306 - Step: 455, Training loss: 0.5578866600990295
2025-03-24 17:05:29,476 - father_agent.py:306 - Step: 460, Training loss: 0.47738227248191833
2025-03-24 17:05:30,841 - father_agent.py:306 - Step: 465, Training loss: 0.6567355990409851
2025-03-24 17:05:32,091 - father_agent.py:306 - Step: 470, Training loss: 0.48194244503974915
2025-03-24 17:05:33,297 - father_agent.py:306 - Step: 475, Training loss: 0.336241751909256
2025-03-24 17:05:34,692 - father_agent.py:306 - Step: 480, Training loss: 0.32082682847976685
2025-03-24 17:05:35,915 - father_agent.py:306 - Step: 485, Training loss: 0.5090379118919373
2025-03-24 17:05:37,130 - father_agent.py:306 - Step: 490, Training loss: 0.4809598922729492
2025-03-24 17:05:38,474 - father_agent.py:306 - Step: 495, Training loss: 0.5920629501342773
2025-03-24 17:05:39,725 - father_agent.py:306 - Step: 500, Training loss: 0.4936843514442444
2025-03-24 17:05:39,768 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:05:48,823 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:05:48,823 - father_agent.py:564 - Average Virtual Goal Value = 1.6962659358978271
2025-03-24 17:05:48,823 - father_agent.py:566 - Goal Reach Probability = 0.033925318761384333
2025-03-24 17:05:48,823 - father_agent.py:568 - Trap Reach Probability = 0.9660746812386156
2025-03-24 17:05:48,824 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:05:48,824 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:05:48,824 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:05:48,871 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:05:50,902 - father_agent.py:306 - Step: 505, Training loss: 0.2531576454639435
2025-03-24 17:05:52,120 - father_agent.py:306 - Step: 510, Training loss: 0.36938315629959106
2025-03-24 17:05:53,348 - father_agent.py:306 - Step: 515, Training loss: 0.33731675148010254
2025-03-24 17:05:54,747 - father_agent.py:306 - Step: 520, Training loss: 0.4195353388786316
2025-03-24 17:05:55,958 - father_agent.py:306 - Step: 525, Training loss: 0.427335649728775
2025-03-24 17:05:57,183 - father_agent.py:306 - Step: 530, Training loss: 0.42088204622268677
2025-03-24 17:05:58,585 - father_agent.py:306 - Step: 535, Training loss: 0.4066208004951477
2025-03-24 17:05:59,817 - father_agent.py:306 - Step: 540, Training loss: 0.3414527475833893
2025-03-24 17:06:01,159 - father_agent.py:306 - Step: 545, Training loss: 0.5316030979156494
2025-03-24 17:06:02,453 - father_agent.py:306 - Step: 550, Training loss: 0.3820122480392456
2025-03-24 17:06:03,665 - father_agent.py:306 - Step: 555, Training loss: 0.5830716490745544
2025-03-24 17:06:05,017 - father_agent.py:306 - Step: 560, Training loss: 0.3629802167415619
2025-03-24 17:06:06,304 - father_agent.py:306 - Step: 565, Training loss: 0.6266231536865234
2025-03-24 17:06:07,498 - father_agent.py:306 - Step: 570, Training loss: 0.269298791885376
2025-03-24 17:06:08,852 - father_agent.py:306 - Step: 575, Training loss: 0.48076340556144714
2025-03-24 17:06:10,129 - father_agent.py:306 - Step: 580, Training loss: 0.45214107632637024
2025-03-24 17:06:11,354 - father_agent.py:306 - Step: 585, Training loss: 0.31620076298713684
2025-03-24 17:06:12,683 - father_agent.py:306 - Step: 590, Training loss: 0.41448765993118286
2025-03-24 17:06:14,001 - father_agent.py:306 - Step: 595, Training loss: 0.45728832483291626
2025-03-24 17:06:15,329 - father_agent.py:306 - Step: 600, Training loss: 0.5842829346656799
2025-03-24 17:06:15,371 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:06:25,092 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:06:25,092 - father_agent.py:564 - Average Virtual Goal Value = 1.419948935508728
2025-03-24 17:06:25,092 - father_agent.py:566 - Goal Reach Probability = 0.028398978565348603
2025-03-24 17:06:25,092 - father_agent.py:568 - Trap Reach Probability = 0.9716010214346514
2025-03-24 17:06:25,092 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:06:25,093 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:06:25,093 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:06:25,133 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:06:27,075 - father_agent.py:306 - Step: 605, Training loss: 0.3545672297477722
2025-03-24 17:06:28,407 - father_agent.py:306 - Step: 610, Training loss: 0.5702004432678223
2025-03-24 17:06:29,655 - father_agent.py:306 - Step: 615, Training loss: 0.3358944356441498
2025-03-24 17:06:31,034 - father_agent.py:306 - Step: 620, Training loss: 0.5022041201591492
2025-03-24 17:06:32,282 - father_agent.py:306 - Step: 625, Training loss: 0.34857434034347534
2025-03-24 17:06:33,519 - father_agent.py:306 - Step: 630, Training loss: 0.3519662320613861
2025-03-24 17:06:34,922 - father_agent.py:306 - Step: 635, Training loss: 0.4109540283679962
2025-03-24 17:06:36,152 - father_agent.py:306 - Step: 640, Training loss: 0.31992509961128235
2025-03-24 17:06:37,384 - father_agent.py:306 - Step: 645, Training loss: 0.6313595175743103
2025-03-24 17:06:38,743 - father_agent.py:306 - Step: 650, Training loss: 0.5481530427932739
2025-03-24 17:06:39,935 - father_agent.py:306 - Step: 655, Training loss: 0.554185152053833
2025-03-24 17:06:41,259 - father_agent.py:306 - Step: 660, Training loss: 0.7201042771339417
2025-03-24 17:06:42,548 - father_agent.py:306 - Step: 665, Training loss: 0.4443777799606323
2025-03-24 17:06:43,771 - father_agent.py:306 - Step: 670, Training loss: 0.6233581900596619
2025-03-24 17:06:45,122 - father_agent.py:306 - Step: 675, Training loss: 0.4280359745025635
2025-03-24 17:06:46,406 - father_agent.py:306 - Step: 680, Training loss: 0.5164458155632019
2025-03-24 17:06:47,669 - father_agent.py:306 - Step: 685, Training loss: 0.3610367774963379
2025-03-24 17:06:49,009 - father_agent.py:306 - Step: 690, Training loss: 0.4217316508293152
2025-03-24 17:06:50,399 - father_agent.py:306 - Step: 695, Training loss: 0.4175824522972107
2025-03-24 17:06:51,633 - father_agent.py:306 - Step: 700, Training loss: 0.3493483364582062
2025-03-24 17:06:51,675 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:07:01,728 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:07:01,728 - father_agent.py:564 - Average Virtual Goal Value = 1.0973831415176392
2025-03-24 17:07:01,728 - father_agent.py:566 - Goal Reach Probability = 0.02194766326452306
2025-03-24 17:07:01,728 - father_agent.py:568 - Trap Reach Probability = 0.978052336735477
2025-03-24 17:07:01,728 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:07:01,728 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:07:01,728 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:07:01,770 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:07:01,888 - father_agent.py:455 - Training finished.
2025-03-24 17:07:01,894 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:07:02,233 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 17:07:11,686 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:07:11,686 - father_agent.py:564 - Average Virtual Goal Value = 1.151012897491455
2025-03-24 17:07:11,686 - father_agent.py:566 - Goal Reach Probability = 0.02302025782688766
2025-03-24 17:07:11,686 - father_agent.py:568 - Trap Reach Probability = 0.9769797421731123
2025-03-24 17:07:11,686 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:07:11,686 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:07:11,686 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:07:11,737 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:07:29,870 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:07:29,870 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:07:29,870 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:07:29,870 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:07:29,870 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:07:29,870 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:07:29,870 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:07:29,887 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:08:12,402 - cloned_fsc_actor_policy.py:189 - Epoch 0, Loss: 1.9299
2025-03-24 17:08:12,402 - cloned_fsc_actor_policy.py:190 - Epoch 0, Accuracy: 0.0506
2025-03-24 17:08:12,470 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:08:29,093 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:08:29,093 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:08:29,093 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:08:29,093 - evaluators.py:134 - Trap Reach Probability = 0.0
2025-03-24 17:08:29,093 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:08:29,093 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:08:29,093 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:08:32,552 - cloned_fsc_actor_policy.py:189 - Epoch 1000, Loss: 0.4733
2025-03-24 17:08:32,553 - cloned_fsc_actor_policy.py:190 - Epoch 1000, Accuracy: 0.9193
2025-03-24 17:08:35,832 - cloned_fsc_actor_policy.py:189 - Epoch 2000, Loss: 0.0245
2025-03-24 17:08:35,833 - cloned_fsc_actor_policy.py:190 - Epoch 2000, Accuracy: 0.9994
2025-03-24 17:08:39,279 - cloned_fsc_actor_policy.py:189 - Epoch 3000, Loss: 0.0078
2025-03-24 17:08:39,279 - cloned_fsc_actor_policy.py:190 - Epoch 3000, Accuracy: 0.9993
2025-03-24 17:08:42,593 - cloned_fsc_actor_policy.py:189 - Epoch 4000, Loss: 0.0042
2025-03-24 17:08:42,593 - cloned_fsc_actor_policy.py:190 - Epoch 4000, Accuracy: 0.9994
2025-03-24 17:08:45,985 - cloned_fsc_actor_policy.py:189 - Epoch 5000, Loss: 0.0026
2025-03-24 17:08:45,985 - cloned_fsc_actor_policy.py:190 - Epoch 5000, Accuracy: 0.9993
2025-03-24 17:08:46,030 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:09:03,403 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:09:03,403 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:09:03,403 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:09:03,403 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:09:03,404 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:09:03,404 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:09:03,404 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:09:06,743 - cloned_fsc_actor_policy.py:189 - Epoch 6000, Loss: 0.0014
2025-03-24 17:09:06,743 - cloned_fsc_actor_policy.py:190 - Epoch 6000, Accuracy: 0.9996
2025-03-24 17:09:10,110 - cloned_fsc_actor_policy.py:189 - Epoch 7000, Loss: 0.0006
2025-03-24 17:09:10,110 - cloned_fsc_actor_policy.py:190 - Epoch 7000, Accuracy: 1.0000
2025-03-24 17:09:13,493 - cloned_fsc_actor_policy.py:189 - Epoch 8000, Loss: 0.0003
2025-03-24 17:09:13,493 - cloned_fsc_actor_policy.py:190 - Epoch 8000, Accuracy: 1.0000
2025-03-24 17:09:16,858 - cloned_fsc_actor_policy.py:189 - Epoch 9000, Loss: 0.0001
2025-03-24 17:09:16,858 - cloned_fsc_actor_policy.py:190 - Epoch 9000, Accuracy: 1.0000
2025-03-24 17:09:20,183 - cloned_fsc_actor_policy.py:189 - Epoch 10000, Loss: 0.0001
2025-03-24 17:09:20,183 - cloned_fsc_actor_policy.py:190 - Epoch 10000, Accuracy: 1.0000
2025-03-24 17:09:20,225 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:09:36,862 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:09:36,862 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:09:36,862 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:09:36,862 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:09:36,862 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:09:36,862 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:09:36,862 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:09:40,224 - cloned_fsc_actor_policy.py:189 - Epoch 11000, Loss: 0.0000
2025-03-24 17:09:40,224 - cloned_fsc_actor_policy.py:190 - Epoch 11000, Accuracy: 1.0000
2025-03-24 17:09:43,593 - cloned_fsc_actor_policy.py:189 - Epoch 12000, Loss: 0.0000
2025-03-24 17:09:43,594 - cloned_fsc_actor_policy.py:190 - Epoch 12000, Accuracy: 1.0000
2025-03-24 17:09:46,943 - cloned_fsc_actor_policy.py:189 - Epoch 13000, Loss: 0.0000
2025-03-24 17:09:46,943 - cloned_fsc_actor_policy.py:190 - Epoch 13000, Accuracy: 1.0000
2025-03-24 17:09:50,365 - cloned_fsc_actor_policy.py:189 - Epoch 14000, Loss: 0.0000
2025-03-24 17:09:50,365 - cloned_fsc_actor_policy.py:190 - Epoch 14000, Accuracy: 1.0000
2025-03-24 17:09:53,622 - cloned_fsc_actor_policy.py:189 - Epoch 15000, Loss: 0.0000
2025-03-24 17:09:53,622 - cloned_fsc_actor_policy.py:190 - Epoch 15000, Accuracy: 1.0000
2025-03-24 17:09:53,660 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:10:10,468 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:10:10,469 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:10:10,469 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:10:10,469 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:10:10,469 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:10:10,469 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:10:10,469 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:10:13,817 - cloned_fsc_actor_policy.py:189 - Epoch 16000, Loss: 0.0000
2025-03-24 17:10:13,817 - cloned_fsc_actor_policy.py:190 - Epoch 16000, Accuracy: 1.0000
2025-03-24 17:10:17,167 - cloned_fsc_actor_policy.py:189 - Epoch 17000, Loss: 0.0000
2025-03-24 17:10:17,168 - cloned_fsc_actor_policy.py:190 - Epoch 17000, Accuracy: 1.0000
2025-03-24 17:10:20,517 - cloned_fsc_actor_policy.py:189 - Epoch 18000, Loss: 0.0000
2025-03-24 17:10:20,517 - cloned_fsc_actor_policy.py:190 - Epoch 18000, Accuracy: 1.0000
2025-03-24 17:10:23,845 - cloned_fsc_actor_policy.py:189 - Epoch 19000, Loss: 0.0000
2025-03-24 17:10:23,845 - cloned_fsc_actor_policy.py:190 - Epoch 19000, Accuracy: 1.0000
2025-03-24 17:10:27,518 - cloned_fsc_actor_policy.py:189 - Epoch 20000, Loss: 0.0000
2025-03-24 17:10:27,518 - cloned_fsc_actor_policy.py:190 - Epoch 20000, Accuracy: 1.0000
2025-03-24 17:10:27,567 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:10:44,175 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:10:44,175 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:10:44,175 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:10:44,175 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:10:44,175 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:10:44,175 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:10:44,175 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:10:47,567 - cloned_fsc_actor_policy.py:189 - Epoch 21000, Loss: 0.0000
2025-03-24 17:10:47,567 - cloned_fsc_actor_policy.py:190 - Epoch 21000, Accuracy: 1.0000
2025-03-24 17:10:50,926 - cloned_fsc_actor_policy.py:189 - Epoch 22000, Loss: 0.0000
2025-03-24 17:10:50,927 - cloned_fsc_actor_policy.py:190 - Epoch 22000, Accuracy: 1.0000
2025-03-24 17:10:54,357 - cloned_fsc_actor_policy.py:189 - Epoch 23000, Loss: 0.0000
2025-03-24 17:10:54,357 - cloned_fsc_actor_policy.py:190 - Epoch 23000, Accuracy: 1.0000
2025-03-24 17:10:57,604 - cloned_fsc_actor_policy.py:189 - Epoch 24000, Loss: 0.0000
2025-03-24 17:10:57,604 - cloned_fsc_actor_policy.py:190 - Epoch 24000, Accuracy: 1.0000
2025-03-24 17:11:00,917 - cloned_fsc_actor_policy.py:189 - Epoch 25000, Loss: 0.0000
2025-03-24 17:11:00,918 - cloned_fsc_actor_policy.py:190 - Epoch 25000, Accuracy: 1.0000
2025-03-24 17:11:00,954 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:11:18,397 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:11:18,398 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:11:18,398 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:11:18,398 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:11:18,398 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:11:18,398 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:11:18,398 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:11:21,669 - cloned_fsc_actor_policy.py:189 - Epoch 26000, Loss: 0.0000
2025-03-24 17:11:21,669 - cloned_fsc_actor_policy.py:190 - Epoch 26000, Accuracy: 1.0000
2025-03-24 17:11:25,127 - cloned_fsc_actor_policy.py:189 - Epoch 27000, Loss: 0.0000
2025-03-24 17:11:25,128 - cloned_fsc_actor_policy.py:190 - Epoch 27000, Accuracy: 1.0000
2025-03-24 17:11:28,442 - cloned_fsc_actor_policy.py:189 - Epoch 28000, Loss: 0.0000
2025-03-24 17:11:28,443 - cloned_fsc_actor_policy.py:190 - Epoch 28000, Accuracy: 1.0000
2025-03-24 17:11:31,685 - cloned_fsc_actor_policy.py:189 - Epoch 29000, Loss: 0.0000
2025-03-24 17:11:31,685 - cloned_fsc_actor_policy.py:190 - Epoch 29000, Accuracy: 1.0000
2025-03-24 17:11:35,098 - cloned_fsc_actor_policy.py:189 - Epoch 30000, Loss: 0.0000
2025-03-24 17:11:35,098 - cloned_fsc_actor_policy.py:190 - Epoch 30000, Accuracy: 1.0000
2025-03-24 17:11:35,131 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:11:52,674 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:11:52,675 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:11:52,675 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:11:52,675 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:11:52,675 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:11:52,675 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:11:52,675 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:11:55,916 - cloned_fsc_actor_policy.py:189 - Epoch 31000, Loss: 0.0000
2025-03-24 17:11:55,916 - cloned_fsc_actor_policy.py:190 - Epoch 31000, Accuracy: 1.0000
2025-03-24 17:11:59,276 - cloned_fsc_actor_policy.py:189 - Epoch 32000, Loss: 0.0000
2025-03-24 17:11:59,276 - cloned_fsc_actor_policy.py:190 - Epoch 32000, Accuracy: 1.0000
2025-03-24 17:12:02,687 - cloned_fsc_actor_policy.py:189 - Epoch 33000, Loss: 0.0000
2025-03-24 17:12:02,687 - cloned_fsc_actor_policy.py:190 - Epoch 33000, Accuracy: 1.0000
2025-03-24 17:12:05,927 - cloned_fsc_actor_policy.py:189 - Epoch 34000, Loss: 0.0000
2025-03-24 17:12:05,927 - cloned_fsc_actor_policy.py:190 - Epoch 34000, Accuracy: 1.0000
2025-03-24 17:12:09,207 - cloned_fsc_actor_policy.py:189 - Epoch 35000, Loss: 0.0000
2025-03-24 17:12:09,207 - cloned_fsc_actor_policy.py:190 - Epoch 35000, Accuracy: 1.0000
2025-03-24 17:12:09,252 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:12:26,298 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:12:26,298 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:12:26,298 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:12:26,298 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:12:26,298 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:12:26,298 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:12:26,298 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:12:29,850 - cloned_fsc_actor_policy.py:189 - Epoch 36000, Loss: 0.0000
2025-03-24 17:12:29,850 - cloned_fsc_actor_policy.py:190 - Epoch 36000, Accuracy: 1.0000
2025-03-24 17:12:33,840 - cloned_fsc_actor_policy.py:189 - Epoch 37000, Loss: 0.0000
2025-03-24 17:12:33,840 - cloned_fsc_actor_policy.py:190 - Epoch 37000, Accuracy: 1.0000
2025-03-24 17:12:37,299 - cloned_fsc_actor_policy.py:189 - Epoch 38000, Loss: 0.0000
2025-03-24 17:12:37,299 - cloned_fsc_actor_policy.py:190 - Epoch 38000, Accuracy: 1.0000
2025-03-24 17:12:40,646 - cloned_fsc_actor_policy.py:189 - Epoch 39000, Loss: 0.0000
2025-03-24 17:12:40,646 - cloned_fsc_actor_policy.py:190 - Epoch 39000, Accuracy: 1.0000
2025-03-24 17:12:43,990 - cloned_fsc_actor_policy.py:189 - Epoch 40000, Loss: 0.0000
2025-03-24 17:12:43,990 - cloned_fsc_actor_policy.py:190 - Epoch 40000, Accuracy: 1.0000
2025-03-24 17:12:44,038 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:13:00,732 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:13:00,732 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:13:00,732 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:13:00,732 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:13:00,732 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:13:00,732 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:13:00,732 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:13:04,007 - cloned_fsc_actor_policy.py:189 - Epoch 41000, Loss: 0.0000
2025-03-24 17:13:04,007 - cloned_fsc_actor_policy.py:190 - Epoch 41000, Accuracy: 1.0000
2025-03-24 17:13:07,453 - cloned_fsc_actor_policy.py:189 - Epoch 42000, Loss: 0.0000
2025-03-24 17:13:07,454 - cloned_fsc_actor_policy.py:190 - Epoch 42000, Accuracy: 1.0000
2025-03-24 17:13:11,179 - cloned_fsc_actor_policy.py:189 - Epoch 43000, Loss: 0.0000
2025-03-24 17:13:11,179 - cloned_fsc_actor_policy.py:190 - Epoch 43000, Accuracy: 1.0000
2025-03-24 17:13:14,592 - cloned_fsc_actor_policy.py:189 - Epoch 44000, Loss: 0.0000
2025-03-24 17:13:14,592 - cloned_fsc_actor_policy.py:190 - Epoch 44000, Accuracy: 1.0000
2025-03-24 17:13:17,956 - cloned_fsc_actor_policy.py:189 - Epoch 45000, Loss: 0.0000
2025-03-24 17:13:17,956 - cloned_fsc_actor_policy.py:190 - Epoch 45000, Accuracy: 1.0000
2025-03-24 17:13:17,997 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:13:34,599 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:13:34,599 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:13:34,599 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:13:34,599 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:13:34,600 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:13:34,600 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:13:34,600 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:13:37,926 - cloned_fsc_actor_policy.py:189 - Epoch 46000, Loss: 0.0000
2025-03-24 17:13:37,926 - cloned_fsc_actor_policy.py:190 - Epoch 46000, Accuracy: 1.0000
2025-03-24 17:13:41,277 - cloned_fsc_actor_policy.py:189 - Epoch 47000, Loss: 0.0000
2025-03-24 17:13:41,277 - cloned_fsc_actor_policy.py:190 - Epoch 47000, Accuracy: 1.0000
2025-03-24 17:13:44,838 - cloned_fsc_actor_policy.py:189 - Epoch 48000, Loss: 0.0000
2025-03-24 17:13:44,838 - cloned_fsc_actor_policy.py:190 - Epoch 48000, Accuracy: 1.0000
2025-03-24 17:13:48,428 - cloned_fsc_actor_policy.py:189 - Epoch 49000, Loss: 0.0000
2025-03-24 17:13:48,428 - cloned_fsc_actor_policy.py:190 - Epoch 49000, Accuracy: 1.0000
2025-03-24 17:13:51,723 - cloned_fsc_actor_policy.py:189 - Epoch 50000, Loss: 0.0000
2025-03-24 17:13:51,723 - cloned_fsc_actor_policy.py:190 - Epoch 50000, Accuracy: 1.0000
2025-03-24 17:13:51,760 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:14:08,299 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:14:08,299 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:14:08,299 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:14:08,299 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:14:08,299 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:14:08,299 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:14:08,299 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:14:11,644 - cloned_fsc_actor_policy.py:189 - Epoch 51000, Loss: 0.0000
2025-03-24 17:14:11,644 - cloned_fsc_actor_policy.py:190 - Epoch 51000, Accuracy: 1.0000
2025-03-24 17:14:15,039 - cloned_fsc_actor_policy.py:189 - Epoch 52000, Loss: 0.0000
2025-03-24 17:14:15,039 - cloned_fsc_actor_policy.py:190 - Epoch 52000, Accuracy: 1.0000
2025-03-24 17:14:18,353 - cloned_fsc_actor_policy.py:189 - Epoch 53000, Loss: 0.0000
2025-03-24 17:14:18,353 - cloned_fsc_actor_policy.py:190 - Epoch 53000, Accuracy: 1.0000
2025-03-24 17:14:21,695 - cloned_fsc_actor_policy.py:189 - Epoch 54000, Loss: 0.0000
2025-03-24 17:14:21,695 - cloned_fsc_actor_policy.py:190 - Epoch 54000, Accuracy: 1.0000
2025-03-24 17:14:25,058 - cloned_fsc_actor_policy.py:189 - Epoch 55000, Loss: 0.0000
2025-03-24 17:14:25,058 - cloned_fsc_actor_policy.py:190 - Epoch 55000, Accuracy: 1.0000
2025-03-24 17:14:25,099 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:14:42,297 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:14:42,298 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:14:42,298 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:14:42,298 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:14:42,298 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:14:42,298 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:14:42,298 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:14:45,666 - cloned_fsc_actor_policy.py:189 - Epoch 56000, Loss: 0.0000
2025-03-24 17:14:45,666 - cloned_fsc_actor_policy.py:190 - Epoch 56000, Accuracy: 1.0000
2025-03-24 17:14:49,009 - cloned_fsc_actor_policy.py:189 - Epoch 57000, Loss: 0.0000
2025-03-24 17:14:49,009 - cloned_fsc_actor_policy.py:190 - Epoch 57000, Accuracy: 1.0000
2025-03-24 17:14:52,428 - cloned_fsc_actor_policy.py:189 - Epoch 58000, Loss: 0.0000
2025-03-24 17:14:52,428 - cloned_fsc_actor_policy.py:190 - Epoch 58000, Accuracy: 1.0000
2025-03-24 17:14:55,671 - cloned_fsc_actor_policy.py:189 - Epoch 59000, Loss: 0.0000
2025-03-24 17:14:55,671 - cloned_fsc_actor_policy.py:190 - Epoch 59000, Accuracy: 1.0000
2025-03-24 17:14:59,091 - cloned_fsc_actor_policy.py:189 - Epoch 60000, Loss: 0.0000
2025-03-24 17:14:59,091 - cloned_fsc_actor_policy.py:190 - Epoch 60000, Accuracy: 1.0000
2025-03-24 17:14:59,129 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:15:16,323 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:15:16,323 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:15:16,323 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:15:16,323 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:15:16,323 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:15:16,323 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:15:16,323 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:15:17,176 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:15:33,315 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:15:33,315 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:15:33,315 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:15:33,315 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:15:33,315 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:15:33,315 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:15:33,315 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:15:33,894 - pomdp.py:349 - unfolding 5-FSC template into POMDP...
2025-03-24 17:15:34,797 - pomdp.py:355 - constructed quotient MDP having 33898 states and 617106 actions.
2025-03-24 17:15:36,324 - rl_family_extractor.py:329 - Building family from FFNN...
2025-03-24 17:15:36,422 - rl_family_extractor.py:357 - Number of misses: 0 out of 1000
2025-03-24 17:15:36,422 - rl_family_extractor.py:359 - Number of complete misses: 0 out of 1000
2025-03-24 17:15:36,423 - statistic.py:67 - synthesis initiated, design space: 1
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 0.01 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 100 %
DTMC stats: avg DTMC size: 310, iterations: 1

optimum: 0.140482
--------------------
2025-03-24 17:15:36,434 - statistic.py:67 - synthesis initiated, design space: 1e609
> progress 0.0%, elapsed 3 s, estimated 3001975 s (34 days), iters = {DTMC: 340}, opt = 0.1405
> progress 0.0%, elapsed 6 s, estimated 6007452 s (69 days), iters = {DTMC: 680}, opt = 0.1405
> progress 0.0%, elapsed 9 s, estimated 9008830 s (104 days), iters = {DTMC: 1020}, opt = 0.1405
> progress 0.0%, elapsed 12 s, estimated 12013195 s (139 days), iters = {DTMC: 1359}, opt = 0.1405
> progress 0.0%, elapsed 15 s, estimated 15020642 s (173 days), iters = {DTMC: 1642}, opt = 0.1405
> progress 0.0%, elapsed 18 s, estimated 18023245 s (208 days), iters = {DTMC: 1976}, opt = 0.1405
> progress 0.0%, elapsed 21 s, estimated 21028465 s (243 days), iters = {DTMC: 2323}, opt = 0.1405
> progress 0.0%, elapsed 24 s, estimated 24032625 s (278 days), iters = {DTMC: 2662}, opt = 0.1405
> progress 0.0%, elapsed 27 s, estimated 27038085 s (312 days), iters = {DTMC: 3000}, opt = 0.1405
> progress 0.0%, elapsed 30 s, estimated 30045359 s (347 days), iters = {DTMC: 3338}, opt = 0.1405
> progress 0.0%, elapsed 33 s, estimated 33050868 s (1 year), iters = {DTMC: 3678}, opt = 0.1405
> progress 0.0%, elapsed 36 s, estimated 36059442 s (1 year), iters = {DTMC: 4020}, opt = 0.1405
> progress 0.0%, elapsed 39 s, estimated 39066184 s (1 year), iters = {DTMC: 4358}, opt = 0.1405
> progress 0.0%, elapsed 42 s, estimated 42068385 s (1 year), iters = {DTMC: 4694}, opt = 0.1405
> progress 0.0%, elapsed 45 s, estimated 45071003 s (1 year), iters = {DTMC: 5033}, opt = 0.1405
> progress 0.0%, elapsed 48 s, estimated 48073573 s (1 year), iters = {DTMC: 5376}, opt = 0.1405
> progress 0.0%, elapsed 51 s, estimated 51076713 s (1 year), iters = {DTMC: 5720}, opt = 0.1405
> progress 0.0%, elapsed 54 s, estimated 54077419 s (1 year), iters = {DTMC: 6054}, opt = 0.1405
> progress 0.0%, elapsed 57 s, estimated 57081218 s (1 year), iters = {DTMC: 6391}, opt = 0.1405
2025-03-24 17:16:36,441 - synthesizer_onebyone.py:19 - Time limit reached
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.01 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 0 %
DTMC stats: avg DTMC size: 212, iterations: 6725

optimum: 0.140482
--------------------
2025-03-24 17:16:36,442 - synthesizer_rl_storm_paynt.py:280 - No improving assignment found.
2025-03-24 17:16:36,443 - storm_pomdp_control.py:246 - Interactive Storm resumed
2025-03-24 17:16:36,443 - storm_pomdp_control.py:291 - Updating FSC values in Storm
2025-03-24 17:17:07,470 - storm_pomdp_control.py:305 - Pausing Storm
-----------Storm-----------               
Value = 0.23461992224252154 | Time elapsed = 7698.7s | FSC size = 1636

2025-03-24 17:18:49,735 - synthesizer_pomdp.py:253 - Timeout for PAYNT started
2025-03-24 17:18:50,109 - synthesizer_ar_storm.py:136 - Resuming synthesis
2025-03-24 17:18:50,116 - synthesizer_ar_storm.py:147 - PAYNT's value is better. Prioritizing synthesis results
2025-03-24 17:19:19,931 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 17:19:19,982 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:19:31,144 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:19:31,144 - father_agent.py:564 - Average Virtual Goal Value = 1.1801338195800781
2025-03-24 17:19:31,144 - father_agent.py:566 - Goal Reach Probability = 0.023602675482432535
2025-03-24 17:19:31,144 - father_agent.py:568 - Trap Reach Probability = 0.9763973245175674
2025-03-24 17:19:31,144 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:19:31,144 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:19:31,144 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:19:35,074 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:19:39,106 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:19:39,106 - evaluators.py:130 - Average Virtual Goal Value = 0.021235931664705276
2025-03-24 17:19:39,106 - evaluators.py:132 - Goal Reach Probability = 0.00042471862391165854
2025-03-24 17:19:39,106 - evaluators.py:134 - Trap Reach Probability = 0.9995752813760883
2025-03-24 17:19:39,106 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:19:39,106 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:19:39,106 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:19:39,106 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 2.1610758304595947
Epoch: 0, Critic Loss: 13.875938415527344
Epoch: 10, Actor Loss: 0.19279952347278595
Epoch: 10, Critic Loss: 12.546222686767578
Epoch: 20, Actor Loss: 0.1357908844947815
Epoch: 20, Critic Loss: 11.509772300720215
Epoch: 30, Actor Loss: 0.11023619025945663
Epoch: 30, Critic Loss: 9.187494277954102
Epoch: 40, Actor Loss: 0.0880478098988533
Epoch: 40, Critic Loss: 6.932360649108887
2025-03-24 17:20:12,041 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:20:15,719 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:20:15,719 - evaluators.py:130 - Average Virtual Goal Value = 5.579464912414551
2025-03-24 17:20:15,719 - evaluators.py:132 - Goal Reach Probability = 0.11158930218260067
2025-03-24 17:20:15,719 - evaluators.py:134 - Trap Reach Probability = 0.8884106978173993
2025-03-24 17:20:15,719 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:20:15,719 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:20:15,719 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:20:15,719 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.07418923825025558
Epoch: 50, Critic Loss: 13.020641326904297
Epoch: 60, Actor Loss: 0.06189549341797829
Epoch: 60, Critic Loss: 8.137070655822754
Epoch: 70, Actor Loss: 0.05564044788479805
Epoch: 70, Critic Loss: 13.768228530883789
Epoch: 80, Actor Loss: 0.03951537236571312
Epoch: 80, Critic Loss: 13.18649959564209
Epoch: 90, Actor Loss: 0.035186003893613815
Epoch: 90, Critic Loss: 15.627561569213867
2025-03-24 17:20:48,939 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:20:52,156 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:20:52,156 - evaluators.py:130 - Average Virtual Goal Value = 9.336697578430176
2025-03-24 17:20:52,156 - evaluators.py:132 - Goal Reach Probability = 0.18673395818312905
2025-03-24 17:20:52,156 - evaluators.py:134 - Trap Reach Probability = 0.8132660418168709
2025-03-24 17:20:52,156 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:20:52,156 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:20:52,156 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:20:52,156 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.031189536675810814
Epoch: 100, Critic Loss: 14.996590614318848
Epoch: 110, Actor Loss: 0.017394309863448143
Epoch: 110, Critic Loss: 11.181901931762695
Epoch: 120, Actor Loss: 0.03555367887020111
Epoch: 120, Critic Loss: 12.06637954711914
Epoch: 130, Actor Loss: 0.027678703889250755
Epoch: 130, Critic Loss: 10.78701400756836
Epoch: 140, Actor Loss: 0.022463945671916008
Epoch: 140, Critic Loss: 4.93132209777832
2025-03-24 17:21:25,806 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:21:29,176 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:21:29,176 - evaluators.py:130 - Average Virtual Goal Value = 8.76344108581543
2025-03-24 17:21:29,176 - evaluators.py:132 - Goal Reach Probability = 0.17526881720430107
2025-03-24 17:21:29,176 - evaluators.py:134 - Trap Reach Probability = 0.8247311827956989
2025-03-24 17:21:29,176 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:21:29,176 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:21:29,176 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:21:29,176 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.029253626242280006
Epoch: 150, Critic Loss: 13.78529167175293
Epoch: 160, Actor Loss: 0.01852656900882721
Epoch: 160, Critic Loss: 10.618572235107422
Epoch: 170, Actor Loss: 0.02439240925014019
Epoch: 170, Critic Loss: 7.621135711669922
Epoch: 180, Actor Loss: 0.02342771738767624
Epoch: 180, Critic Loss: 10.071796417236328
Epoch: 190, Actor Loss: 0.025762075558304787
Epoch: 190, Critic Loss: 14.756240844726562
2025-03-24 17:22:01,646 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:22:05,034 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:22:05,034 - evaluators.py:130 - Average Virtual Goal Value = 9.065054893493652
2025-03-24 17:22:05,034 - evaluators.py:132 - Goal Reach Probability = 0.18130110202630642
2025-03-24 17:22:05,034 - evaluators.py:134 - Trap Reach Probability = 0.8186988979736936
2025-03-24 17:22:05,034 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:22:05,034 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:22:05,034 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:22:05,034 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.026095928624272346
Epoch: 200, Critic Loss: 9.613780975341797
Epoch: 210, Actor Loss: 0.02537023276090622
Epoch: 210, Critic Loss: 10.368745803833008
Epoch: 220, Actor Loss: 0.019653664901852608
Epoch: 220, Critic Loss: 14.520185470581055
Epoch: 230, Actor Loss: 0.015857476741075516
Epoch: 230, Critic Loss: 12.038883209228516
Epoch: 240, Actor Loss: 0.04391888529062271
Epoch: 240, Critic Loss: 12.050628662109375
2025-03-24 17:22:37,498 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:22:41,093 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:22:41,093 - evaluators.py:130 - Average Virtual Goal Value = 10.02223014831543
2025-03-24 17:22:41,093 - evaluators.py:132 - Goal Reach Probability = 0.20044460911448686
2025-03-24 17:22:41,093 - evaluators.py:134 - Trap Reach Probability = 0.7995553908855132
2025-03-24 17:22:41,093 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:22:41,093 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:22:41,093 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:22:41,094 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.018353570252656937
Epoch: 250, Critic Loss: 12.563552856445312
Epoch: 260, Actor Loss: 0.03342650830745697
Epoch: 260, Critic Loss: 12.587846755981445
Epoch: 270, Actor Loss: 0.023896193131804466
Epoch: 270, Critic Loss: 10.817841529846191
Epoch: 280, Actor Loss: 0.017445530742406845
Epoch: 280, Critic Loss: 6.955862045288086
Epoch: 290, Actor Loss: 0.016754090785980225
Epoch: 290, Critic Loss: 14.630843162536621
2025-03-24 17:23:14,650 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:23:17,972 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:23:17,973 - evaluators.py:130 - Average Virtual Goal Value = 9.31283950805664
2025-03-24 17:23:17,973 - evaluators.py:132 - Goal Reach Probability = 0.18625678119349007
2025-03-24 17:23:17,973 - evaluators.py:134 - Trap Reach Probability = 0.8137432188065099
2025-03-24 17:23:17,973 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:23:17,973 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:23:17,973 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:23:17,973 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.023239197209477425
Epoch: 300, Critic Loss: 11.442691802978516
Epoch: 310, Actor Loss: 0.020393498241901398
Epoch: 310, Critic Loss: 14.27817153930664
Epoch: 320, Actor Loss: 0.020073432475328445
Epoch: 320, Critic Loss: 5.901671409606934
Epoch: 330, Actor Loss: 0.01661577634513378
Epoch: 330, Critic Loss: 10.041399002075195
Epoch: 340, Actor Loss: 0.017387384548783302
Epoch: 340, Critic Loss: 8.050607681274414
2025-03-24 17:23:51,237 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:23:55,028 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:23:55,028 - evaluators.py:130 - Average Virtual Goal Value = 9.476765632629395
2025-03-24 17:23:55,028 - evaluators.py:132 - Goal Reach Probability = 0.18953530918404685
2025-03-24 17:23:55,028 - evaluators.py:134 - Trap Reach Probability = 0.8104646908159532
2025-03-24 17:23:55,029 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:23:55,029 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:23:55,029 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:23:55,029 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.030909813940525055
Epoch: 350, Critic Loss: 7.186870574951172
Epoch: 360, Actor Loss: 0.020709648728370667
Epoch: 360, Critic Loss: 11.799123764038086
Epoch: 370, Actor Loss: 0.025101816281676292
Epoch: 370, Critic Loss: 7.5654616355896
Epoch: 380, Actor Loss: 0.025243153795599937
Epoch: 380, Critic Loss: 10.977991104125977
Epoch: 390, Actor Loss: 0.03333822637796402
Epoch: 390, Critic Loss: 11.04696273803711
2025-03-24 17:24:28,211 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:24:31,482 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:24:31,482 - evaluators.py:130 - Average Virtual Goal Value = 10.624524116516113
2025-03-24 17:24:31,482 - evaluators.py:132 - Goal Reach Probability = 0.2124904798172125
2025-03-24 17:24:31,482 - evaluators.py:134 - Trap Reach Probability = 0.7875095201827875
2025-03-24 17:24:31,482 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:24:31,482 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:24:31,482 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:24:31,482 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.024172035977244377
Epoch: 400, Critic Loss: 8.429770469665527
Epoch: 410, Actor Loss: 0.018463633954524994
Epoch: 410, Critic Loss: 10.000173568725586
Epoch: 420, Actor Loss: 0.021161114796996117
Epoch: 420, Critic Loss: 6.405269145965576
Epoch: 430, Actor Loss: 0.02858099900186062
Epoch: 430, Critic Loss: 6.711516380310059
Epoch: 440, Actor Loss: 0.03159002959728241
Epoch: 440, Critic Loss: 11.209383010864258
2025-03-24 17:25:04,634 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:25:07,906 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:25:07,906 - evaluators.py:130 - Average Virtual Goal Value = 10.162447929382324
2025-03-24 17:25:07,906 - evaluators.py:132 - Goal Reach Probability = 0.20324896108802418
2025-03-24 17:25:07,906 - evaluators.py:134 - Trap Reach Probability = 0.7967510389119759
2025-03-24 17:25:07,907 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:25:07,907 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:25:07,907 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:25:07,907 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.022761574015021324
Epoch: 450, Critic Loss: 6.048757553100586
Epoch: 460, Actor Loss: 0.0176989883184433
Epoch: 460, Critic Loss: 5.942265510559082
Epoch: 470, Actor Loss: 0.02326846681535244
Epoch: 470, Critic Loss: 6.913329124450684
Epoch: 480, Actor Loss: 0.01877848617732525
Epoch: 480, Critic Loss: 7.732754707336426
Epoch: 490, Actor Loss: 0.02310590259730816
Epoch: 490, Critic Loss: 4.501014709472656
2025-03-24 17:25:41,214 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:25:44,551 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:25:44,551 - evaluators.py:130 - Average Virtual Goal Value = 10.922329902648926
2025-03-24 17:25:44,551 - evaluators.py:132 - Goal Reach Probability = 0.21844660194174756
2025-03-24 17:25:44,551 - evaluators.py:134 - Trap Reach Probability = 0.7815533980582524
2025-03-24 17:25:44,551 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:25:44,551 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:25:44,551 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:25:44,551 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.03961305320262909
Epoch: 500, Critic Loss: 10.919429779052734
Epoch: 510, Actor Loss: 0.019639145582914352
Epoch: 510, Critic Loss: 10.126046180725098
Epoch: 520, Actor Loss: 0.02656545490026474
Epoch: 520, Critic Loss: 7.286772727966309
Epoch: 530, Actor Loss: 0.01675971783697605
Epoch: 530, Critic Loss: 12.348847389221191
Epoch: 540, Actor Loss: 0.03875620663166046
Epoch: 540, Critic Loss: 15.696420669555664
2025-03-24 17:26:17,239 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:26:20,540 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:26:20,540 - evaluators.py:130 - Average Virtual Goal Value = 10.302457809448242
2025-03-24 17:26:20,540 - evaluators.py:132 - Goal Reach Probability = 0.2060491493383743
2025-03-24 17:26:20,540 - evaluators.py:134 - Trap Reach Probability = 0.7939508506616257
2025-03-24 17:26:20,540 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:26:20,540 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:26:20,540 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:26:20,541 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.02539585530757904
Epoch: 550, Critic Loss: 11.900543212890625
Epoch: 560, Actor Loss: 0.02481747232377529
Epoch: 560, Critic Loss: 10.570096015930176
Epoch: 570, Actor Loss: 0.026128463447093964
Epoch: 570, Critic Loss: 13.411006927490234
Epoch: 580, Actor Loss: 0.023016180843114853
Epoch: 580, Critic Loss: 10.400941848754883
Epoch: 590, Actor Loss: 0.01507897861301899
Epoch: 590, Critic Loss: 6.868204116821289
2025-03-24 17:26:53,729 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:26:57,317 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:26:57,318 - evaluators.py:130 - Average Virtual Goal Value = 10.723709106445312
2025-03-24 17:26:57,318 - evaluators.py:132 - Goal Reach Probability = 0.21447418017338862
2025-03-24 17:26:57,318 - evaluators.py:134 - Trap Reach Probability = 0.7855258198266114
2025-03-24 17:26:57,318 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:26:57,318 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:26:57,318 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:26:57,318 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.02091214619576931
Epoch: 600, Critic Loss: 8.406661987304688
Epoch: 610, Actor Loss: 0.014420182444155216
Epoch: 610, Critic Loss: 15.410436630249023
Epoch: 620, Actor Loss: 0.02289392799139023
Epoch: 620, Critic Loss: 9.139078140258789
Epoch: 630, Actor Loss: 0.022575482726097107
Epoch: 630, Critic Loss: 7.137735366821289
Epoch: 640, Actor Loss: 0.02841034345328808
Epoch: 640, Critic Loss: 10.793243408203125
2025-03-24 17:27:30,632 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:27:34,923 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:27:34,923 - evaluators.py:130 - Average Virtual Goal Value = 8.493247985839844
2025-03-24 17:27:34,923 - evaluators.py:132 - Goal Reach Probability = 0.16986496090973702
2025-03-24 17:27:34,924 - evaluators.py:134 - Trap Reach Probability = 0.830135039090263
2025-03-24 17:27:34,924 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:27:34,924 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:27:34,924 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:27:34,924 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.013755150139331818
Epoch: 650, Critic Loss: 6.641102313995361
Epoch: 660, Actor Loss: 0.02547774463891983
Epoch: 660, Critic Loss: 13.3749418258667
Epoch: 670, Actor Loss: 0.024146806448698044
Epoch: 670, Critic Loss: 13.34449577331543
Epoch: 680, Actor Loss: 0.03369053453207016
Epoch: 680, Critic Loss: 11.972010612487793
Epoch: 690, Actor Loss: 0.02437336929142475
Epoch: 690, Critic Loss: 8.49454402923584
2025-03-24 17:28:07,721 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:28:11,064 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:28:11,064 - evaluators.py:130 - Average Virtual Goal Value = 9.733382225036621
2025-03-24 17:28:11,064 - evaluators.py:132 - Goal Reach Probability = 0.19466764061358657
2025-03-24 17:28:11,064 - evaluators.py:134 - Trap Reach Probability = 0.8053323593864135
2025-03-24 17:28:11,064 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:28:11,064 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:28:11,064 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:28:11,064 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.021381769329309464
Epoch: 700, Critic Loss: 16.449581146240234
2025-03-24 17:28:11,067 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 17:28:11,067 - father_agent.py:449 - Before training evaluation.
2025-03-24 17:28:11,067 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:28:18,369 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:28:18,369 - father_agent.py:564 - Average Virtual Goal Value = 10.401062965393066
2025-03-24 17:28:18,369 - father_agent.py:566 - Goal Reach Probability = 0.20802126117419667
2025-03-24 17:28:18,369 - father_agent.py:568 - Trap Reach Probability = 0.7919787388258034
2025-03-24 17:28:18,369 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:28:18,369 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:28:18,369 - father_agent.py:574 - Current Best Reach Probability = 0.21934604904632152
2025-03-24 17:28:18,479 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:28:18,595 - father_agent.py:359 - Training agent on-policy
2025-03-24 17:28:22,676 - father_agent.py:306 - Step: 0, Training loss: 103.96302032470703
2025-03-24 17:28:22,721 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:28:30,830 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:28:30,830 - father_agent.py:564 - Average Virtual Goal Value = 11.03396987915039
2025-03-24 17:28:30,830 - father_agent.py:566 - Goal Reach Probability = 0.2206793949913216
2025-03-24 17:28:30,830 - father_agent.py:568 - Trap Reach Probability = 0.7793206050086784
2025-03-24 17:28:30,830 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:28:30,830 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:28:30,830 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:28:30,888 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:28:32,876 - father_agent.py:306 - Step: 5, Training loss: 72.96856689453125
2025-03-24 17:28:34,024 - father_agent.py:306 - Step: 10, Training loss: 48.61986541748047
2025-03-24 17:28:35,252 - father_agent.py:306 - Step: 15, Training loss: 29.90345573425293
2025-03-24 17:28:36,575 - father_agent.py:306 - Step: 20, Training loss: 14.580276489257812
2025-03-24 17:28:37,749 - father_agent.py:306 - Step: 25, Training loss: 8.165160179138184
2025-03-24 17:28:38,893 - father_agent.py:306 - Step: 30, Training loss: 4.813233852386475
2025-03-24 17:28:40,211 - father_agent.py:306 - Step: 35, Training loss: 2.181156635284424
2025-03-24 17:28:41,412 - father_agent.py:306 - Step: 40, Training loss: 1.5787482261657715
2025-03-24 17:28:42,703 - father_agent.py:306 - Step: 45, Training loss: 1.4942394495010376
2025-03-24 17:28:47,155 - father_agent.py:306 - Step: 50, Training loss: 0.5507563948631287
2025-03-24 17:28:48,393 - father_agent.py:306 - Step: 55, Training loss: 0.9806079268455505
2025-03-24 17:28:49,580 - father_agent.py:306 - Step: 60, Training loss: 0.9998882412910461
2025-03-24 17:28:51,030 - father_agent.py:306 - Step: 65, Training loss: 1.3127129077911377
2025-03-24 17:28:52,410 - father_agent.py:306 - Step: 70, Training loss: 0.6291050910949707
2025-03-24 17:28:53,703 - father_agent.py:306 - Step: 75, Training loss: 0.6462963223457336
2025-03-24 17:28:55,052 - father_agent.py:306 - Step: 80, Training loss: 0.7056777477264404
2025-03-24 17:28:56,256 - father_agent.py:306 - Step: 85, Training loss: 1.002524495124817
2025-03-24 17:28:57,508 - father_agent.py:306 - Step: 90, Training loss: 0.7057144045829773
2025-03-24 17:28:58,885 - father_agent.py:306 - Step: 95, Training loss: 0.5712088942527771
2025-03-24 17:29:00,277 - father_agent.py:306 - Step: 100, Training loss: 0.6549227833747864
2025-03-24 17:29:00,517 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:29:10,024 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:29:10,024 - father_agent.py:564 - Average Virtual Goal Value = 3.55009388923645
2025-03-24 17:29:10,024 - father_agent.py:566 - Goal Reach Probability = 0.07100187745349036
2025-03-24 17:29:10,024 - father_agent.py:568 - Trap Reach Probability = 0.9289981225465096
2025-03-24 17:29:10,024 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:29:10,024 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:29:10,024 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:29:10,071 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:29:12,091 - father_agent.py:306 - Step: 105, Training loss: 0.8003532290458679
2025-03-24 17:29:13,352 - father_agent.py:306 - Step: 110, Training loss: 0.9541261196136475
2025-03-24 17:29:14,562 - father_agent.py:306 - Step: 115, Training loss: 0.9644666910171509
2025-03-24 17:29:15,782 - father_agent.py:306 - Step: 120, Training loss: 0.9579616785049438
2025-03-24 17:29:17,133 - father_agent.py:306 - Step: 125, Training loss: 0.6473076343536377
2025-03-24 17:29:18,336 - father_agent.py:306 - Step: 130, Training loss: 0.710486114025116
2025-03-24 17:29:19,607 - father_agent.py:306 - Step: 135, Training loss: 0.8705303072929382
2025-03-24 17:29:20,951 - father_agent.py:306 - Step: 140, Training loss: 0.8216093182563782
2025-03-24 17:29:22,197 - father_agent.py:306 - Step: 145, Training loss: 0.6665388345718384
2025-03-24 17:29:23,502 - father_agent.py:306 - Step: 150, Training loss: 0.8130367398262024
2025-03-24 17:29:24,838 - father_agent.py:306 - Step: 155, Training loss: 0.7167247533798218
2025-03-24 17:29:26,139 - father_agent.py:306 - Step: 160, Training loss: 0.416800856590271
2025-03-24 17:29:27,418 - father_agent.py:306 - Step: 165, Training loss: 0.6266568899154663
2025-03-24 17:29:28,757 - father_agent.py:306 - Step: 170, Training loss: 0.6120448112487793
2025-03-24 17:29:30,066 - father_agent.py:306 - Step: 175, Training loss: 0.5559616088867188
2025-03-24 17:29:31,442 - father_agent.py:306 - Step: 180, Training loss: 0.8225231766700745
2025-03-24 17:29:32,673 - father_agent.py:306 - Step: 185, Training loss: 0.8307345509529114
2025-03-24 17:29:33,848 - father_agent.py:306 - Step: 190, Training loss: 1.4184614419937134
2025-03-24 17:29:35,251 - father_agent.py:306 - Step: 195, Training loss: 0.9483170509338379
2025-03-24 17:29:36,616 - father_agent.py:306 - Step: 200, Training loss: 1.1701083183288574
2025-03-24 17:29:36,660 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:29:45,801 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:29:45,801 - father_agent.py:564 - Average Virtual Goal Value = 4.682967185974121
2025-03-24 17:29:45,801 - father_agent.py:566 - Goal Reach Probability = 0.09365934521342727
2025-03-24 17:29:45,801 - father_agent.py:568 - Trap Reach Probability = 0.9063406547865728
2025-03-24 17:29:45,801 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:29:45,801 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:29:45,801 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:29:45,844 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:29:48,173 - father_agent.py:306 - Step: 205, Training loss: 0.805199146270752
2025-03-24 17:29:49,452 - father_agent.py:306 - Step: 210, Training loss: 0.6797762513160706
2025-03-24 17:29:50,657 - father_agent.py:306 - Step: 215, Training loss: 1.3778595924377441
2025-03-24 17:29:51,953 - father_agent.py:306 - Step: 220, Training loss: 1.0588511228561401
2025-03-24 17:29:53,313 - father_agent.py:306 - Step: 225, Training loss: 0.9534987807273865
2025-03-24 17:29:54,541 - father_agent.py:306 - Step: 230, Training loss: 1.2592698335647583
2025-03-24 17:29:55,837 - father_agent.py:306 - Step: 235, Training loss: 0.8802940249443054
2025-03-24 17:29:57,177 - father_agent.py:306 - Step: 240, Training loss: 1.2909563779830933
2025-03-24 17:29:58,382 - father_agent.py:306 - Step: 245, Training loss: 1.159942865371704
2025-03-24 17:29:59,661 - father_agent.py:306 - Step: 250, Training loss: 1.1674035787582397
2025-03-24 17:30:00,982 - father_agent.py:306 - Step: 255, Training loss: 1.1855980157852173
2025-03-24 17:30:02,178 - father_agent.py:306 - Step: 260, Training loss: 0.9270023107528687
2025-03-24 17:30:03,428 - father_agent.py:306 - Step: 265, Training loss: 1.0122485160827637
2025-03-24 17:30:04,771 - father_agent.py:306 - Step: 270, Training loss: 0.9817243814468384
2025-03-24 17:30:06,028 - father_agent.py:306 - Step: 275, Training loss: 1.3956650495529175
2025-03-24 17:30:07,367 - father_agent.py:306 - Step: 280, Training loss: 1.2486686706542969
2025-03-24 17:30:08,564 - father_agent.py:306 - Step: 285, Training loss: 1.132120132446289
2025-03-24 17:30:09,750 - father_agent.py:306 - Step: 290, Training loss: 1.212214708328247
2025-03-24 17:30:11,127 - father_agent.py:306 - Step: 295, Training loss: 1.4068337678909302
2025-03-24 17:30:12,399 - father_agent.py:306 - Step: 300, Training loss: 1.3894071578979492
2025-03-24 17:30:12,518 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:30:21,078 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:30:21,078 - father_agent.py:564 - Average Virtual Goal Value = 5.2405500411987305
2025-03-24 17:30:21,078 - father_agent.py:566 - Goal Reach Probability = 0.10481099656357389
2025-03-24 17:30:21,078 - father_agent.py:568 - Trap Reach Probability = 0.8951890034364262
2025-03-24 17:30:21,078 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:30:21,078 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:30:21,078 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:30:21,121 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:30:23,104 - father_agent.py:306 - Step: 305, Training loss: 1.4439581632614136
2025-03-24 17:30:24,432 - father_agent.py:306 - Step: 310, Training loss: 1.2621359825134277
2025-03-24 17:30:25,623 - father_agent.py:306 - Step: 315, Training loss: 1.2401130199432373
2025-03-24 17:30:26,858 - father_agent.py:306 - Step: 320, Training loss: 1.176742434501648
2025-03-24 17:30:28,125 - father_agent.py:306 - Step: 325, Training loss: 1.1328984498977661
2025-03-24 17:30:29,414 - father_agent.py:306 - Step: 330, Training loss: 0.8758570551872253
2025-03-24 17:30:30,675 - father_agent.py:306 - Step: 335, Training loss: 1.449341893196106
2025-03-24 17:30:31,894 - father_agent.py:306 - Step: 340, Training loss: 1.3717021942138672
2025-03-24 17:30:33,216 - father_agent.py:306 - Step: 345, Training loss: 1.2534857988357544
2025-03-24 17:30:34,474 - father_agent.py:306 - Step: 350, Training loss: 1.2738786935806274
2025-03-24 17:30:35,629 - father_agent.py:306 - Step: 355, Training loss: 1.2827705144882202
2025-03-24 17:30:36,927 - father_agent.py:306 - Step: 360, Training loss: 1.0033963918685913
2025-03-24 17:30:38,229 - father_agent.py:306 - Step: 365, Training loss: 1.4988172054290771
2025-03-24 17:30:39,449 - father_agent.py:306 - Step: 370, Training loss: 1.5510668754577637
2025-03-24 17:30:40,745 - father_agent.py:306 - Step: 375, Training loss: 1.1910877227783203
2025-03-24 17:30:42,008 - father_agent.py:306 - Step: 380, Training loss: 1.6622982025146484
2025-03-24 17:30:43,187 - father_agent.py:306 - Step: 385, Training loss: 1.454155683517456
2025-03-24 17:30:44,482 - father_agent.py:306 - Step: 390, Training loss: 1.449852705001831
2025-03-24 17:30:45,721 - father_agent.py:306 - Step: 395, Training loss: 1.2482566833496094
2025-03-24 17:30:46,898 - father_agent.py:306 - Step: 400, Training loss: 1.455395221710205
2025-03-24 17:30:46,943 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:30:55,389 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:30:55,389 - father_agent.py:564 - Average Virtual Goal Value = 6.139171123504639
2025-03-24 17:30:55,389 - father_agent.py:566 - Goal Reach Probability = 0.12278342717875451
2025-03-24 17:30:55,389 - father_agent.py:568 - Trap Reach Probability = 0.8772165728212454
2025-03-24 17:30:55,389 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:30:55,389 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:30:55,389 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:30:55,432 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:30:57,373 - father_agent.py:306 - Step: 405, Training loss: 1.162245512008667
2025-03-24 17:30:58,650 - father_agent.py:306 - Step: 410, Training loss: 1.1659578084945679
2025-03-24 17:30:59,787 - father_agent.py:306 - Step: 415, Training loss: 1.241815209388733
2025-03-24 17:31:01,087 - father_agent.py:306 - Step: 420, Training loss: 1.4222841262817383
2025-03-24 17:31:02,241 - father_agent.py:306 - Step: 425, Training loss: 1.1871113777160645
2025-03-24 17:31:03,394 - father_agent.py:306 - Step: 430, Training loss: 1.5471667051315308
2025-03-24 17:31:04,735 - father_agent.py:306 - Step: 435, Training loss: 1.5326279401779175
2025-03-24 17:31:06,358 - father_agent.py:306 - Step: 440, Training loss: 1.3013185262680054
2025-03-24 17:31:08,124 - father_agent.py:306 - Step: 445, Training loss: 1.397437334060669
2025-03-24 17:31:09,381 - father_agent.py:306 - Step: 450, Training loss: 1.363827109336853
2025-03-24 17:31:10,680 - father_agent.py:306 - Step: 455, Training loss: 1.6530486345291138
2025-03-24 17:31:11,871 - father_agent.py:306 - Step: 460, Training loss: 1.0666568279266357
2025-03-24 17:31:13,186 - father_agent.py:306 - Step: 465, Training loss: 1.4932355880737305
2025-03-24 17:31:14,511 - father_agent.py:306 - Step: 470, Training loss: 1.1424498558044434
2025-03-24 17:31:15,707 - father_agent.py:306 - Step: 475, Training loss: 1.5288419723510742
2025-03-24 17:31:16,941 - father_agent.py:306 - Step: 480, Training loss: 1.6056175231933594
2025-03-24 17:31:18,196 - father_agent.py:306 - Step: 485, Training loss: 1.4405022859573364
2025-03-24 17:31:19,439 - father_agent.py:306 - Step: 490, Training loss: 1.2661502361297607
2025-03-24 17:31:20,667 - father_agent.py:306 - Step: 495, Training loss: 1.2823957204818726
2025-03-24 17:31:21,852 - father_agent.py:306 - Step: 500, Training loss: 1.3133208751678467
2025-03-24 17:31:21,897 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:31:30,766 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:31:30,767 - father_agent.py:564 - Average Virtual Goal Value = 5.616347312927246
2025-03-24 17:31:30,767 - father_agent.py:566 - Goal Reach Probability = 0.11232695017823095
2025-03-24 17:31:30,767 - father_agent.py:568 - Trap Reach Probability = 0.8876730498217691
2025-03-24 17:31:30,767 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:31:30,767 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:31:30,767 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:31:30,809 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:31:33,245 - father_agent.py:306 - Step: 505, Training loss: 1.4113836288452148
2025-03-24 17:31:34,542 - father_agent.py:306 - Step: 510, Training loss: 1.1853948831558228
2025-03-24 17:31:35,755 - father_agent.py:306 - Step: 515, Training loss: 1.4509130716323853
2025-03-24 17:31:37,108 - father_agent.py:306 - Step: 520, Training loss: 1.2928229570388794
2025-03-24 17:31:38,274 - father_agent.py:306 - Step: 525, Training loss: 1.4739471673965454
2025-03-24 17:31:39,453 - father_agent.py:306 - Step: 530, Training loss: 1.4386889934539795
2025-03-24 17:31:41,030 - father_agent.py:306 - Step: 535, Training loss: 1.164677619934082
2025-03-24 17:31:42,831 - father_agent.py:306 - Step: 540, Training loss: 1.4952646493911743
2025-03-24 17:31:44,225 - father_agent.py:306 - Step: 545, Training loss: 1.6293632984161377
2025-03-24 17:31:45,395 - father_agent.py:306 - Step: 550, Training loss: 1.2744312286376953
2025-03-24 17:31:46,711 - father_agent.py:306 - Step: 555, Training loss: 1.2629237174987793
2025-03-24 17:31:48,004 - father_agent.py:306 - Step: 560, Training loss: 1.2247421741485596
2025-03-24 17:31:49,311 - father_agent.py:306 - Step: 565, Training loss: 1.0966713428497314
2025-03-24 17:31:50,608 - father_agent.py:306 - Step: 570, Training loss: 1.0863878726959229
2025-03-24 17:31:51,892 - father_agent.py:306 - Step: 575, Training loss: 1.132820963859558
2025-03-24 17:31:53,081 - father_agent.py:306 - Step: 580, Training loss: 1.2879915237426758
2025-03-24 17:31:54,364 - father_agent.py:306 - Step: 585, Training loss: 1.3569854497909546
2025-03-24 17:31:55,645 - father_agent.py:306 - Step: 590, Training loss: 1.4065381288528442
2025-03-24 17:31:56,993 - father_agent.py:306 - Step: 595, Training loss: 1.1235581636428833
2025-03-24 17:31:58,195 - father_agent.py:306 - Step: 600, Training loss: 1.3200623989105225
2025-03-24 17:31:58,241 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:32:06,820 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:32:06,820 - father_agent.py:564 - Average Virtual Goal Value = 5.1070709228515625
2025-03-24 17:32:06,820 - father_agent.py:566 - Goal Reach Probability = 0.10214141414141414
2025-03-24 17:32:06,820 - father_agent.py:568 - Trap Reach Probability = 0.8978585858585859
2025-03-24 17:32:06,821 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:32:06,821 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:32:06,821 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:32:06,867 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:32:08,822 - father_agent.py:306 - Step: 605, Training loss: 1.0689697265625
2025-03-24 17:32:10,061 - father_agent.py:306 - Step: 610, Training loss: 1.3255558013916016
2025-03-24 17:32:11,343 - father_agent.py:306 - Step: 615, Training loss: 1.4099348783493042
2025-03-24 17:32:12,681 - father_agent.py:306 - Step: 620, Training loss: 1.149691104888916
2025-03-24 17:32:13,871 - father_agent.py:306 - Step: 625, Training loss: 1.026476502418518
2025-03-24 17:32:15,081 - father_agent.py:306 - Step: 630, Training loss: 1.504469871520996
2025-03-24 17:32:16,441 - father_agent.py:306 - Step: 635, Training loss: 1.227923035621643
2025-03-24 17:32:17,595 - father_agent.py:306 - Step: 640, Training loss: 1.1824774742126465
2025-03-24 17:32:18,880 - father_agent.py:306 - Step: 645, Training loss: 1.212445616722107
2025-03-24 17:32:20,180 - father_agent.py:306 - Step: 650, Training loss: 1.744820237159729
2025-03-24 17:32:21,402 - father_agent.py:306 - Step: 655, Training loss: 1.4842464923858643
2025-03-24 17:32:22,583 - father_agent.py:306 - Step: 660, Training loss: 1.6224961280822754
2025-03-24 17:32:23,895 - father_agent.py:306 - Step: 665, Training loss: 1.5483969449996948
2025-03-24 17:32:25,213 - father_agent.py:306 - Step: 670, Training loss: 1.198864459991455
2025-03-24 17:32:26,506 - father_agent.py:306 - Step: 675, Training loss: 1.3368182182312012
2025-03-24 17:32:27,742 - father_agent.py:306 - Step: 680, Training loss: 1.142118215560913
2025-03-24 17:32:29,048 - father_agent.py:306 - Step: 685, Training loss: 1.4713761806488037
2025-03-24 17:32:30,224 - father_agent.py:306 - Step: 690, Training loss: 1.3860220909118652
2025-03-24 17:32:31,433 - father_agent.py:306 - Step: 695, Training loss: 1.1137655973434448
2025-03-24 17:32:32,775 - father_agent.py:306 - Step: 700, Training loss: 1.2471452951431274
2025-03-24 17:32:32,821 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:32:41,256 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:32:41,256 - father_agent.py:564 - Average Virtual Goal Value = 5.2749128341674805
2025-03-24 17:32:41,256 - father_agent.py:566 - Goal Reach Probability = 0.1054982538780151
2025-03-24 17:32:41,256 - father_agent.py:568 - Trap Reach Probability = 0.8945017461219849
2025-03-24 17:32:41,256 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:32:41,256 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:32:41,256 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:32:41,299 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:32:41,421 - father_agent.py:455 - Training finished.
2025-03-24 17:32:41,427 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:32:41,535 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 17:32:50,437 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:32:50,437 - father_agent.py:564 - Average Virtual Goal Value = 5.112118721008301
2025-03-24 17:32:50,437 - father_agent.py:566 - Goal Reach Probability = 0.10224237027442726
2025-03-24 17:32:50,437 - father_agent.py:568 - Trap Reach Probability = 0.8977576297255727
2025-03-24 17:32:50,437 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:32:50,437 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:32:50,437 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:32:50,542 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:33:06,163 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:33:06,164 - evaluators.py:130 - Average Virtual Goal Value = 6.980360507965088
2025-03-24 17:33:06,164 - evaluators.py:132 - Goal Reach Probability = 0.13960720828185574
2025-03-24 17:33:06,164 - evaluators.py:134 - Trap Reach Probability = 0.8603927917181442
2025-03-24 17:33:06,164 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:33:06,164 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:33:06,164 - evaluators.py:140 - Current Best Reach Probability = 0.13960720828185574
2025-03-24 17:33:06,179 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:33:43,603 - cloned_fsc_actor_policy.py:189 - Epoch 0, Loss: 1.7752
2025-03-24 17:33:43,603 - cloned_fsc_actor_policy.py:190 - Epoch 0, Accuracy: 0.4344
2025-03-24 17:33:43,647 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:34:01,707 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:34:01,707 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:34:01,707 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:34:01,707 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:34:01,707 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:34:01,707 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:34:01,707 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:34:05,134 - cloned_fsc_actor_policy.py:189 - Epoch 1000, Loss: 0.9657
2025-03-24 17:34:05,134 - cloned_fsc_actor_policy.py:190 - Epoch 1000, Accuracy: 0.5969
2025-03-24 17:34:08,529 - cloned_fsc_actor_policy.py:189 - Epoch 2000, Loss: 0.4556
2025-03-24 17:34:08,529 - cloned_fsc_actor_policy.py:190 - Epoch 2000, Accuracy: 0.7630
2025-03-24 17:34:11,911 - cloned_fsc_actor_policy.py:189 - Epoch 3000, Loss: 0.3455
2025-03-24 17:34:11,911 - cloned_fsc_actor_policy.py:190 - Epoch 3000, Accuracy: 0.7600
2025-03-24 17:34:15,300 - cloned_fsc_actor_policy.py:189 - Epoch 4000, Loss: 0.2879
2025-03-24 17:34:15,301 - cloned_fsc_actor_policy.py:190 - Epoch 4000, Accuracy: 0.8422
2025-03-24 17:34:18,601 - cloned_fsc_actor_policy.py:189 - Epoch 5000, Loss: 0.2078
2025-03-24 17:34:18,601 - cloned_fsc_actor_policy.py:190 - Epoch 5000, Accuracy: 0.9211
2025-03-24 17:34:18,646 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:34:35,602 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:34:35,602 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:34:35,602 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:34:35,602 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:34:35,602 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:34:35,602 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:34:35,602 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:34:38,977 - cloned_fsc_actor_policy.py:189 - Epoch 6000, Loss: 0.1603
2025-03-24 17:34:38,978 - cloned_fsc_actor_policy.py:190 - Epoch 6000, Accuracy: 0.9447
2025-03-24 17:34:42,326 - cloned_fsc_actor_policy.py:189 - Epoch 7000, Loss: 0.1402
2025-03-24 17:34:42,326 - cloned_fsc_actor_policy.py:190 - Epoch 7000, Accuracy: 0.9468
2025-03-24 17:34:45,702 - cloned_fsc_actor_policy.py:189 - Epoch 8000, Loss: 0.1294
2025-03-24 17:34:45,702 - cloned_fsc_actor_policy.py:190 - Epoch 8000, Accuracy: 0.9476
2025-03-24 17:34:49,081 - cloned_fsc_actor_policy.py:189 - Epoch 9000, Loss: 0.1198
2025-03-24 17:34:49,081 - cloned_fsc_actor_policy.py:190 - Epoch 9000, Accuracy: 0.9477
2025-03-24 17:34:52,442 - cloned_fsc_actor_policy.py:189 - Epoch 10000, Loss: 0.1100
2025-03-24 17:34:52,443 - cloned_fsc_actor_policy.py:190 - Epoch 10000, Accuracy: 0.9486
2025-03-24 17:34:52,484 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:35:09,549 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:35:09,550 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:35:09,550 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:35:09,550 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:35:09,550 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:35:09,550 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:35:09,550 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:35:12,899 - cloned_fsc_actor_policy.py:189 - Epoch 11000, Loss: 0.1001
2025-03-24 17:35:12,899 - cloned_fsc_actor_policy.py:190 - Epoch 11000, Accuracy: 0.9488
2025-03-24 17:35:16,886 - cloned_fsc_actor_policy.py:189 - Epoch 12000, Loss: 0.0878
2025-03-24 17:35:16,886 - cloned_fsc_actor_policy.py:190 - Epoch 12000, Accuracy: 0.9678
2025-03-24 17:35:20,216 - cloned_fsc_actor_policy.py:189 - Epoch 13000, Loss: 0.0741
2025-03-24 17:35:20,216 - cloned_fsc_actor_policy.py:190 - Epoch 13000, Accuracy: 0.9812
2025-03-24 17:35:23,558 - cloned_fsc_actor_policy.py:189 - Epoch 14000, Loss: 0.0592
2025-03-24 17:35:23,558 - cloned_fsc_actor_policy.py:190 - Epoch 14000, Accuracy: 0.9848
2025-03-24 17:35:27,048 - cloned_fsc_actor_policy.py:189 - Epoch 15000, Loss: 0.0477
2025-03-24 17:35:27,049 - cloned_fsc_actor_policy.py:190 - Epoch 15000, Accuracy: 0.9872
2025-03-24 17:35:27,086 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:35:44,591 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:35:44,591 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:35:44,591 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:35:44,591 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:35:44,591 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:35:44,591 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:35:44,591 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:35:48,031 - cloned_fsc_actor_policy.py:189 - Epoch 16000, Loss: 0.0415
2025-03-24 17:35:48,031 - cloned_fsc_actor_policy.py:190 - Epoch 16000, Accuracy: 0.9876
2025-03-24 17:35:52,020 - cloned_fsc_actor_policy.py:189 - Epoch 17000, Loss: 0.0379
2025-03-24 17:35:52,021 - cloned_fsc_actor_policy.py:190 - Epoch 17000, Accuracy: 0.9878
2025-03-24 17:35:55,378 - cloned_fsc_actor_policy.py:189 - Epoch 18000, Loss: 0.0357
2025-03-24 17:35:55,378 - cloned_fsc_actor_policy.py:190 - Epoch 18000, Accuracy: 0.9879
2025-03-24 17:35:58,805 - cloned_fsc_actor_policy.py:189 - Epoch 19000, Loss: 0.0342
2025-03-24 17:35:58,805 - cloned_fsc_actor_policy.py:190 - Epoch 19000, Accuracy: 0.9880
2025-03-24 17:36:02,287 - cloned_fsc_actor_policy.py:189 - Epoch 20000, Loss: 0.0344
2025-03-24 17:36:02,287 - cloned_fsc_actor_policy.py:190 - Epoch 20000, Accuracy: 0.9876
2025-03-24 17:36:02,512 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:36:19,289 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:36:19,289 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:36:19,289 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:36:19,289 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:36:19,289 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:36:19,290 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:36:19,290 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:36:22,642 - cloned_fsc_actor_policy.py:189 - Epoch 21000, Loss: 0.0330
2025-03-24 17:36:22,642 - cloned_fsc_actor_policy.py:190 - Epoch 21000, Accuracy: 0.9878
2025-03-24 17:36:25,937 - cloned_fsc_actor_policy.py:189 - Epoch 22000, Loss: 0.0321
2025-03-24 17:36:25,937 - cloned_fsc_actor_policy.py:190 - Epoch 22000, Accuracy: 0.9881
2025-03-24 17:36:29,363 - cloned_fsc_actor_policy.py:189 - Epoch 23000, Loss: 0.0319
2025-03-24 17:36:29,363 - cloned_fsc_actor_policy.py:190 - Epoch 23000, Accuracy: 0.9880
2025-03-24 17:36:32,673 - cloned_fsc_actor_policy.py:189 - Epoch 24000, Loss: 0.0314
2025-03-24 17:36:32,673 - cloned_fsc_actor_policy.py:190 - Epoch 24000, Accuracy: 0.9880
2025-03-24 17:36:35,894 - cloned_fsc_actor_policy.py:189 - Epoch 25000, Loss: 0.0308
2025-03-24 17:36:35,895 - cloned_fsc_actor_policy.py:190 - Epoch 25000, Accuracy: 0.9881
2025-03-24 17:36:36,476 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:36:53,233 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:36:53,233 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:36:53,233 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:36:53,233 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:36:53,233 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:36:53,233 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:36:53,233 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:36:56,548 - cloned_fsc_actor_policy.py:189 - Epoch 26000, Loss: 0.0308
2025-03-24 17:36:56,548 - cloned_fsc_actor_policy.py:190 - Epoch 26000, Accuracy: 0.9880
2025-03-24 17:36:59,899 - cloned_fsc_actor_policy.py:189 - Epoch 27000, Loss: 0.0309
2025-03-24 17:36:59,899 - cloned_fsc_actor_policy.py:190 - Epoch 27000, Accuracy: 0.9880
2025-03-24 17:37:03,251 - cloned_fsc_actor_policy.py:189 - Epoch 28000, Loss: 0.0308
2025-03-24 17:37:03,251 - cloned_fsc_actor_policy.py:190 - Epoch 28000, Accuracy: 0.9881
2025-03-24 17:37:06,582 - cloned_fsc_actor_policy.py:189 - Epoch 29000, Loss: 0.0297
2025-03-24 17:37:06,582 - cloned_fsc_actor_policy.py:190 - Epoch 29000, Accuracy: 0.9883
2025-03-24 17:37:09,900 - cloned_fsc_actor_policy.py:189 - Epoch 30000, Loss: 0.0294
2025-03-24 17:37:09,900 - cloned_fsc_actor_policy.py:190 - Epoch 30000, Accuracy: 0.9884
2025-03-24 17:37:09,944 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:37:27,766 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:37:27,766 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:37:27,766 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:37:27,766 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:37:27,766 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:37:27,766 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:37:27,766 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:37:31,137 - cloned_fsc_actor_policy.py:189 - Epoch 31000, Loss: 0.0293
2025-03-24 17:37:31,137 - cloned_fsc_actor_policy.py:190 - Epoch 31000, Accuracy: 0.9883
2025-03-24 17:37:34,582 - cloned_fsc_actor_policy.py:189 - Epoch 32000, Loss: 0.0293
2025-03-24 17:37:34,582 - cloned_fsc_actor_policy.py:190 - Epoch 32000, Accuracy: 0.9884
2025-03-24 17:37:37,856 - cloned_fsc_actor_policy.py:189 - Epoch 33000, Loss: 0.0298
2025-03-24 17:37:37,856 - cloned_fsc_actor_policy.py:190 - Epoch 33000, Accuracy: 0.9880
2025-03-24 17:37:41,220 - cloned_fsc_actor_policy.py:189 - Epoch 34000, Loss: 0.0292
2025-03-24 17:37:41,220 - cloned_fsc_actor_policy.py:190 - Epoch 34000, Accuracy: 0.9882
2025-03-24 17:37:44,543 - cloned_fsc_actor_policy.py:189 - Epoch 35000, Loss: 0.0283
2025-03-24 17:37:44,543 - cloned_fsc_actor_policy.py:190 - Epoch 35000, Accuracy: 0.9886
2025-03-24 17:37:44,581 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:38:02,024 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:38:02,024 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:38:02,024 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:38:02,024 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:38:02,024 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:38:02,024 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:38:02,024 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:38:05,343 - cloned_fsc_actor_policy.py:189 - Epoch 36000, Loss: 0.0293
2025-03-24 17:38:05,343 - cloned_fsc_actor_policy.py:190 - Epoch 36000, Accuracy: 0.9881
2025-03-24 17:38:08,724 - cloned_fsc_actor_policy.py:189 - Epoch 37000, Loss: 0.0289
2025-03-24 17:38:08,724 - cloned_fsc_actor_policy.py:190 - Epoch 37000, Accuracy: 0.9881
2025-03-24 17:38:12,130 - cloned_fsc_actor_policy.py:189 - Epoch 38000, Loss: 0.0283
2025-03-24 17:38:12,130 - cloned_fsc_actor_policy.py:190 - Epoch 38000, Accuracy: 0.9883
2025-03-24 17:38:15,532 - cloned_fsc_actor_policy.py:189 - Epoch 39000, Loss: 0.0283
2025-03-24 17:38:15,532 - cloned_fsc_actor_policy.py:190 - Epoch 39000, Accuracy: 0.9884
2025-03-24 17:38:18,870 - cloned_fsc_actor_policy.py:189 - Epoch 40000, Loss: 0.0283
2025-03-24 17:38:18,870 - cloned_fsc_actor_policy.py:190 - Epoch 40000, Accuracy: 0.9884
2025-03-24 17:38:18,906 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:38:35,461 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:38:35,462 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:38:35,462 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:38:35,462 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:38:35,462 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:38:35,462 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:38:35,462 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:38:38,791 - cloned_fsc_actor_policy.py:189 - Epoch 41000, Loss: 0.0281
2025-03-24 17:38:38,791 - cloned_fsc_actor_policy.py:190 - Epoch 41000, Accuracy: 0.9884
2025-03-24 17:38:42,179 - cloned_fsc_actor_policy.py:189 - Epoch 42000, Loss: 0.0276
2025-03-24 17:38:42,179 - cloned_fsc_actor_policy.py:190 - Epoch 42000, Accuracy: 0.9887
2025-03-24 17:38:45,473 - cloned_fsc_actor_policy.py:189 - Epoch 43000, Loss: 0.0285
2025-03-24 17:38:45,473 - cloned_fsc_actor_policy.py:190 - Epoch 43000, Accuracy: 0.9881
2025-03-24 17:38:48,832 - cloned_fsc_actor_policy.py:189 - Epoch 44000, Loss: 0.0277
2025-03-24 17:38:48,832 - cloned_fsc_actor_policy.py:190 - Epoch 44000, Accuracy: 0.9885
2025-03-24 17:38:52,192 - cloned_fsc_actor_policy.py:189 - Epoch 45000, Loss: 0.0281
2025-03-24 17:38:52,192 - cloned_fsc_actor_policy.py:190 - Epoch 45000, Accuracy: 0.9879
2025-03-24 17:38:52,519 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:39:09,497 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:39:09,497 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:39:09,497 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:39:09,497 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:39:09,497 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:39:09,497 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:39:09,497 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:39:12,973 - cloned_fsc_actor_policy.py:189 - Epoch 46000, Loss: 0.0276
2025-03-24 17:39:12,973 - cloned_fsc_actor_policy.py:190 - Epoch 46000, Accuracy: 0.9884
2025-03-24 17:39:16,282 - cloned_fsc_actor_policy.py:189 - Epoch 47000, Loss: 0.0279
2025-03-24 17:39:16,283 - cloned_fsc_actor_policy.py:190 - Epoch 47000, Accuracy: 0.9880
2025-03-24 17:39:19,562 - cloned_fsc_actor_policy.py:189 - Epoch 48000, Loss: 0.0275
2025-03-24 17:39:19,562 - cloned_fsc_actor_policy.py:190 - Epoch 48000, Accuracy: 0.9882
2025-03-24 17:39:23,245 - cloned_fsc_actor_policy.py:189 - Epoch 49000, Loss: 0.0275
2025-03-24 17:39:23,245 - cloned_fsc_actor_policy.py:190 - Epoch 49000, Accuracy: 0.9882
2025-03-24 17:39:26,941 - cloned_fsc_actor_policy.py:189 - Epoch 50000, Loss: 0.0270
2025-03-24 17:39:26,941 - cloned_fsc_actor_policy.py:190 - Epoch 50000, Accuracy: 0.9884
2025-03-24 17:39:26,973 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:39:43,570 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:39:43,570 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:39:43,570 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:39:43,570 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:39:43,570 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:39:43,570 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:39:43,570 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:39:46,896 - cloned_fsc_actor_policy.py:189 - Epoch 51000, Loss: 0.0263
2025-03-24 17:39:46,896 - cloned_fsc_actor_policy.py:190 - Epoch 51000, Accuracy: 0.9888
2025-03-24 17:39:50,231 - cloned_fsc_actor_policy.py:189 - Epoch 52000, Loss: 0.0268
2025-03-24 17:39:50,232 - cloned_fsc_actor_policy.py:190 - Epoch 52000, Accuracy: 0.9883
2025-03-24 17:39:53,527 - cloned_fsc_actor_policy.py:189 - Epoch 53000, Loss: 0.0262
2025-03-24 17:39:53,527 - cloned_fsc_actor_policy.py:190 - Epoch 53000, Accuracy: 0.9886
2025-03-24 17:39:56,979 - cloned_fsc_actor_policy.py:189 - Epoch 54000, Loss: 0.0270
2025-03-24 17:39:56,979 - cloned_fsc_actor_policy.py:190 - Epoch 54000, Accuracy: 0.9883
2025-03-24 17:40:00,863 - cloned_fsc_actor_policy.py:189 - Epoch 55000, Loss: 0.0254
2025-03-24 17:40:00,863 - cloned_fsc_actor_policy.py:190 - Epoch 55000, Accuracy: 0.9889
2025-03-24 17:40:00,895 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:40:18,261 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:40:18,261 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:40:18,261 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:40:18,261 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:40:18,261 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:40:18,261 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:40:18,261 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:40:21,674 - cloned_fsc_actor_policy.py:189 - Epoch 56000, Loss: 0.0261
2025-03-24 17:40:21,674 - cloned_fsc_actor_policy.py:190 - Epoch 56000, Accuracy: 0.9888
2025-03-24 17:40:25,079 - cloned_fsc_actor_policy.py:189 - Epoch 57000, Loss: 0.0264
2025-03-24 17:40:25,079 - cloned_fsc_actor_policy.py:190 - Epoch 57000, Accuracy: 0.9884
2025-03-24 17:40:28,565 - cloned_fsc_actor_policy.py:189 - Epoch 58000, Loss: 0.0254
2025-03-24 17:40:28,565 - cloned_fsc_actor_policy.py:190 - Epoch 58000, Accuracy: 0.9889
2025-03-24 17:40:31,855 - cloned_fsc_actor_policy.py:189 - Epoch 59000, Loss: 0.0259
2025-03-24 17:40:31,855 - cloned_fsc_actor_policy.py:190 - Epoch 59000, Accuracy: 0.9884
2025-03-24 17:40:35,233 - cloned_fsc_actor_policy.py:189 - Epoch 60000, Loss: 0.0253
2025-03-24 17:40:35,233 - cloned_fsc_actor_policy.py:190 - Epoch 60000, Accuracy: 0.9890
2025-03-24 17:40:35,267 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:40:52,815 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:40:52,815 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:40:52,815 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:40:52,815 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:40:52,815 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:40:52,815 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:40:52,815 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:40:53,671 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:41:09,578 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:41:09,578 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 17:41:09,578 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 17:41:09,578 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 17:41:09,578 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:41:09,578 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:41:09,578 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 17:41:10,143 - pomdp.py:349 - unfolding 5-FSC template into POMDP...
2025-03-24 17:41:10,989 - pomdp.py:355 - constructed quotient MDP having 33898 states and 617106 actions.
2025-03-24 17:41:12,772 - rl_family_extractor.py:329 - Building family from FFNN...
2025-03-24 17:41:12,872 - rl_family_extractor.py:357 - Number of misses: 0 out of 1000
2025-03-24 17:41:12,872 - rl_family_extractor.py:359 - Number of complete misses: 0 out of 1000
2025-03-24 17:41:12,873 - statistic.py:67 - synthesis initiated, design space: 1
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 0.01 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 100 %
DTMC stats: avg DTMC size: 506, iterations: 1

optimum: 0.140482
--------------------
2025-03-24 17:41:12,884 - statistic.py:67 - synthesis initiated, design space: 1e609
> progress 0.0%, elapsed 3 s, estimated 3000297 s (34 days), iters = {DTMC: 344}, opt = 0.1405
> progress 0.0%, elapsed 6 s, estimated 6003956 s (69 days), iters = {DTMC: 681}, opt = 0.1405
> progress 0.0%, elapsed 9 s, estimated 9010393 s (104 days), iters = {DTMC: 1020}, opt = 0.1405
> progress 0.0%, elapsed 12 s, estimated 12016054 s (139 days), iters = {DTMC: 1358}, opt = 0.1405
> progress 0.0%, elapsed 15 s, estimated 15025033 s (173 days), iters = {DTMC: 1648}, opt = 0.1405
> progress 0.0%, elapsed 18 s, estimated 18026833 s (208 days), iters = {DTMC: 1972}, opt = 0.1405
> progress 0.0%, elapsed 21 s, estimated 21035935 s (243 days), iters = {DTMC: 2315}, opt = 0.1405
> progress 0.0%, elapsed 24 s, estimated 24045119 s (278 days), iters = {DTMC: 2649}, opt = 0.1405
> progress 0.0%, elapsed 27 s, estimated 27050910 s (313 days), iters = {DTMC: 2986}, opt = 0.1405
> progress 0.0%, elapsed 30 s, estimated 30051377 s (347 days), iters = {DTMC: 3321}, opt = 0.1405
> progress 0.0%, elapsed 33 s, estimated 33059099 s (1 year), iters = {DTMC: 3655}, opt = 0.1405
> progress 0.0%, elapsed 36 s, estimated 36066928 s (1 year), iters = {DTMC: 3994}, opt = 0.1405
> progress 0.0%, elapsed 39 s, estimated 39074231 s (1 year), iters = {DTMC: 4330}, opt = 0.1405
> progress 0.0%, elapsed 42 s, estimated 42078186 s (1 year), iters = {DTMC: 4664}, opt = 0.1405
> progress 0.0%, elapsed 45 s, estimated 45079407 s (1 year), iters = {DTMC: 5000}, opt = 0.1405
> progress 0.0%, elapsed 48 s, estimated 48087720 s (1 year), iters = {DTMC: 5332}, opt = 0.1405
> progress 0.0%, elapsed 51 s, estimated 51090082 s (1 year), iters = {DTMC: 5641}, opt = 0.1405
> progress 0.0%, elapsed 54 s, estimated 54095974 s (1 year), iters = {DTMC: 5979}, opt = 0.1405
> progress 0.0%, elapsed 57 s, estimated 57099671 s (1 year), iters = {DTMC: 6316}, opt = 0.1405
2025-03-24 17:42:12,890 - synthesizer_onebyone.py:19 - Time limit reached
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.01 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 0 %
DTMC stats: avg DTMC size: 212, iterations: 6642

optimum: 0.140482
--------------------
2025-03-24 17:42:12,891 - synthesizer_rl_storm_paynt.py:280 - No improving assignment found.
2025-03-24 17:42:12,892 - storm_pomdp_control.py:246 - Interactive Storm resumed
2025-03-24 17:42:12,892 - storm_pomdp_control.py:291 - Updating FSC values in Storm
2025-03-24 17:42:43,901 - storm_pomdp_control.py:305 - Pausing Storm
-----------Storm-----------               
Value = 0.23461992224252154 | Time elapsed = 9236.1s | FSC size = 1636

2025-03-24 17:44:27,216 - synthesizer_pomdp.py:253 - Timeout for PAYNT started
2025-03-24 17:44:27,250 - synthesizer_ar_storm.py:136 - Resuming synthesis
2025-03-24 17:44:27,252 - synthesizer_ar_storm.py:147 - PAYNT's value is better. Prioritizing synthesis results
2025-03-24 17:44:57,379 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 17:44:57,442 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:45:07,101 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:45:07,102 - father_agent.py:564 - Average Virtual Goal Value = 5.364233016967773
2025-03-24 17:45:07,102 - father_agent.py:566 - Goal Reach Probability = 0.10728466088647598
2025-03-24 17:45:07,102 - father_agent.py:568 - Trap Reach Probability = 0.8927153391135241
2025-03-24 17:45:07,102 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:45:07,102 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:45:07,102 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:45:12,082 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:45:16,417 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:45:16,417 - evaluators.py:130 - Average Virtual Goal Value = 0.36112257838249207
2025-03-24 17:45:16,417 - evaluators.py:132 - Goal Reach Probability = 0.007222451506397029
2025-03-24 17:45:16,417 - evaluators.py:134 - Trap Reach Probability = 0.992777548493603
2025-03-24 17:45:16,417 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:45:16,417 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:45:16,417 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:45:16,418 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 1.969398021697998
Epoch: 0, Critic Loss: 10.112152099609375
Epoch: 10, Actor Loss: 0.039617232978343964
Epoch: 10, Critic Loss: 17.173965454101562
Epoch: 20, Actor Loss: 0.024600062519311905
Epoch: 20, Critic Loss: 9.1117525100708
Epoch: 30, Actor Loss: 0.03225422278046608
Epoch: 30, Critic Loss: 14.30079174041748
Epoch: 40, Actor Loss: 0.026127485558390617
Epoch: 40, Critic Loss: 9.60390853881836
2025-03-24 17:45:49,596 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:45:53,001 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:45:53,001 - evaluators.py:130 - Average Virtual Goal Value = 9.941412925720215
2025-03-24 17:45:53,001 - evaluators.py:132 - Goal Reach Probability = 0.1988282680336873
2025-03-24 17:45:53,001 - evaluators.py:134 - Trap Reach Probability = 0.8011717319663128
2025-03-24 17:45:53,001 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:45:53,001 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:45:53,001 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:45:53,001 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.02347397617995739
Epoch: 50, Critic Loss: 11.312536239624023
Epoch: 60, Actor Loss: 0.042625684291124344
Epoch: 60, Critic Loss: 14.295186996459961
Epoch: 70, Actor Loss: 0.02232249081134796
Epoch: 70, Critic Loss: 15.218202590942383
Epoch: 80, Actor Loss: 0.02161340042948723
Epoch: 80, Critic Loss: 10.553330421447754
Epoch: 90, Actor Loss: 0.03202801197767258
Epoch: 90, Critic Loss: 9.460461616516113
2025-03-24 17:46:26,623 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:46:29,972 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:46:29,973 - evaluators.py:130 - Average Virtual Goal Value = 9.9049711227417
2025-03-24 17:46:29,973 - evaluators.py:132 - Goal Reach Probability = 0.19809941520467836
2025-03-24 17:46:29,973 - evaluators.py:134 - Trap Reach Probability = 0.8019005847953217
2025-03-24 17:46:29,973 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:46:29,973 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:46:29,973 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:46:29,973 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.036806970834732056
Epoch: 100, Critic Loss: 15.991373062133789
Epoch: 110, Actor Loss: 0.02200687862932682
Epoch: 110, Critic Loss: 9.782678604125977
Epoch: 120, Actor Loss: 0.03146154060959816
Epoch: 120, Critic Loss: 8.57754898071289
Epoch: 130, Actor Loss: 0.03653072938323021
Epoch: 130, Critic Loss: 6.760459899902344
Epoch: 140, Actor Loss: 0.02500740997493267
Epoch: 140, Critic Loss: 11.887389183044434
2025-03-24 17:47:03,380 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:47:07,083 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:47:07,083 - evaluators.py:130 - Average Virtual Goal Value = 9.532641410827637
2025-03-24 17:47:07,083 - evaluators.py:132 - Goal Reach Probability = 0.19065281899109793
2025-03-24 17:47:07,083 - evaluators.py:134 - Trap Reach Probability = 0.8093471810089021
2025-03-24 17:47:07,083 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:47:07,083 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:47:07,083 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:47:07,083 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.015286398120224476
Epoch: 150, Critic Loss: 7.271706581115723
Epoch: 160, Actor Loss: 0.023313920944929123
Epoch: 160, Critic Loss: 5.507996559143066
Epoch: 170, Actor Loss: 0.0225564856082201
Epoch: 170, Critic Loss: 13.033547401428223
Epoch: 180, Actor Loss: 0.024271665140986443
Epoch: 180, Critic Loss: 12.419705390930176
Epoch: 190, Actor Loss: 0.024546224623918533
Epoch: 190, Critic Loss: 6.697717666625977
2025-03-24 17:47:40,786 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:47:44,043 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:47:44,043 - evaluators.py:130 - Average Virtual Goal Value = 9.428157806396484
2025-03-24 17:47:44,043 - evaluators.py:132 - Goal Reach Probability = 0.1885631559898661
2025-03-24 17:47:44,043 - evaluators.py:134 - Trap Reach Probability = 0.8114368440101339
2025-03-24 17:47:44,044 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:47:44,044 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:47:44,044 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:47:44,044 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.022969281300902367
Epoch: 200, Critic Loss: 12.822956085205078
Epoch: 210, Actor Loss: 0.026516761630773544
Epoch: 210, Critic Loss: 6.121073246002197
Epoch: 220, Actor Loss: 0.022784315049648285
Epoch: 220, Critic Loss: 11.905963897705078
Epoch: 230, Actor Loss: 0.03611801937222481
Epoch: 230, Critic Loss: 6.479462623596191
Epoch: 240, Actor Loss: 0.03271586075425148
Epoch: 240, Critic Loss: 10.210140228271484
2025-03-24 17:48:17,378 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:48:20,893 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:48:20,893 - evaluators.py:130 - Average Virtual Goal Value = 10.247283935546875
2025-03-24 17:48:20,893 - evaluators.py:132 - Goal Reach Probability = 0.20494567253653054
2025-03-24 17:48:20,893 - evaluators.py:134 - Trap Reach Probability = 0.7950543274634695
2025-03-24 17:48:20,893 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:48:20,893 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:48:20,893 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:48:20,893 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.02436440996825695
Epoch: 250, Critic Loss: 10.892440795898438
Epoch: 260, Actor Loss: 0.021156778559088707
Epoch: 260, Critic Loss: 9.664939880371094
Epoch: 270, Actor Loss: 0.01896018534898758
Epoch: 270, Critic Loss: 8.810369491577148
Epoch: 280, Actor Loss: 0.021606003865599632
Epoch: 280, Critic Loss: 8.070557594299316
Epoch: 290, Actor Loss: 0.025780651718378067
Epoch: 290, Critic Loss: 14.138276100158691
2025-03-24 17:48:53,501 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:48:56,967 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:48:56,968 - evaluators.py:130 - Average Virtual Goal Value = 8.65384578704834
2025-03-24 17:48:56,968 - evaluators.py:132 - Goal Reach Probability = 0.17307692307692307
2025-03-24 17:48:56,968 - evaluators.py:134 - Trap Reach Probability = 0.8269230769230769
2025-03-24 17:48:56,968 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:48:56,968 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:48:56,968 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:48:56,968 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.02781020663678646
Epoch: 300, Critic Loss: 11.726228713989258
Epoch: 310, Actor Loss: 0.018800286576151848
Epoch: 310, Critic Loss: 9.83542251586914
Epoch: 320, Actor Loss: 0.014956098981201649
Epoch: 320, Critic Loss: 7.932851314544678
Epoch: 330, Actor Loss: 0.02094968408346176
Epoch: 330, Critic Loss: 11.62286376953125
Epoch: 340, Actor Loss: 0.03316238895058632
Epoch: 340, Critic Loss: 13.012163162231445
2025-03-24 17:49:30,493 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:49:34,403 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:49:34,403 - evaluators.py:130 - Average Virtual Goal Value = 9.467348098754883
2025-03-24 17:49:34,403 - evaluators.py:132 - Goal Reach Probability = 0.18934695366654505
2025-03-24 17:49:34,403 - evaluators.py:134 - Trap Reach Probability = 0.8106530463334549
2025-03-24 17:49:34,403 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:49:34,403 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:49:34,403 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:49:34,403 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.024278301745653152
Epoch: 350, Critic Loss: 7.581554412841797
Epoch: 360, Actor Loss: 0.01947149448096752
Epoch: 360, Critic Loss: 8.320396423339844
Epoch: 370, Actor Loss: 0.021875733509659767
Epoch: 370, Critic Loss: 6.803709030151367
Epoch: 380, Actor Loss: 0.01788869872689247
Epoch: 380, Critic Loss: 9.59441089630127
Epoch: 390, Actor Loss: 0.020335139706730843
Epoch: 390, Critic Loss: 12.035157203674316
2025-03-24 17:50:08,526 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:50:11,846 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:50:11,846 - evaluators.py:130 - Average Virtual Goal Value = 8.858964920043945
2025-03-24 17:50:11,846 - evaluators.py:132 - Goal Reach Probability = 0.1771793054571226
2025-03-24 17:50:11,846 - evaluators.py:134 - Trap Reach Probability = 0.8228206945428774
2025-03-24 17:50:11,846 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:50:11,846 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:50:11,846 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:50:11,846 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.016053717583417892
Epoch: 400, Critic Loss: 9.290223121643066
Epoch: 410, Actor Loss: 0.03448692709207535
Epoch: 410, Critic Loss: 6.222231388092041
Epoch: 420, Actor Loss: 0.035496070981025696
Epoch: 420, Critic Loss: 14.110885620117188
Epoch: 430, Actor Loss: 0.0145519794896245
Epoch: 430, Critic Loss: 15.439326286315918
Epoch: 440, Actor Loss: 0.034349843859672546
Epoch: 440, Critic Loss: 15.946138381958008
2025-03-24 17:50:45,180 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:50:48,674 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:50:48,674 - evaluators.py:130 - Average Virtual Goal Value = 10.033507347106934
2025-03-24 17:50:48,674 - evaluators.py:132 - Goal Reach Probability = 0.20067014147431125
2025-03-24 17:50:48,674 - evaluators.py:134 - Trap Reach Probability = 0.7993298585256887
2025-03-24 17:50:48,674 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:50:48,674 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:50:48,674 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:50:48,674 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.024042807519435883
Epoch: 450, Critic Loss: 5.650359153747559
Epoch: 460, Actor Loss: 0.027529917657375336
Epoch: 460, Critic Loss: 13.922799110412598
Epoch: 470, Actor Loss: 0.02640601247549057
Epoch: 470, Critic Loss: 7.8596062660217285
Epoch: 480, Actor Loss: 0.02071947045624256
Epoch: 480, Critic Loss: 7.419735908508301
Epoch: 490, Actor Loss: 0.021325355395674706
Epoch: 490, Critic Loss: 11.371278762817383
2025-03-24 17:51:21,734 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:51:25,217 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:51:25,217 - evaluators.py:130 - Average Virtual Goal Value = 9.115426063537598
2025-03-24 17:51:25,217 - evaluators.py:132 - Goal Reach Probability = 0.18230852211434737
2025-03-24 17:51:25,217 - evaluators.py:134 - Trap Reach Probability = 0.8176914778856527
2025-03-24 17:51:25,217 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:51:25,217 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:51:25,218 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:51:25,218 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.018980955705046654
Epoch: 500, Critic Loss: 7.389039993286133
Epoch: 510, Actor Loss: 0.018676508218050003
Epoch: 510, Critic Loss: 11.554244041442871
Epoch: 520, Actor Loss: 0.027350809425115585
Epoch: 520, Critic Loss: 7.391637802124023
Epoch: 530, Actor Loss: 0.022908177226781845
Epoch: 530, Critic Loss: 12.722143173217773
Epoch: 540, Actor Loss: 0.02037198469042778
Epoch: 540, Critic Loss: 10.837190628051758
2025-03-24 17:51:58,811 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:52:01,974 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:52:01,974 - evaluators.py:130 - Average Virtual Goal Value = 10.633147239685059
2025-03-24 17:52:01,974 - evaluators.py:132 - Goal Reach Probability = 0.21266294227188082
2025-03-24 17:52:01,975 - evaluators.py:134 - Trap Reach Probability = 0.7873370577281191
2025-03-24 17:52:01,975 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:52:01,975 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:52:01,975 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:52:01,975 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.012953227385878563
Epoch: 550, Critic Loss: 10.954789161682129
Epoch: 560, Actor Loss: 0.016171036288142204
Epoch: 560, Critic Loss: 18.335895538330078
Epoch: 570, Actor Loss: 0.030706286430358887
Epoch: 570, Critic Loss: 10.932279586791992
Epoch: 580, Actor Loss: 0.045323487371206284
Epoch: 580, Critic Loss: 12.209638595581055
Epoch: 590, Actor Loss: 0.03915040194988251
Epoch: 590, Critic Loss: 13.819204330444336
2025-03-24 17:52:35,340 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:52:38,973 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:52:38,973 - evaluators.py:130 - Average Virtual Goal Value = 9.258576393127441
2025-03-24 17:52:38,973 - evaluators.py:132 - Goal Reach Probability = 0.18517152342309112
2025-03-24 17:52:38,973 - evaluators.py:134 - Trap Reach Probability = 0.8148284765769089
2025-03-24 17:52:38,973 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:52:38,973 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:52:38,973 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:52:38,973 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.011900003999471664
Epoch: 600, Critic Loss: 11.627339363098145
Epoch: 610, Actor Loss: 0.026698598638176918
Epoch: 610, Critic Loss: 13.936269760131836
Epoch: 620, Actor Loss: 0.010549749247729778
Epoch: 620, Critic Loss: 12.106832504272461
Epoch: 630, Actor Loss: 0.031456176191568375
Epoch: 630, Critic Loss: 12.49439525604248
Epoch: 640, Actor Loss: 0.021553045138716698
Epoch: 640, Critic Loss: 8.084956169128418
2025-03-24 17:53:11,790 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:53:15,207 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:53:15,207 - evaluators.py:130 - Average Virtual Goal Value = 9.896602630615234
2025-03-24 17:53:15,207 - evaluators.py:132 - Goal Reach Probability = 0.19793205317577547
2025-03-24 17:53:15,207 - evaluators.py:134 - Trap Reach Probability = 0.8020679468242246
2025-03-24 17:53:15,207 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:53:15,207 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:53:15,207 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:53:15,207 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.028484812006354332
Epoch: 650, Critic Loss: 11.919330596923828
Epoch: 660, Actor Loss: 0.0315755233168602
Epoch: 660, Critic Loss: 10.117259979248047
Epoch: 670, Actor Loss: 0.022973358631134033
Epoch: 670, Critic Loss: 8.470953941345215
Epoch: 680, Actor Loss: 0.018147235736250877
Epoch: 680, Critic Loss: 9.653017044067383
Epoch: 690, Actor Loss: 0.029583310708403587
Epoch: 690, Critic Loss: 8.90821361541748
2025-03-24 17:53:47,762 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:53:52,421 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:53:52,421 - evaluators.py:130 - Average Virtual Goal Value = 10.161836624145508
2025-03-24 17:53:52,421 - evaluators.py:132 - Goal Reach Probability = 0.20323673315769666
2025-03-24 17:53:52,421 - evaluators.py:134 - Trap Reach Probability = 0.7967632668423034
2025-03-24 17:53:52,421 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:53:52,421 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:53:52,421 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 17:53:52,421 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.02039692923426628
Epoch: 700, Critic Loss: 8.006897926330566
2025-03-24 17:53:52,424 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 17:53:52,424 - father_agent.py:449 - Before training evaluation.
2025-03-24 17:53:52,424 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:53:59,838 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:53:59,838 - father_agent.py:564 - Average Virtual Goal Value = 10.622238159179688
2025-03-24 17:53:59,838 - father_agent.py:566 - Goal Reach Probability = 0.21244477172312223
2025-03-24 17:53:59,838 - father_agent.py:568 - Trap Reach Probability = 0.7875552282768777
2025-03-24 17:53:59,838 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:53:59,838 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:53:59,838 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:53:59,882 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:54:00,004 - father_agent.py:359 - Training agent on-policy
2025-03-24 17:54:04,821 - father_agent.py:306 - Step: 0, Training loss: 95.63629913330078
2025-03-24 17:54:04,869 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:54:12,990 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:54:12,991 - father_agent.py:564 - Average Virtual Goal Value = 10.50522232055664
2025-03-24 17:54:12,991 - father_agent.py:566 - Goal Reach Probability = 0.2101044449842118
2025-03-24 17:54:12,991 - father_agent.py:568 - Trap Reach Probability = 0.7898955550157882
2025-03-24 17:54:12,991 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:54:12,991 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:54:12,991 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:54:13,031 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:54:15,085 - father_agent.py:306 - Step: 5, Training loss: 62.773258209228516
2025-03-24 17:54:16,213 - father_agent.py:306 - Step: 10, Training loss: 35.36186218261719
2025-03-24 17:54:17,323 - father_agent.py:306 - Step: 15, Training loss: 17.377634048461914
2025-03-24 17:54:18,633 - father_agent.py:306 - Step: 20, Training loss: 10.451552391052246
2025-03-24 17:54:19,773 - father_agent.py:306 - Step: 25, Training loss: 5.627030372619629
2025-03-24 17:54:25,920 - father_agent.py:306 - Step: 30, Training loss: 2.0529186725616455
2025-03-24 17:54:27,651 - father_agent.py:306 - Step: 35, Training loss: 0.5752216577529907
2025-03-24 17:54:29,072 - father_agent.py:306 - Step: 40, Training loss: 0.7177969813346863
2025-03-24 17:54:30,229 - father_agent.py:306 - Step: 45, Training loss: 0.5985508561134338
2025-03-24 17:54:31,546 - father_agent.py:306 - Step: 50, Training loss: 0.7719850540161133
2025-03-24 17:54:32,978 - father_agent.py:306 - Step: 55, Training loss: 0.17752189934253693
2025-03-24 17:54:34,305 - father_agent.py:306 - Step: 60, Training loss: 0.42954233288764954
2025-03-24 17:54:35,646 - father_agent.py:306 - Step: 65, Training loss: 0.7369002103805542
2025-03-24 17:54:36,850 - father_agent.py:306 - Step: 70, Training loss: 0.7976688146591187
2025-03-24 17:54:38,093 - father_agent.py:306 - Step: 75, Training loss: 0.7385870218276978
2025-03-24 17:54:39,499 - father_agent.py:306 - Step: 80, Training loss: 0.6894272565841675
2025-03-24 17:54:40,710 - father_agent.py:306 - Step: 85, Training loss: 0.9288922548294067
2025-03-24 17:54:41,905 - father_agent.py:306 - Step: 90, Training loss: 0.8279995918273926
2025-03-24 17:54:43,291 - father_agent.py:306 - Step: 95, Training loss: 0.8383525013923645
2025-03-24 17:54:44,592 - father_agent.py:306 - Step: 100, Training loss: 0.7828718423843384
2025-03-24 17:54:44,637 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:54:53,473 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:54:53,473 - father_agent.py:564 - Average Virtual Goal Value = 3.605900526046753
2025-03-24 17:54:53,473 - father_agent.py:566 - Goal Reach Probability = 0.07211801129120379
2025-03-24 17:54:53,473 - father_agent.py:568 - Trap Reach Probability = 0.9278819887087962
2025-03-24 17:54:53,473 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:54:53,473 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:54:53,473 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:54:53,516 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:54:55,851 - father_agent.py:306 - Step: 105, Training loss: 0.5315607190132141
2025-03-24 17:54:57,274 - father_agent.py:306 - Step: 110, Training loss: 0.8412330746650696
2025-03-24 17:54:58,587 - father_agent.py:306 - Step: 115, Training loss: 0.7444455623626709
2025-03-24 17:54:59,895 - father_agent.py:306 - Step: 120, Training loss: 0.5433993935585022
2025-03-24 17:55:01,306 - father_agent.py:306 - Step: 125, Training loss: 0.521436333656311
2025-03-24 17:55:02,766 - father_agent.py:306 - Step: 130, Training loss: 0.5953205823898315
2025-03-24 17:55:04,062 - father_agent.py:306 - Step: 135, Training loss: 0.8716044425964355
2025-03-24 17:55:05,377 - father_agent.py:306 - Step: 140, Training loss: 0.7577980756759644
2025-03-24 17:55:06,821 - father_agent.py:306 - Step: 145, Training loss: 0.8254610300064087
2025-03-24 17:55:08,134 - father_agent.py:306 - Step: 150, Training loss: 0.5306434631347656
2025-03-24 17:55:09,458 - father_agent.py:306 - Step: 155, Training loss: 1.0181889533996582
2025-03-24 17:55:10,777 - father_agent.py:306 - Step: 160, Training loss: 0.6975678205490112
2025-03-24 17:55:12,033 - father_agent.py:306 - Step: 165, Training loss: 0.6164348721504211
2025-03-24 17:55:13,331 - father_agent.py:306 - Step: 170, Training loss: 0.47647228837013245
2025-03-24 17:55:14,757 - father_agent.py:306 - Step: 175, Training loss: 0.7900882363319397
2025-03-24 17:55:15,959 - father_agent.py:306 - Step: 180, Training loss: 0.9075890779495239
2025-03-24 17:55:17,193 - father_agent.py:306 - Step: 185, Training loss: 0.47652387619018555
2025-03-24 17:55:18,612 - father_agent.py:306 - Step: 190, Training loss: 1.174362301826477
2025-03-24 17:55:19,877 - father_agent.py:306 - Step: 195, Training loss: 0.567138135433197
2025-03-24 17:55:21,243 - father_agent.py:306 - Step: 200, Training loss: 0.47612056136131287
2025-03-24 17:55:21,287 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:55:30,679 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:55:30,680 - father_agent.py:564 - Average Virtual Goal Value = 2.7011725902557373
2025-03-24 17:55:30,680 - father_agent.py:566 - Goal Reach Probability = 0.0540234532955924
2025-03-24 17:55:30,680 - father_agent.py:568 - Trap Reach Probability = 0.9459765467044076
2025-03-24 17:55:30,680 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:55:30,680 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:55:30,680 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:55:30,729 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:55:32,840 - father_agent.py:306 - Step: 205, Training loss: 0.8398200273513794
2025-03-24 17:55:34,031 - father_agent.py:306 - Step: 210, Training loss: 0.7654052376747131
2025-03-24 17:55:35,288 - father_agent.py:306 - Step: 215, Training loss: 0.9036605358123779
2025-03-24 17:55:36,669 - father_agent.py:306 - Step: 220, Training loss: 0.9655539989471436
2025-03-24 17:55:37,869 - father_agent.py:306 - Step: 225, Training loss: 0.9073373079299927
2025-03-24 17:55:39,107 - father_agent.py:306 - Step: 230, Training loss: 0.6324296593666077
2025-03-24 17:55:40,549 - father_agent.py:306 - Step: 235, Training loss: 0.6229182481765747
2025-03-24 17:55:41,787 - father_agent.py:306 - Step: 240, Training loss: 0.5707548260688782
2025-03-24 17:55:43,028 - father_agent.py:306 - Step: 245, Training loss: 1.117425799369812
2025-03-24 17:55:44,410 - father_agent.py:306 - Step: 250, Training loss: 0.7046074867248535
2025-03-24 17:55:45,659 - father_agent.py:306 - Step: 255, Training loss: 0.7506046891212463
2025-03-24 17:55:46,994 - father_agent.py:306 - Step: 260, Training loss: 0.7047638297080994
2025-03-24 17:55:48,318 - father_agent.py:306 - Step: 265, Training loss: 0.7208850383758545
2025-03-24 17:55:49,580 - father_agent.py:306 - Step: 270, Training loss: 0.7312068939208984
2025-03-24 17:55:51,141 - father_agent.py:306 - Step: 275, Training loss: 0.7081832885742188
2025-03-24 17:55:52,591 - father_agent.py:306 - Step: 280, Training loss: 0.46931326389312744
2025-03-24 17:55:54,085 - father_agent.py:306 - Step: 285, Training loss: 0.42660269141197205
2025-03-24 17:55:55,895 - father_agent.py:306 - Step: 290, Training loss: 0.6239995956420898
2025-03-24 17:55:57,292 - father_agent.py:306 - Step: 295, Training loss: 0.626894474029541
2025-03-24 17:55:58,530 - father_agent.py:306 - Step: 300, Training loss: 0.5243371725082397
2025-03-24 17:55:58,576 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:56:08,225 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:56:08,225 - father_agent.py:564 - Average Virtual Goal Value = 1.0403120517730713
2025-03-24 17:56:08,225 - father_agent.py:566 - Goal Reach Probability = 0.02080624187256177
2025-03-24 17:56:08,225 - father_agent.py:568 - Trap Reach Probability = 0.9791937581274383
2025-03-24 17:56:08,225 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:56:08,225 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:56:08,225 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:56:08,270 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:56:10,443 - father_agent.py:306 - Step: 305, Training loss: 0.28798002004623413
2025-03-24 17:56:11,720 - father_agent.py:306 - Step: 310, Training loss: 0.3570703864097595
2025-03-24 17:56:13,131 - father_agent.py:306 - Step: 315, Training loss: 0.524709165096283
2025-03-24 17:56:14,460 - father_agent.py:306 - Step: 320, Training loss: 0.5771639943122864
2025-03-24 17:56:15,705 - father_agent.py:306 - Step: 325, Training loss: 0.5422542691230774
2025-03-24 17:56:17,091 - father_agent.py:306 - Step: 330, Training loss: 0.6295629739761353
2025-03-24 17:56:18,557 - father_agent.py:306 - Step: 335, Training loss: 0.39943283796310425
2025-03-24 17:56:19,818 - father_agent.py:306 - Step: 340, Training loss: 0.38591456413269043
2025-03-24 17:56:21,134 - father_agent.py:306 - Step: 345, Training loss: 0.28219661116600037
2025-03-24 17:56:22,620 - father_agent.py:306 - Step: 350, Training loss: 0.18560850620269775
2025-03-24 17:56:23,915 - father_agent.py:306 - Step: 355, Training loss: 0.18531471490859985
2025-03-24 17:56:25,408 - father_agent.py:306 - Step: 360, Training loss: 0.23371726274490356
2025-03-24 17:56:26,831 - father_agent.py:306 - Step: 365, Training loss: 0.23542934656143188
2025-03-24 17:56:28,558 - father_agent.py:306 - Step: 370, Training loss: 0.23215563595294952
2025-03-24 17:56:30,548 - father_agent.py:306 - Step: 375, Training loss: 0.10506963729858398
2025-03-24 17:56:31,848 - father_agent.py:306 - Step: 380, Training loss: 0.1031467393040657
2025-03-24 17:56:33,238 - father_agent.py:306 - Step: 385, Training loss: 0.3541356325149536
2025-03-24 17:56:34,696 - father_agent.py:306 - Step: 390, Training loss: 0.14101117849349976
2025-03-24 17:56:36,021 - father_agent.py:306 - Step: 395, Training loss: 0.14452703297138214
2025-03-24 17:56:37,484 - father_agent.py:306 - Step: 400, Training loss: 0.2641996145248413
2025-03-24 17:56:37,529 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:56:47,668 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:56:47,668 - father_agent.py:564 - Average Virtual Goal Value = 0.15243902802467346
2025-03-24 17:56:47,668 - father_agent.py:566 - Goal Reach Probability = 0.003048780487804878
2025-03-24 17:56:47,668 - father_agent.py:568 - Trap Reach Probability = 0.9969512195121951
2025-03-24 17:56:47,668 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:56:47,668 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:56:47,668 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:56:47,714 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:56:50,060 - father_agent.py:306 - Step: 405, Training loss: 0.0907360166311264
2025-03-24 17:56:51,373 - father_agent.py:306 - Step: 410, Training loss: 0.16705501079559326
2025-03-24 17:56:52,820 - father_agent.py:306 - Step: 415, Training loss: 0.13853712379932404
2025-03-24 17:56:54,246 - father_agent.py:306 - Step: 420, Training loss: 0.1359652876853943
2025-03-24 17:56:55,570 - father_agent.py:306 - Step: 425, Training loss: 0.26911816000938416
2025-03-24 17:56:57,054 - father_agent.py:306 - Step: 430, Training loss: 0.17482063174247742
2025-03-24 17:56:58,374 - father_agent.py:306 - Step: 435, Training loss: 0.21500106155872345
2025-03-24 17:56:59,643 - father_agent.py:306 - Step: 440, Training loss: 0.13450689613819122
2025-03-24 17:57:01,112 - father_agent.py:306 - Step: 445, Training loss: 0.23229371011257172
2025-03-24 17:57:02,426 - father_agent.py:306 - Step: 450, Training loss: 0.27069011330604553
2025-03-24 17:57:03,694 - father_agent.py:306 - Step: 455, Training loss: 0.08899751305580139
2025-03-24 17:57:05,171 - father_agent.py:306 - Step: 460, Training loss: 0.12335462868213654
2025-03-24 17:57:06,592 - father_agent.py:306 - Step: 465, Training loss: 0.1418665051460266
2025-03-24 17:57:07,878 - father_agent.py:306 - Step: 470, Training loss: 0.11686652898788452
2025-03-24 17:57:09,231 - father_agent.py:306 - Step: 475, Training loss: 0.22014449536800385
2025-03-24 17:57:10,636 - father_agent.py:306 - Step: 480, Training loss: 0.31815391778945923
2025-03-24 17:57:11,953 - father_agent.py:306 - Step: 485, Training loss: 0.11867347359657288
2025-03-24 17:57:13,325 - father_agent.py:306 - Step: 490, Training loss: 0.1406588852405548
2025-03-24 17:57:14,719 - father_agent.py:306 - Step: 495, Training loss: 0.4920097291469574
2025-03-24 17:57:16,072 - father_agent.py:306 - Step: 500, Training loss: 0.1243281438946724
2025-03-24 17:57:16,524 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:57:26,167 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:57:26,168 - father_agent.py:564 - Average Virtual Goal Value = 0.4308640956878662
2025-03-24 17:57:26,168 - father_agent.py:566 - Goal Reach Probability = 0.008617281998576963
2025-03-24 17:57:26,168 - father_agent.py:568 - Trap Reach Probability = 0.9913827180014231
2025-03-24 17:57:26,168 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:57:26,168 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:57:26,168 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:57:26,517 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:57:28,675 - father_agent.py:306 - Step: 505, Training loss: 0.09365492314100266
2025-03-24 17:57:29,956 - father_agent.py:306 - Step: 510, Training loss: 0.22285783290863037
2025-03-24 17:57:31,414 - father_agent.py:306 - Step: 515, Training loss: 0.13054510951042175
2025-03-24 17:57:32,748 - father_agent.py:306 - Step: 520, Training loss: 0.20368531346321106
2025-03-24 17:57:34,107 - father_agent.py:306 - Step: 525, Training loss: 0.27695363759994507
2025-03-24 17:57:35,506 - father_agent.py:306 - Step: 530, Training loss: 0.5442732572555542
2025-03-24 17:57:36,901 - father_agent.py:306 - Step: 535, Training loss: 0.5429341793060303
2025-03-24 17:57:38,181 - father_agent.py:306 - Step: 540, Training loss: 0.41597047448158264
2025-03-24 17:57:39,496 - father_agent.py:306 - Step: 545, Training loss: 0.40372249484062195
2025-03-24 17:57:40,890 - father_agent.py:306 - Step: 550, Training loss: 0.46235474944114685
2025-03-24 17:57:42,118 - father_agent.py:306 - Step: 555, Training loss: 0.45588448643684387
2025-03-24 17:57:43,475 - father_agent.py:306 - Step: 560, Training loss: 0.49989786744117737
2025-03-24 17:57:44,864 - father_agent.py:306 - Step: 565, Training loss: 0.5307595729827881
2025-03-24 17:57:46,132 - father_agent.py:306 - Step: 570, Training loss: 0.4150969386100769
2025-03-24 17:57:47,448 - father_agent.py:306 - Step: 575, Training loss: 0.35157161951065063
2025-03-24 17:57:48,830 - father_agent.py:306 - Step: 580, Training loss: 0.4598715901374817
2025-03-24 17:57:50,206 - father_agent.py:306 - Step: 585, Training loss: 0.26699328422546387
2025-03-24 17:57:51,750 - father_agent.py:306 - Step: 590, Training loss: 0.29708144068717957
2025-03-24 17:57:53,225 - father_agent.py:306 - Step: 595, Training loss: 0.37158656120300293
2025-03-24 17:57:54,561 - father_agent.py:306 - Step: 600, Training loss: 0.2534809708595276
2025-03-24 17:57:54,610 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:58:05,419 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:58:05,419 - father_agent.py:564 - Average Virtual Goal Value = 0.8643248081207275
2025-03-24 17:58:05,419 - father_agent.py:566 - Goal Reach Probability = 0.017286495916263578
2025-03-24 17:58:05,419 - father_agent.py:568 - Trap Reach Probability = 0.9827135040837364
2025-03-24 17:58:05,419 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:58:05,419 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:58:05,419 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:58:05,466 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:58:07,839 - father_agent.py:306 - Step: 605, Training loss: 0.32319074869155884
2025-03-24 17:58:09,120 - father_agent.py:306 - Step: 610, Training loss: 0.3325171172618866
2025-03-24 17:58:10,548 - father_agent.py:306 - Step: 615, Training loss: 0.25843191146850586
2025-03-24 17:58:11,851 - father_agent.py:306 - Step: 620, Training loss: 0.31143102049827576
2025-03-24 17:58:13,121 - father_agent.py:306 - Step: 625, Training loss: 0.41772276163101196
2025-03-24 17:58:14,537 - father_agent.py:306 - Step: 630, Training loss: 0.38418495655059814
2025-03-24 17:58:15,833 - father_agent.py:306 - Step: 635, Training loss: 0.34482696652412415
2025-03-24 17:58:17,107 - father_agent.py:306 - Step: 640, Training loss: 0.377725213766098
2025-03-24 17:58:18,549 - father_agent.py:306 - Step: 645, Training loss: 0.35714203119277954
2025-03-24 17:58:19,813 - father_agent.py:306 - Step: 650, Training loss: 0.2532147467136383
2025-03-24 17:58:21,134 - father_agent.py:306 - Step: 655, Training loss: 0.32209512591362
2025-03-24 17:58:22,560 - father_agent.py:306 - Step: 660, Training loss: 0.417855441570282
2025-03-24 17:58:23,812 - father_agent.py:306 - Step: 665, Training loss: 0.43201786279678345
2025-03-24 17:58:25,100 - father_agent.py:306 - Step: 670, Training loss: 0.4488402009010315
2025-03-24 17:58:26,529 - father_agent.py:306 - Step: 675, Training loss: 0.6007997989654541
2025-03-24 17:58:27,813 - father_agent.py:306 - Step: 680, Training loss: 0.5212432742118835
2025-03-24 17:58:29,220 - father_agent.py:306 - Step: 685, Training loss: 0.38481515645980835
2025-03-24 17:58:30,669 - father_agent.py:306 - Step: 690, Training loss: 0.27851206064224243
2025-03-24 17:58:32,367 - father_agent.py:306 - Step: 695, Training loss: 0.30332955718040466
2025-03-24 17:58:34,244 - father_agent.py:306 - Step: 700, Training loss: 0.3271806240081787
2025-03-24 17:58:34,290 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:58:43,901 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:58:43,901 - father_agent.py:564 - Average Virtual Goal Value = 0.7218056917190552
2025-03-24 17:58:43,901 - father_agent.py:566 - Goal Reach Probability = 0.01443611421279311
2025-03-24 17:58:43,901 - father_agent.py:568 - Trap Reach Probability = 0.9855638857872069
2025-03-24 17:58:43,901 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:58:43,901 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:58:43,901 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:58:43,958 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:58:44,078 - father_agent.py:455 - Training finished.
2025-03-24 17:58:44,084 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:58:44,186 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 17:58:53,582 - father_agent.py:562 - Average Return = 0.0
2025-03-24 17:58:53,582 - father_agent.py:564 - Average Virtual Goal Value = 0.6470821499824524
2025-03-24 17:58:53,582 - father_agent.py:566 - Goal Reach Probability = 0.012941643509329099
2025-03-24 17:58:53,582 - father_agent.py:568 - Trap Reach Probability = 0.9870583564906709
2025-03-24 17:58:53,582 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 17:58:53,582 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 17:58:53,582 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 17:58:53,639 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:59:11,332 - evaluators.py:128 - Average Return = 0.0
2025-03-24 17:59:11,332 - evaluators.py:130 - Average Virtual Goal Value = 1.3187991380691528
2025-03-24 17:59:11,332 - evaluators.py:132 - Goal Reach Probability = 0.026375982844889206
2025-03-24 17:59:11,332 - evaluators.py:134 - Trap Reach Probability = 0.9736240171551108
2025-03-24 17:59:11,332 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 17:59:11,332 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 17:59:11,332 - evaluators.py:140 - Current Best Reach Probability = 0.026375982844889206
2025-03-24 17:59:11,348 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 17:59:53,018 - cloned_fsc_actor_policy.py:189 - Epoch 0, Loss: 1.8478
2025-03-24 17:59:53,018 - cloned_fsc_actor_policy.py:190 - Epoch 0, Accuracy: 0.1700
2025-03-24 17:59:53,064 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:00:10,733 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:00:10,733 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:00:10,733 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:00:10,733 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:00:10,733 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:00:10,733 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:00:10,733 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 18:00:14,215 - cloned_fsc_actor_policy.py:189 - Epoch 1000, Loss: 1.0889
2025-03-24 18:00:14,216 - cloned_fsc_actor_policy.py:190 - Epoch 1000, Accuracy: 0.5742
2025-03-24 18:00:17,603 - cloned_fsc_actor_policy.py:189 - Epoch 2000, Loss: 0.6734
2025-03-24 18:00:17,603 - cloned_fsc_actor_policy.py:190 - Epoch 2000, Accuracy: 0.7477
2025-03-24 18:00:21,048 - cloned_fsc_actor_policy.py:189 - Epoch 3000, Loss: 0.5032
2025-03-24 18:00:21,048 - cloned_fsc_actor_policy.py:190 - Epoch 3000, Accuracy: 0.8279
2025-03-24 18:00:24,940 - cloned_fsc_actor_policy.py:189 - Epoch 4000, Loss: 0.4171
2025-03-24 18:00:24,941 - cloned_fsc_actor_policy.py:190 - Epoch 4000, Accuracy: 0.8542
2025-03-24 18:00:28,531 - cloned_fsc_actor_policy.py:189 - Epoch 5000, Loss: 0.3604
2025-03-24 18:00:28,531 - cloned_fsc_actor_policy.py:190 - Epoch 5000, Accuracy: 0.8895
2025-03-24 18:00:28,584 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:00:45,948 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:00:45,949 - evaluators.py:130 - Average Virtual Goal Value = 0.06658016890287399
2025-03-24 18:00:45,949 - evaluators.py:132 - Goal Reach Probability = 0.0013316033870527178
2025-03-24 18:00:45,949 - evaluators.py:134 - Trap Reach Probability = 0.9986683966129473
2025-03-24 18:00:45,949 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:00:45,949 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:00:45,949 - evaluators.py:140 - Current Best Reach Probability = 0.0013316033870527178
2025-03-24 18:00:49,333 - cloned_fsc_actor_policy.py:189 - Epoch 6000, Loss: 0.3345
2025-03-24 18:00:49,333 - cloned_fsc_actor_policy.py:190 - Epoch 6000, Accuracy: 0.8965
2025-03-24 18:00:52,802 - cloned_fsc_actor_policy.py:189 - Epoch 7000, Loss: 0.3181
2025-03-24 18:00:52,802 - cloned_fsc_actor_policy.py:190 - Epoch 7000, Accuracy: 0.8969
2025-03-24 18:00:56,311 - cloned_fsc_actor_policy.py:189 - Epoch 8000, Loss: 0.3086
2025-03-24 18:00:56,311 - cloned_fsc_actor_policy.py:190 - Epoch 8000, Accuracy: 0.8969
2025-03-24 18:00:59,643 - cloned_fsc_actor_policy.py:189 - Epoch 9000, Loss: 0.2999
2025-03-24 18:00:59,643 - cloned_fsc_actor_policy.py:190 - Epoch 9000, Accuracy: 0.8969
2025-03-24 18:01:03,138 - cloned_fsc_actor_policy.py:189 - Epoch 10000, Loss: 0.2925
2025-03-24 18:01:03,138 - cloned_fsc_actor_policy.py:190 - Epoch 10000, Accuracy: 0.8965
2025-03-24 18:01:03,183 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:01:19,607 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:01:19,607 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:01:19,607 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:01:19,607 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:01:19,607 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:01:19,607 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:01:19,607 - evaluators.py:140 - Current Best Reach Probability = 0.0013316033870527178
2025-03-24 18:01:22,969 - cloned_fsc_actor_policy.py:189 - Epoch 11000, Loss: 0.2839
2025-03-24 18:01:22,970 - cloned_fsc_actor_policy.py:190 - Epoch 11000, Accuracy: 0.8953
2025-03-24 18:01:26,444 - cloned_fsc_actor_policy.py:189 - Epoch 12000, Loss: 0.2769
2025-03-24 18:01:26,444 - cloned_fsc_actor_policy.py:190 - Epoch 12000, Accuracy: 0.8943
2025-03-24 18:01:29,759 - cloned_fsc_actor_policy.py:189 - Epoch 13000, Loss: 0.2692
2025-03-24 18:01:29,759 - cloned_fsc_actor_policy.py:190 - Epoch 13000, Accuracy: 0.8935
2025-03-24 18:01:33,153 - cloned_fsc_actor_policy.py:189 - Epoch 14000, Loss: 0.2590
2025-03-24 18:01:33,153 - cloned_fsc_actor_policy.py:190 - Epoch 14000, Accuracy: 0.8952
2025-03-24 18:01:36,516 - cloned_fsc_actor_policy.py:189 - Epoch 15000, Loss: 0.2505
2025-03-24 18:01:36,516 - cloned_fsc_actor_policy.py:190 - Epoch 15000, Accuracy: 0.9012
2025-03-24 18:01:36,560 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:01:53,386 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:01:53,386 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:01:53,386 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:01:53,386 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:01:53,386 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:01:53,386 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:01:53,386 - evaluators.py:140 - Current Best Reach Probability = 0.0013316033870527178
2025-03-24 18:01:57,070 - cloned_fsc_actor_policy.py:189 - Epoch 16000, Loss: 0.2443
2025-03-24 18:01:57,070 - cloned_fsc_actor_policy.py:190 - Epoch 16000, Accuracy: 0.9064
2025-03-24 18:02:00,529 - cloned_fsc_actor_policy.py:189 - Epoch 17000, Loss: 0.2340
2025-03-24 18:02:00,529 - cloned_fsc_actor_policy.py:190 - Epoch 17000, Accuracy: 0.9090
2025-03-24 18:02:03,849 - cloned_fsc_actor_policy.py:189 - Epoch 18000, Loss: 0.2216
2025-03-24 18:02:03,849 - cloned_fsc_actor_policy.py:190 - Epoch 18000, Accuracy: 0.9197
2025-03-24 18:02:07,697 - cloned_fsc_actor_policy.py:189 - Epoch 19000, Loss: 0.2075
2025-03-24 18:02:07,697 - cloned_fsc_actor_policy.py:190 - Epoch 19000, Accuracy: 0.9247
2025-03-24 18:02:11,252 - cloned_fsc_actor_policy.py:189 - Epoch 20000, Loss: 0.1896
2025-03-24 18:02:11,252 - cloned_fsc_actor_policy.py:190 - Epoch 20000, Accuracy: 0.9299
2025-03-24 18:02:11,293 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:02:27,337 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:02:27,337 - evaluators.py:130 - Average Virtual Goal Value = 1.328740119934082
2025-03-24 18:02:27,337 - evaluators.py:132 - Goal Reach Probability = 0.0265748031496063
2025-03-24 18:02:27,337 - evaluators.py:134 - Trap Reach Probability = 0.9734251968503937
2025-03-24 18:02:27,337 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:02:27,337 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:02:27,337 - evaluators.py:140 - Current Best Reach Probability = 0.0265748031496063
2025-03-24 18:02:30,819 - cloned_fsc_actor_policy.py:189 - Epoch 21000, Loss: 0.1625
2025-03-24 18:02:30,819 - cloned_fsc_actor_policy.py:190 - Epoch 21000, Accuracy: 0.9382
2025-03-24 18:02:34,168 - cloned_fsc_actor_policy.py:189 - Epoch 22000, Loss: 0.1225
2025-03-24 18:02:34,168 - cloned_fsc_actor_policy.py:190 - Epoch 22000, Accuracy: 0.9622
2025-03-24 18:02:37,531 - cloned_fsc_actor_policy.py:189 - Epoch 23000, Loss: 0.1093
2025-03-24 18:02:37,532 - cloned_fsc_actor_policy.py:190 - Epoch 23000, Accuracy: 0.9643
2025-03-24 18:02:41,435 - cloned_fsc_actor_policy.py:189 - Epoch 24000, Loss: 0.1046
2025-03-24 18:02:41,435 - cloned_fsc_actor_policy.py:190 - Epoch 24000, Accuracy: 0.9649
2025-03-24 18:02:44,973 - cloned_fsc_actor_policy.py:189 - Epoch 25000, Loss: 0.1003
2025-03-24 18:02:44,973 - cloned_fsc_actor_policy.py:190 - Epoch 25000, Accuracy: 0.9660
2025-03-24 18:02:45,025 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:03:01,677 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:03:01,677 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:03:01,677 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:03:01,677 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:03:01,677 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:03:01,677 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:03:01,677 - evaluators.py:140 - Current Best Reach Probability = 0.0265748031496063
2025-03-24 18:03:05,085 - cloned_fsc_actor_policy.py:189 - Epoch 26000, Loss: 0.0961
2025-03-24 18:03:05,085 - cloned_fsc_actor_policy.py:190 - Epoch 26000, Accuracy: 0.9679
2025-03-24 18:03:08,508 - cloned_fsc_actor_policy.py:189 - Epoch 27000, Loss: 0.0938
2025-03-24 18:03:08,508 - cloned_fsc_actor_policy.py:190 - Epoch 27000, Accuracy: 0.9696
2025-03-24 18:03:11,961 - cloned_fsc_actor_policy.py:189 - Epoch 28000, Loss: 0.0919
2025-03-24 18:03:11,961 - cloned_fsc_actor_policy.py:190 - Epoch 28000, Accuracy: 0.9705
2025-03-24 18:03:15,341 - cloned_fsc_actor_policy.py:189 - Epoch 29000, Loss: 0.0895
2025-03-24 18:03:15,342 - cloned_fsc_actor_policy.py:190 - Epoch 29000, Accuracy: 0.9711
2025-03-24 18:03:18,668 - cloned_fsc_actor_policy.py:189 - Epoch 30000, Loss: 0.0883
2025-03-24 18:03:18,669 - cloned_fsc_actor_policy.py:190 - Epoch 30000, Accuracy: 0.9711
2025-03-24 18:03:18,718 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:03:35,313 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:03:35,313 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:03:35,313 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:03:35,313 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:03:35,313 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:03:35,313 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:03:35,313 - evaluators.py:140 - Current Best Reach Probability = 0.0265748031496063
2025-03-24 18:03:38,670 - cloned_fsc_actor_policy.py:189 - Epoch 31000, Loss: 0.0858
2025-03-24 18:03:38,670 - cloned_fsc_actor_policy.py:190 - Epoch 31000, Accuracy: 0.9712
2025-03-24 18:03:42,034 - cloned_fsc_actor_policy.py:189 - Epoch 32000, Loss: 0.0846
2025-03-24 18:03:42,035 - cloned_fsc_actor_policy.py:190 - Epoch 32000, Accuracy: 0.9713
2025-03-24 18:03:45,371 - cloned_fsc_actor_policy.py:189 - Epoch 33000, Loss: 0.0836
2025-03-24 18:03:45,371 - cloned_fsc_actor_policy.py:190 - Epoch 33000, Accuracy: 0.9711
2025-03-24 18:03:48,706 - cloned_fsc_actor_policy.py:189 - Epoch 34000, Loss: 0.0828
2025-03-24 18:03:48,706 - cloned_fsc_actor_policy.py:190 - Epoch 34000, Accuracy: 0.9710
2025-03-24 18:03:52,371 - cloned_fsc_actor_policy.py:189 - Epoch 35000, Loss: 0.0818
2025-03-24 18:03:52,371 - cloned_fsc_actor_policy.py:190 - Epoch 35000, Accuracy: 0.9711
2025-03-24 18:03:52,576 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:04:09,015 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:04:09,015 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:04:09,016 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:04:09,016 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:04:09,016 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:04:09,016 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:04:09,016 - evaluators.py:140 - Current Best Reach Probability = 0.0265748031496063
2025-03-24 18:04:12,901 - cloned_fsc_actor_policy.py:189 - Epoch 36000, Loss: 0.0800
2025-03-24 18:04:12,901 - cloned_fsc_actor_policy.py:190 - Epoch 36000, Accuracy: 0.9715
2025-03-24 18:04:16,371 - cloned_fsc_actor_policy.py:189 - Epoch 37000, Loss: 0.0790
2025-03-24 18:04:16,371 - cloned_fsc_actor_policy.py:190 - Epoch 37000, Accuracy: 0.9712
2025-03-24 18:04:19,683 - cloned_fsc_actor_policy.py:189 - Epoch 38000, Loss: 0.0779
2025-03-24 18:04:19,683 - cloned_fsc_actor_policy.py:190 - Epoch 38000, Accuracy: 0.9712
2025-03-24 18:04:23,109 - cloned_fsc_actor_policy.py:189 - Epoch 39000, Loss: 0.0779
2025-03-24 18:04:23,109 - cloned_fsc_actor_policy.py:190 - Epoch 39000, Accuracy: 0.9709
2025-03-24 18:04:26,670 - cloned_fsc_actor_policy.py:189 - Epoch 40000, Loss: 0.0758
2025-03-24 18:04:26,670 - cloned_fsc_actor_policy.py:190 - Epoch 40000, Accuracy: 0.9715
2025-03-24 18:04:26,709 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:04:43,235 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:04:43,235 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:04:43,235 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:04:43,235 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:04:43,235 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:04:43,235 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:04:43,235 - evaluators.py:140 - Current Best Reach Probability = 0.0265748031496063
2025-03-24 18:04:47,069 - cloned_fsc_actor_policy.py:189 - Epoch 41000, Loss: 0.0770
2025-03-24 18:04:47,070 - cloned_fsc_actor_policy.py:190 - Epoch 41000, Accuracy: 0.9705
2025-03-24 18:04:50,569 - cloned_fsc_actor_policy.py:189 - Epoch 42000, Loss: 0.0751
2025-03-24 18:04:50,569 - cloned_fsc_actor_policy.py:190 - Epoch 42000, Accuracy: 0.9710
2025-03-24 18:04:53,912 - cloned_fsc_actor_policy.py:189 - Epoch 43000, Loss: 0.0744
2025-03-24 18:04:53,912 - cloned_fsc_actor_policy.py:190 - Epoch 43000, Accuracy: 0.9711
2025-03-24 18:04:57,424 - cloned_fsc_actor_policy.py:189 - Epoch 44000, Loss: 0.0734
2025-03-24 18:04:57,424 - cloned_fsc_actor_policy.py:190 - Epoch 44000, Accuracy: 0.9714
2025-03-24 18:05:00,841 - cloned_fsc_actor_policy.py:189 - Epoch 45000, Loss: 0.0729
2025-03-24 18:05:00,842 - cloned_fsc_actor_policy.py:190 - Epoch 45000, Accuracy: 0.9713
2025-03-24 18:05:00,876 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:05:17,424 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:05:17,424 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:05:17,424 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:05:17,424 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:05:17,424 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:05:17,424 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:05:17,424 - evaluators.py:140 - Current Best Reach Probability = 0.0265748031496063
2025-03-24 18:05:20,871 - cloned_fsc_actor_policy.py:189 - Epoch 46000, Loss: 0.0729
2025-03-24 18:05:20,871 - cloned_fsc_actor_policy.py:190 - Epoch 46000, Accuracy: 0.9712
2025-03-24 18:05:24,228 - cloned_fsc_actor_policy.py:189 - Epoch 47000, Loss: 0.0724
2025-03-24 18:05:24,228 - cloned_fsc_actor_policy.py:190 - Epoch 47000, Accuracy: 0.9714
2025-03-24 18:05:27,608 - cloned_fsc_actor_policy.py:189 - Epoch 48000, Loss: 0.0698
2025-03-24 18:05:27,608 - cloned_fsc_actor_policy.py:190 - Epoch 48000, Accuracy: 0.9720
2025-03-24 18:05:31,010 - cloned_fsc_actor_policy.py:189 - Epoch 49000, Loss: 0.0714
2025-03-24 18:05:31,010 - cloned_fsc_actor_policy.py:190 - Epoch 49000, Accuracy: 0.9717
2025-03-24 18:05:34,409 - cloned_fsc_actor_policy.py:189 - Epoch 50000, Loss: 0.0709
2025-03-24 18:05:34,409 - cloned_fsc_actor_policy.py:190 - Epoch 50000, Accuracy: 0.9718
2025-03-24 18:05:34,447 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:05:51,111 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:05:51,111 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:05:51,111 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:05:51,112 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:05:51,112 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:05:51,112 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:05:51,112 - evaluators.py:140 - Current Best Reach Probability = 0.0265748031496063
2025-03-24 18:05:54,777 - cloned_fsc_actor_policy.py:189 - Epoch 51000, Loss: 0.0708
2025-03-24 18:05:54,778 - cloned_fsc_actor_policy.py:190 - Epoch 51000, Accuracy: 0.9716
2025-03-24 18:05:58,078 - cloned_fsc_actor_policy.py:189 - Epoch 52000, Loss: 0.0696
2025-03-24 18:05:58,078 - cloned_fsc_actor_policy.py:190 - Epoch 52000, Accuracy: 0.9719
2025-03-24 18:06:01,512 - cloned_fsc_actor_policy.py:189 - Epoch 53000, Loss: 0.0692
2025-03-24 18:06:01,512 - cloned_fsc_actor_policy.py:190 - Epoch 53000, Accuracy: 0.9721
2025-03-24 18:06:04,874 - cloned_fsc_actor_policy.py:189 - Epoch 54000, Loss: 0.0686
2025-03-24 18:06:04,874 - cloned_fsc_actor_policy.py:190 - Epoch 54000, Accuracy: 0.9722
2025-03-24 18:06:08,245 - cloned_fsc_actor_policy.py:189 - Epoch 55000, Loss: 0.0692
2025-03-24 18:06:08,246 - cloned_fsc_actor_policy.py:190 - Epoch 55000, Accuracy: 0.9720
2025-03-24 18:06:08,280 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:06:26,113 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:06:26,113 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:06:26,113 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:06:26,113 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:06:26,113 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:06:26,113 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:06:26,113 - evaluators.py:140 - Current Best Reach Probability = 0.0265748031496063
2025-03-24 18:06:29,546 - cloned_fsc_actor_policy.py:189 - Epoch 56000, Loss: 0.0690
2025-03-24 18:06:29,546 - cloned_fsc_actor_policy.py:190 - Epoch 56000, Accuracy: 0.9721
2025-03-24 18:06:32,990 - cloned_fsc_actor_policy.py:189 - Epoch 57000, Loss: 0.0692
2025-03-24 18:06:32,990 - cloned_fsc_actor_policy.py:190 - Epoch 57000, Accuracy: 0.9717
2025-03-24 18:06:36,404 - cloned_fsc_actor_policy.py:189 - Epoch 58000, Loss: 0.0680
2025-03-24 18:06:36,404 - cloned_fsc_actor_policy.py:190 - Epoch 58000, Accuracy: 0.9724
2025-03-24 18:06:39,834 - cloned_fsc_actor_policy.py:189 - Epoch 59000, Loss: 0.0671
2025-03-24 18:06:39,834 - cloned_fsc_actor_policy.py:190 - Epoch 59000, Accuracy: 0.9728
2025-03-24 18:06:43,237 - cloned_fsc_actor_policy.py:189 - Epoch 60000, Loss: 0.0681
2025-03-24 18:06:43,237 - cloned_fsc_actor_policy.py:190 - Epoch 60000, Accuracy: 0.9721
2025-03-24 18:06:43,270 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:07:00,830 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:07:00,830 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 18:07:00,830 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 18:07:00,830 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 18:07:00,830 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:07:00,830 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:07:00,830 - evaluators.py:140 - Current Best Reach Probability = 0.0265748031496063
2025-03-24 18:07:01,652 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:07:16,849 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:07:16,849 - evaluators.py:130 - Average Virtual Goal Value = 1.3662831783294678
2025-03-24 18:07:16,849 - evaluators.py:132 - Goal Reach Probability = 0.02732566297179739
2025-03-24 18:07:16,849 - evaluators.py:134 - Trap Reach Probability = 0.9726743370282026
2025-03-24 18:07:16,849 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:07:16,849 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:07:16,849 - evaluators.py:140 - Current Best Reach Probability = 0.02732566297179739
2025-03-24 18:07:17,516 - pomdp.py:349 - unfolding 5-FSC template into POMDP...
2025-03-24 18:07:18,402 - pomdp.py:355 - constructed quotient MDP having 33898 states and 617106 actions.
2025-03-24 18:07:20,496 - rl_family_extractor.py:329 - Building family from FFNN...
2025-03-24 18:07:20,580 - rl_family_extractor.py:357 - Number of misses: 1 out of 1000
2025-03-24 18:07:20,580 - rl_family_extractor.py:359 - Number of complete misses: 1 out of 1000
2025-03-24 18:07:20,581 - statistic.py:67 - synthesis initiated, design space: 4
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 0.04 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 100 %
DTMC stats: avg DTMC size: 938, iterations: 4

optimum: 0.140482
--------------------
2025-03-24 18:07:20,619 - statistic.py:67 - synthesis initiated, design space: 1e609
> progress 0.0%, elapsed 3 s, estimated 3008170 s (34 days), iters = {DTMC: 342}, opt = 0.1405
> progress 0.0%, elapsed 6 s, estimated 6011135 s (69 days), iters = {DTMC: 681}, opt = 0.1405
> progress 0.0%, elapsed 9 s, estimated 9019509 s (104 days), iters = {DTMC: 1024}, opt = 0.1405
> progress 0.0%, elapsed 12 s, estimated 12025954 s (139 days), iters = {DTMC: 1366}, opt = 0.1405
> progress 0.0%, elapsed 15 s, estimated 15026934 s (173 days), iters = {DTMC: 1711}, opt = 0.1405
> progress 0.0%, elapsed 18 s, estimated 18033093 s (208 days), iters = {DTMC: 2046}, opt = 0.1405
> progress 0.0%, elapsed 21 s, estimated 21040945 s (243 days), iters = {DTMC: 2392}, opt = 0.1405
> progress 0.0%, elapsed 24 s, estimated 24047558 s (278 days), iters = {DTMC: 2731}, opt = 0.1405
> progress 0.0%, elapsed 27 s, estimated 27050276 s (313 days), iters = {DTMC: 3077}, opt = 0.1405
> progress 0.0%, elapsed 30 s, estimated 30055366 s (347 days), iters = {DTMC: 3408}, opt = 0.1405
> progress 0.0%, elapsed 33 s, estimated 33063852 s (1 year), iters = {DTMC: 3734}, opt = 0.1405
> progress 0.0%, elapsed 36 s, estimated 36065331 s (1 year), iters = {DTMC: 4078}, opt = 0.1405
> progress 0.0%, elapsed 39 s, estimated 39073469 s (1 year), iters = {DTMC: 4419}, opt = 0.1405
> progress 0.0%, elapsed 42 s, estimated 42075431 s (1 year), iters = {DTMC: 4758}, opt = 0.1405
> progress 0.0%, elapsed 45 s, estimated 45079891 s (1 year), iters = {DTMC: 5098}, opt = 0.1405
> progress 0.0%, elapsed 48 s, estimated 48083943 s (1 year), iters = {DTMC: 5439}, opt = 0.1405
> progress 0.0%, elapsed 51 s, estimated 51085980 s (1 year), iters = {DTMC: 5780}, opt = 0.1405
> progress 0.0%, elapsed 54 s, estimated 54087911 s (1 year), iters = {DTMC: 6120}, opt = 0.1405
> progress 0.0%, elapsed 57 s, estimated 57088325 s (1 year), iters = {DTMC: 6452}, opt = 0.1405
2025-03-24 18:08:20,628 - synthesizer_onebyone.py:19 - Time limit reached
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.01 s
number of holes: 1000, family size: 1e609, quotient: 33898 states / 617106 actions
explored: 0 %
DTMC stats: avg DTMC size: 212, iterations: 6749

optimum: 0.140482
--------------------
2025-03-24 18:08:20,629 - synthesizer_rl_storm_paynt.py:280 - No improving assignment found.
2025-03-24 18:08:20,630 - storm_pomdp_control.py:246 - Interactive Storm resumed
2025-03-24 18:08:20,630 - storm_pomdp_control.py:291 - Updating FSC values in Storm
2025-03-24 18:08:51,661 - storm_pomdp_control.py:305 - Pausing Storm
-----------Storm-----------               
Value = 0.23461992224252154 | Time elapsed = 10872.0s | FSC size = 1636

2025-03-24 18:11:43,170 - synthesizer_pomdp.py:253 - Timeout for PAYNT started
2025-03-24 18:11:43,792 - synthesizer_ar_storm.py:136 - Resuming synthesis
2025-03-24 18:11:43,804 - synthesizer_ar_storm.py:147 - PAYNT's value is better. Prioritizing synthesis results
2025-03-24 18:12:13,353 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 18:12:13,428 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:12:25,039 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:12:25,039 - father_agent.py:564 - Average Virtual Goal Value = 0.6673060059547424
2025-03-24 18:12:25,039 - father_agent.py:566 - Goal Reach Probability = 0.01334612003516343
2025-03-24 18:12:25,039 - father_agent.py:568 - Trap Reach Probability = 0.9866538799648366
2025-03-24 18:12:25,039 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:12:25,039 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:12:25,039 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 18:12:31,336 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:12:39,052 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:12:39,052 - evaluators.py:130 - Average Virtual Goal Value = 0.031935278326272964
2025-03-24 18:12:39,052 - evaluators.py:132 - Goal Reach Probability = 0.0006387055567383436
2025-03-24 18:12:39,052 - evaluators.py:134 - Trap Reach Probability = 0.9993612944432616
2025-03-24 18:12:39,052 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:12:39,052 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:12:39,052 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:12:39,052 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 1.7667275667190552
Epoch: 0, Critic Loss: 6.826574802398682
Epoch: 10, Actor Loss: 0.17408409714698792
Epoch: 10, Critic Loss: 17.604095458984375
Epoch: 20, Actor Loss: 0.1133347824215889
Epoch: 20, Critic Loss: 15.084495544433594
Epoch: 30, Actor Loss: 0.08845964074134827
Epoch: 30, Critic Loss: 13.533598899841309
Epoch: 40, Actor Loss: 0.07340168952941895
Epoch: 40, Critic Loss: 21.484329223632812
2025-03-24 18:13:13,052 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:13:16,638 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:13:16,638 - evaluators.py:130 - Average Virtual Goal Value = 6.858847141265869
2025-03-24 18:13:16,638 - evaluators.py:132 - Goal Reach Probability = 0.13717693836978131
2025-03-24 18:13:16,638 - evaluators.py:134 - Trap Reach Probability = 0.8628230616302187
2025-03-24 18:13:16,638 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:13:16,638 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:13:16,639 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:13:16,639 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.05849943682551384
Epoch: 50, Critic Loss: 12.926685333251953
Epoch: 60, Actor Loss: 0.039946164935827255
Epoch: 60, Critic Loss: 18.156808853149414
Epoch: 70, Actor Loss: 0.0480986163020134
Epoch: 70, Critic Loss: 11.97940731048584
Epoch: 80, Actor Loss: 0.05307278782129288
Epoch: 80, Critic Loss: 6.833810806274414
Epoch: 90, Actor Loss: 0.038382064551115036
Epoch: 90, Critic Loss: 9.55186653137207
2025-03-24 18:13:50,135 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:13:53,528 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:13:53,528 - evaluators.py:130 - Average Virtual Goal Value = 9.847923278808594
2025-03-24 18:13:53,528 - evaluators.py:132 - Goal Reach Probability = 0.19695845697329376
2025-03-24 18:13:53,528 - evaluators.py:134 - Trap Reach Probability = 0.8030415430267063
2025-03-24 18:13:53,528 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:13:53,528 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:13:53,528 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:13:53,528 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.03155995532870293
Epoch: 100, Critic Loss: 11.592679977416992
Epoch: 110, Actor Loss: 0.02163856476545334
Epoch: 110, Critic Loss: 15.618902206420898
Epoch: 120, Actor Loss: 0.032334331423044205
Epoch: 120, Critic Loss: 11.608442306518555
Epoch: 130, Actor Loss: 0.026292672380805016
Epoch: 130, Critic Loss: 8.67296314239502
Epoch: 140, Actor Loss: 0.024599451571702957
Epoch: 140, Critic Loss: 12.145079612731934
2025-03-24 18:14:26,877 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:14:31,061 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:14:31,062 - evaluators.py:130 - Average Virtual Goal Value = 10.141159057617188
2025-03-24 18:14:31,062 - evaluators.py:132 - Goal Reach Probability = 0.20282317979197623
2025-03-24 18:14:31,062 - evaluators.py:134 - Trap Reach Probability = 0.7971768202080238
2025-03-24 18:14:31,062 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:14:31,062 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:14:31,062 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:14:31,062 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.04246295988559723
Epoch: 150, Critic Loss: 13.204305648803711
Epoch: 160, Actor Loss: 0.0307358019053936
Epoch: 160, Critic Loss: 9.37896728515625
Epoch: 170, Actor Loss: 0.024576181545853615
Epoch: 170, Critic Loss: 5.331021308898926
Epoch: 180, Actor Loss: 0.02769939973950386
Epoch: 180, Critic Loss: 12.238738059997559
Epoch: 190, Actor Loss: 0.035068124532699585
Epoch: 190, Critic Loss: 5.236968040466309
2025-03-24 18:15:04,569 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:15:08,091 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:15:08,091 - evaluators.py:130 - Average Virtual Goal Value = 9.287297248840332
2025-03-24 18:15:08,091 - evaluators.py:132 - Goal Reach Probability = 0.18574593796159528
2025-03-24 18:15:08,091 - evaluators.py:134 - Trap Reach Probability = 0.8142540620384048
2025-03-24 18:15:08,091 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:15:08,091 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:15:08,091 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:15:08,091 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.028114069253206253
Epoch: 200, Critic Loss: 12.110380172729492
Epoch: 210, Actor Loss: 0.03498407453298569
Epoch: 210, Critic Loss: 12.599349975585938
Epoch: 220, Actor Loss: 0.03154558315873146
Epoch: 220, Critic Loss: 7.002286434173584
Epoch: 230, Actor Loss: 0.02669723518192768
Epoch: 230, Critic Loss: 10.602677345275879
Epoch: 240, Actor Loss: 0.03314794600009918
Epoch: 240, Critic Loss: 9.692293167114258
2025-03-24 18:15:41,032 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:15:44,410 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:15:44,410 - evaluators.py:130 - Average Virtual Goal Value = 9.94444465637207
2025-03-24 18:15:44,410 - evaluators.py:132 - Goal Reach Probability = 0.1988888888888889
2025-03-24 18:15:44,410 - evaluators.py:134 - Trap Reach Probability = 0.8011111111111111
2025-03-24 18:15:44,410 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:15:44,410 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:15:44,410 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:15:44,410 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.02438010647892952
Epoch: 250, Critic Loss: 7.596735954284668
Epoch: 260, Actor Loss: 0.01835954748094082
Epoch: 260, Critic Loss: 5.383468151092529
Epoch: 270, Actor Loss: 0.023502908647060394
Epoch: 270, Critic Loss: 10.673234939575195
Epoch: 280, Actor Loss: 0.028484053909778595
Epoch: 280, Critic Loss: 8.910547256469727
Epoch: 290, Actor Loss: 0.024454321712255478
Epoch: 290, Critic Loss: 11.220637321472168
2025-03-24 18:16:17,559 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:16:20,894 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:16:20,894 - evaluators.py:130 - Average Virtual Goal Value = 9.399333000183105
2025-03-24 18:16:20,894 - evaluators.py:132 - Goal Reach Probability = 0.18798665183537264
2025-03-24 18:16:20,894 - evaluators.py:134 - Trap Reach Probability = 0.8120133481646273
2025-03-24 18:16:20,894 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:16:20,894 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:16:20,894 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:16:20,894 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.024946143850684166
Epoch: 300, Critic Loss: 8.119156837463379
Epoch: 310, Actor Loss: 0.027349261566996574
Epoch: 310, Critic Loss: 10.450645446777344
Epoch: 320, Actor Loss: 0.030064336955547333
Epoch: 320, Critic Loss: 13.365452766418457
Epoch: 330, Actor Loss: 0.027926214039325714
Epoch: 330, Critic Loss: 11.375617980957031
Epoch: 340, Actor Loss: 0.0195214431732893
Epoch: 340, Critic Loss: 5.543165683746338
2025-03-24 18:16:54,508 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:16:57,756 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:16:57,757 - evaluators.py:130 - Average Virtual Goal Value = 9.158679008483887
2025-03-24 18:16:57,757 - evaluators.py:132 - Goal Reach Probability = 0.18317358892438765
2025-03-24 18:16:57,757 - evaluators.py:134 - Trap Reach Probability = 0.8168264110756124
2025-03-24 18:16:57,757 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:16:57,757 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:16:57,757 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:16:57,757 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.025908494368195534
Epoch: 350, Critic Loss: 6.550073146820068
Epoch: 360, Actor Loss: 0.03125039115548134
Epoch: 360, Critic Loss: 10.363910675048828
Epoch: 370, Actor Loss: 0.028951041400432587
Epoch: 370, Critic Loss: 14.990052223205566
Epoch: 380, Actor Loss: 0.04227980598807335
Epoch: 380, Critic Loss: 13.794779777526855
Epoch: 390, Actor Loss: 0.03436673805117607
Epoch: 390, Critic Loss: 12.478583335876465
2025-03-24 18:17:31,887 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:17:35,686 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:17:35,686 - evaluators.py:130 - Average Virtual Goal Value = 9.851024627685547
2025-03-24 18:17:35,686 - evaluators.py:132 - Goal Reach Probability = 0.19702048417132215
2025-03-24 18:17:35,686 - evaluators.py:134 - Trap Reach Probability = 0.8029795158286779
2025-03-24 18:17:35,686 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:17:35,686 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:17:35,686 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:17:35,686 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.02990514226257801
Epoch: 400, Critic Loss: 14.634750366210938
Epoch: 410, Actor Loss: 0.036398883908987045
Epoch: 410, Critic Loss: 12.40806770324707
Epoch: 420, Actor Loss: 0.0186427254229784
Epoch: 420, Critic Loss: 9.657021522521973
Epoch: 430, Actor Loss: 0.018511317670345306
Epoch: 430, Critic Loss: 8.310469627380371
Epoch: 440, Actor Loss: 0.028724713250994682
Epoch: 440, Critic Loss: 12.178716659545898
2025-03-24 18:18:08,812 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:18:12,122 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:18:12,122 - evaluators.py:130 - Average Virtual Goal Value = 10.551151275634766
2025-03-24 18:18:12,122 - evaluators.py:132 - Goal Reach Probability = 0.21102302755756888
2025-03-24 18:18:12,122 - evaluators.py:134 - Trap Reach Probability = 0.7889769724424311
2025-03-24 18:18:12,122 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:18:12,122 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:18:12,122 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:18:12,122 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.01732647977769375
Epoch: 450, Critic Loss: 7.661573886871338
Epoch: 460, Actor Loss: 0.014611562713980675
Epoch: 460, Critic Loss: 13.997241020202637
Epoch: 470, Actor Loss: 0.028493380174040794
Epoch: 470, Critic Loss: 10.899271011352539
Epoch: 480, Actor Loss: 0.027252770960330963
Epoch: 480, Critic Loss: 14.508091926574707
Epoch: 490, Actor Loss: 0.00729990703985095
Epoch: 490, Critic Loss: 10.25201416015625
2025-03-24 18:18:46,532 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:18:49,967 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:18:49,967 - evaluators.py:130 - Average Virtual Goal Value = 9.114583015441895
2025-03-24 18:18:49,968 - evaluators.py:132 - Goal Reach Probability = 0.18229166666666666
2025-03-24 18:18:49,968 - evaluators.py:134 - Trap Reach Probability = 0.8177083333333334
2025-03-24 18:18:49,968 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:18:49,968 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:18:49,968 - evaluators.py:140 - Current Best Reach Probability = 0.2204694113120431
2025-03-24 18:18:49,968 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.014532148838043213
Epoch: 500, Critic Loss: 13.58107852935791
Epoch: 510, Actor Loss: 0.027763670310378075
Epoch: 510, Critic Loss: 11.918664932250977
Epoch: 520, Actor Loss: 0.026246240362524986
Epoch: 520, Critic Loss: 10.015992164611816
Epoch: 530, Actor Loss: 0.03637045994400978
Epoch: 530, Critic Loss: 17.802425384521484
Epoch: 540, Actor Loss: 0.020069792866706848
Epoch: 540, Critic Loss: 11.872387886047363
2025-03-24 18:19:23,681 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:19:27,068 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:19:27,068 - evaluators.py:130 - Average Virtual Goal Value = 11.168384552001953
2025-03-24 18:19:27,068 - evaluators.py:132 - Goal Reach Probability = 0.22336769759450173
2025-03-24 18:19:27,068 - evaluators.py:134 - Trap Reach Probability = 0.7766323024054983
2025-03-24 18:19:27,068 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:19:27,068 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:19:27,068 - evaluators.py:140 - Current Best Reach Probability = 0.22336769759450173
2025-03-24 18:19:27,068 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.016570603474974632
Epoch: 550, Critic Loss: 8.615151405334473
Epoch: 560, Actor Loss: 0.036172378808259964
Epoch: 560, Critic Loss: 14.053828239440918
Epoch: 570, Actor Loss: 0.02423599734902382
Epoch: 570, Critic Loss: 11.255622863769531
Epoch: 580, Actor Loss: 0.023152055218815804
Epoch: 580, Critic Loss: 13.015658378601074
Epoch: 590, Actor Loss: 0.02459520846605301
Epoch: 590, Critic Loss: 9.530614852905273
2025-03-24 18:20:00,508 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:20:03,837 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:20:03,837 - evaluators.py:130 - Average Virtual Goal Value = 9.865721702575684
2025-03-24 18:20:03,837 - evaluators.py:132 - Goal Reach Probability = 0.19731443491234613
2025-03-24 18:20:03,837 - evaluators.py:134 - Trap Reach Probability = 0.8026855650876539
2025-03-24 18:20:03,837 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:20:03,837 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:20:03,837 - evaluators.py:140 - Current Best Reach Probability = 0.22336769759450173
2025-03-24 18:20:03,837 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.01125757209956646
Epoch: 600, Critic Loss: 11.619483947753906
Epoch: 610, Actor Loss: 0.012060945853590965
Epoch: 610, Critic Loss: 12.149694442749023
Epoch: 620, Actor Loss: 0.025970101356506348
Epoch: 620, Critic Loss: 6.194749355316162
Epoch: 630, Actor Loss: 0.010076646693050861
Epoch: 630, Critic Loss: 7.482658863067627
Epoch: 640, Actor Loss: 0.015472186729311943
Epoch: 640, Critic Loss: 13.595598220825195
2025-03-24 18:20:36,678 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:20:39,830 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:20:39,830 - evaluators.py:130 - Average Virtual Goal Value = 10.119715690612793
2025-03-24 18:20:39,830 - evaluators.py:132 - Goal Reach Probability = 0.2023943135054246
2025-03-24 18:20:39,830 - evaluators.py:134 - Trap Reach Probability = 0.7976056864945754
2025-03-24 18:20:39,830 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:20:39,830 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:20:39,830 - evaluators.py:140 - Current Best Reach Probability = 0.22336769759450173
2025-03-24 18:20:39,830 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.031974539160728455
Epoch: 650, Critic Loss: 9.487615585327148
Epoch: 660, Actor Loss: 0.027265802025794983
Epoch: 660, Critic Loss: 6.1016130447387695
Epoch: 670, Actor Loss: 0.02235439047217369
Epoch: 670, Critic Loss: 11.071252822875977
Epoch: 680, Actor Loss: 0.020833365619182587
Epoch: 680, Critic Loss: 11.458824157714844
Epoch: 690, Actor Loss: 0.03632715716958046
Epoch: 690, Critic Loss: 14.685998916625977
2025-03-24 18:21:13,379 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:21:17,363 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:21:17,363 - evaluators.py:130 - Average Virtual Goal Value = 10.792612075805664
2025-03-24 18:21:17,363 - evaluators.py:132 - Goal Reach Probability = 0.21585225086571758
2025-03-24 18:21:17,363 - evaluators.py:134 - Trap Reach Probability = 0.7841477491342824
2025-03-24 18:21:17,363 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:21:17,363 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:21:17,363 - evaluators.py:140 - Current Best Reach Probability = 0.22336769759450173
2025-03-24 18:21:17,363 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.03353364020586014
Epoch: 700, Critic Loss: 5.14869499206543
2025-03-24 18:21:17,365 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 18:21:17,365 - father_agent.py:449 - Before training evaluation.
2025-03-24 18:21:17,365 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:21:24,636 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:21:24,636 - father_agent.py:564 - Average Virtual Goal Value = 10.883848190307617
2025-03-24 18:21:24,636 - father_agent.py:566 - Goal Reach Probability = 0.21767695549770671
2025-03-24 18:21:24,636 - father_agent.py:568 - Trap Reach Probability = 0.7823230445022933
2025-03-24 18:21:24,636 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:21:24,636 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:21:24,636 - father_agent.py:574 - Current Best Reach Probability = 0.2206793949913216
2025-03-24 18:21:24,682 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:21:24,805 - father_agent.py:359 - Training agent on-policy
2025-03-24 18:21:28,937 - father_agent.py:306 - Step: 0, Training loss: 89.3046875
2025-03-24 18:21:28,990 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:21:37,891 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:21:37,892 - father_agent.py:564 - Average Virtual Goal Value = 11.303857803344727
2025-03-24 18:21:37,892 - father_agent.py:566 - Goal Reach Probability = 0.22607715430861725
2025-03-24 18:21:37,892 - father_agent.py:568 - Trap Reach Probability = 0.7739228456913828
2025-03-24 18:21:37,892 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:21:37,892 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:21:37,892 - father_agent.py:574 - Current Best Reach Probability = 0.22607715430861725
2025-03-24 18:21:38,515 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:21:40,489 - father_agent.py:306 - Step: 5, Training loss: 70.90969848632812
2025-03-24 18:21:41,625 - father_agent.py:306 - Step: 10, Training loss: 44.83765411376953
2025-03-24 18:21:42,935 - father_agent.py:306 - Step: 15, Training loss: 24.343900680541992
2025-03-24 18:21:44,098 - father_agent.py:306 - Step: 20, Training loss: 16.77167510986328
2025-03-24 18:21:45,244 - father_agent.py:306 - Step: 25, Training loss: 7.478183746337891
2025-03-24 18:21:46,528 - father_agent.py:306 - Step: 30, Training loss: 4.5408711433410645
2025-03-24 18:21:47,791 - father_agent.py:306 - Step: 35, Training loss: 2.4631106853485107
2025-03-24 18:21:48,939 - father_agent.py:306 - Step: 40, Training loss: 1.584408164024353
2025-03-24 18:21:50,147 - father_agent.py:306 - Step: 45, Training loss: 1.6589645147323608
2025-03-24 18:21:55,363 - father_agent.py:306 - Step: 50, Training loss: 0.6155781745910645
2025-03-24 18:21:56,674 - father_agent.py:306 - Step: 55, Training loss: 0.7388137578964233
2025-03-24 18:21:58,019 - father_agent.py:306 - Step: 60, Training loss: 0.2383480668067932
2025-03-24 18:21:59,421 - father_agent.py:306 - Step: 65, Training loss: 0.18633005023002625
2025-03-24 18:22:00,799 - father_agent.py:306 - Step: 70, Training loss: 0.18426555395126343
2025-03-24 18:22:02,162 - father_agent.py:306 - Step: 75, Training loss: 0.368903785943985
2025-03-24 18:22:03,479 - father_agent.py:306 - Step: 80, Training loss: 0.3826510012149811
2025-03-24 18:22:04,926 - father_agent.py:306 - Step: 85, Training loss: 0.28071045875549316
2025-03-24 18:22:06,340 - father_agent.py:306 - Step: 90, Training loss: 0.15851850807666779
2025-03-24 18:22:07,678 - father_agent.py:306 - Step: 95, Training loss: 0.14016561210155487
2025-03-24 18:22:09,182 - father_agent.py:306 - Step: 100, Training loss: 0.1480296552181244
2025-03-24 18:22:09,229 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:22:18,982 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:22:18,982 - father_agent.py:564 - Average Virtual Goal Value = 0.009796238504350185
2025-03-24 18:22:18,982 - father_agent.py:566 - Goal Reach Probability = 0.00019592476489028212
2025-03-24 18:22:18,982 - father_agent.py:568 - Trap Reach Probability = 0.9998040752351097
2025-03-24 18:22:18,982 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:22:18,982 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:22:18,982 - father_agent.py:574 - Current Best Reach Probability = 0.22607715430861725
2025-03-24 18:22:19,028 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:22:21,191 - father_agent.py:306 - Step: 105, Training loss: 0.13054147362709045
2025-03-24 18:22:22,514 - father_agent.py:306 - Step: 110, Training loss: 0.5897617340087891
2025-03-24 18:22:23,802 - father_agent.py:306 - Step: 115, Training loss: 0.7113388776779175
2025-03-24 18:22:25,124 - father_agent.py:306 - Step: 120, Training loss: 0.14668431878089905
2025-03-24 18:22:26,542 - father_agent.py:306 - Step: 125, Training loss: 0.3444891571998596
2025-03-24 18:22:27,841 - father_agent.py:306 - Step: 130, Training loss: 0.2740658223628998
2025-03-24 18:22:29,288 - father_agent.py:306 - Step: 135, Training loss: 0.30228859186172485
2025-03-24 18:22:30,693 - father_agent.py:306 - Step: 140, Training loss: 0.3577660918235779
2025-03-24 18:22:32,053 - father_agent.py:306 - Step: 145, Training loss: 0.48366454243659973
2025-03-24 18:22:33,448 - father_agent.py:306 - Step: 150, Training loss: 0.15151460468769073
2025-03-24 18:22:34,702 - father_agent.py:306 - Step: 155, Training loss: 0.2982807159423828
2025-03-24 18:22:36,103 - father_agent.py:306 - Step: 160, Training loss: 0.4359132647514343
2025-03-24 18:22:37,496 - father_agent.py:306 - Step: 165, Training loss: 0.19594794511795044
2025-03-24 18:22:38,787 - father_agent.py:306 - Step: 170, Training loss: 0.1299876868724823
2025-03-24 18:22:40,257 - father_agent.py:306 - Step: 175, Training loss: 0.1603296995162964
2025-03-24 18:22:41,590 - father_agent.py:306 - Step: 180, Training loss: 0.1334347426891327
2025-03-24 18:22:42,879 - father_agent.py:306 - Step: 185, Training loss: 0.2384764850139618
2025-03-24 18:22:44,461 - father_agent.py:306 - Step: 190, Training loss: 0.16138780117034912
2025-03-24 18:22:46,272 - father_agent.py:306 - Step: 195, Training loss: 0.12457501143217087
2025-03-24 18:22:47,998 - father_agent.py:306 - Step: 200, Training loss: 0.551405668258667
2025-03-24 18:22:48,533 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:22:57,459 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:22:57,459 - father_agent.py:564 - Average Virtual Goal Value = 2.5280466079711914
2025-03-24 18:22:57,459 - father_agent.py:566 - Goal Reach Probability = 0.05056093437836177
2025-03-24 18:22:57,459 - father_agent.py:568 - Trap Reach Probability = 0.9494390656216383
2025-03-24 18:22:57,459 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:22:57,459 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:22:57,459 - father_agent.py:574 - Current Best Reach Probability = 0.22607715430861725
2025-03-24 18:22:57,507 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:22:59,924 - father_agent.py:306 - Step: 205, Training loss: 0.6538956761360168
2025-03-24 18:23:01,215 - father_agent.py:306 - Step: 210, Training loss: 0.11000873148441315
2025-03-24 18:23:02,654 - father_agent.py:306 - Step: 215, Training loss: 0.46802109479904175
2025-03-24 18:23:03,885 - father_agent.py:306 - Step: 220, Training loss: 0.6498687267303467
2025-03-24 18:23:05,164 - father_agent.py:306 - Step: 225, Training loss: 0.17428576946258545
2025-03-24 18:23:06,602 - father_agent.py:306 - Step: 230, Training loss: 0.27653968334198
2025-03-24 18:23:07,932 - father_agent.py:306 - Step: 235, Training loss: 0.14325033128261566
2025-03-24 18:23:09,196 - father_agent.py:306 - Step: 240, Training loss: 0.3925558924674988
2025-03-24 18:23:10,632 - father_agent.py:306 - Step: 245, Training loss: 0.7701114416122437
2025-03-24 18:23:11,936 - father_agent.py:306 - Step: 250, Training loss: 0.12872111797332764
2025-03-24 18:23:13,204 - father_agent.py:306 - Step: 255, Training loss: 0.7455905079841614
2025-03-24 18:23:14,674 - father_agent.py:306 - Step: 260, Training loss: 0.3235839903354645
2025-03-24 18:23:15,969 - father_agent.py:306 - Step: 265, Training loss: 0.32722243666648865
2025-03-24 18:23:17,232 - father_agent.py:306 - Step: 270, Training loss: 0.3786182403564453
2025-03-24 18:23:18,732 - father_agent.py:306 - Step: 275, Training loss: 0.6315149068832397
2025-03-24 18:23:20,329 - father_agent.py:306 - Step: 280, Training loss: 0.8665227890014648
2025-03-24 18:23:22,040 - father_agent.py:306 - Step: 285, Training loss: 0.866934597492218
2025-03-24 18:23:23,262 - father_agent.py:306 - Step: 290, Training loss: 1.0664374828338623
2025-03-24 18:23:24,640 - father_agent.py:306 - Step: 295, Training loss: 0.7253633737564087
2025-03-24 18:23:25,882 - father_agent.py:306 - Step: 300, Training loss: 0.9525759816169739
2025-03-24 18:23:25,935 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:23:34,978 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:23:34,978 - father_agent.py:564 - Average Virtual Goal Value = 3.207674503326416
2025-03-24 18:23:34,978 - father_agent.py:566 - Goal Reach Probability = 0.06415348872067751
2025-03-24 18:23:34,978 - father_agent.py:568 - Trap Reach Probability = 0.9358465112793225
2025-03-24 18:23:34,978 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:23:34,978 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:23:34,978 - father_agent.py:574 - Current Best Reach Probability = 0.22607715430861725
2025-03-24 18:23:35,025 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:23:37,165 - father_agent.py:306 - Step: 305, Training loss: 1.3233425617218018
2025-03-24 18:23:38,476 - father_agent.py:306 - Step: 310, Training loss: 1.2966173887252808
2025-03-24 18:23:39,680 - father_agent.py:306 - Step: 315, Training loss: 1.133966326713562
2025-03-24 18:23:40,982 - father_agent.py:306 - Step: 320, Training loss: 0.9658015966415405
2025-03-24 18:23:42,242 - father_agent.py:306 - Step: 325, Training loss: 0.989296019077301
2025-03-24 18:23:43,546 - father_agent.py:306 - Step: 330, Training loss: 0.7686996459960938
2025-03-24 18:23:44,835 - father_agent.py:306 - Step: 335, Training loss: 1.4346760511398315
2025-03-24 18:23:46,122 - father_agent.py:306 - Step: 340, Training loss: 0.877239465713501
2025-03-24 18:23:47,392 - father_agent.py:306 - Step: 345, Training loss: 1.0921084880828857
2025-03-24 18:23:48,786 - father_agent.py:306 - Step: 350, Training loss: 1.4584155082702637
2025-03-24 18:23:49,993 - father_agent.py:306 - Step: 355, Training loss: 0.9885104298591614
2025-03-24 18:23:51,338 - father_agent.py:306 - Step: 360, Training loss: 1.3263412714004517
2025-03-24 18:23:52,592 - father_agent.py:306 - Step: 365, Training loss: 1.037980556488037
2025-03-24 18:23:53,772 - father_agent.py:306 - Step: 370, Training loss: 1.5480852127075195
2025-03-24 18:23:55,138 - father_agent.py:306 - Step: 375, Training loss: 1.2373323440551758
2025-03-24 18:23:56,419 - father_agent.py:306 - Step: 380, Training loss: 0.8056412935256958
2025-03-24 18:23:57,621 - father_agent.py:306 - Step: 385, Training loss: 1.1158015727996826
2025-03-24 18:23:59,001 - father_agent.py:306 - Step: 390, Training loss: 0.8028421401977539
2025-03-24 18:24:00,289 - father_agent.py:306 - Step: 395, Training loss: 1.248151183128357
2025-03-24 18:24:01,486 - father_agent.py:306 - Step: 400, Training loss: 1.1759893894195557
2025-03-24 18:24:01,536 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:24:10,476 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:24:10,476 - father_agent.py:564 - Average Virtual Goal Value = 4.018168926239014
2025-03-24 18:24:10,476 - father_agent.py:566 - Goal Reach Probability = 0.0803633822501747
2025-03-24 18:24:10,476 - father_agent.py:568 - Trap Reach Probability = 0.9196366177498253
2025-03-24 18:24:10,476 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:24:10,477 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:24:10,477 - father_agent.py:574 - Current Best Reach Probability = 0.22607715430861725
2025-03-24 18:24:10,523 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:24:12,606 - father_agent.py:306 - Step: 405, Training loss: 1.3267333507537842
2025-03-24 18:24:13,740 - father_agent.py:306 - Step: 410, Training loss: 1.3308994770050049
2025-03-24 18:24:14,900 - father_agent.py:306 - Step: 415, Training loss: 1.251017451286316
2025-03-24 18:24:16,220 - father_agent.py:306 - Step: 420, Training loss: 1.5001459121704102
2025-03-24 18:24:17,489 - father_agent.py:306 - Step: 425, Training loss: 1.0105067491531372
2025-03-24 18:24:18,663 - father_agent.py:306 - Step: 430, Training loss: 1.3681963682174683
2025-03-24 18:24:19,902 - father_agent.py:306 - Step: 435, Training loss: 1.3303134441375732
2025-03-24 18:24:21,231 - father_agent.py:306 - Step: 440, Training loss: 1.5024348497390747
2025-03-24 18:24:22,442 - father_agent.py:306 - Step: 445, Training loss: 1.1984739303588867
2025-03-24 18:24:23,729 - father_agent.py:306 - Step: 450, Training loss: 1.200435996055603
2025-03-24 18:24:25,138 - father_agent.py:306 - Step: 455, Training loss: 1.855402946472168
2025-03-24 18:24:26,280 - father_agent.py:306 - Step: 460, Training loss: 1.3849149942398071
2025-03-24 18:24:27,536 - father_agent.py:306 - Step: 465, Training loss: 1.2375142574310303
2025-03-24 18:24:28,875 - father_agent.py:306 - Step: 470, Training loss: 1.4620680809020996
2025-03-24 18:24:30,062 - father_agent.py:306 - Step: 475, Training loss: 0.993401050567627
2025-03-24 18:24:31,331 - father_agent.py:306 - Step: 480, Training loss: 1.2764530181884766
2025-03-24 18:24:32,701 - father_agent.py:306 - Step: 485, Training loss: 1.5025746822357178
2025-03-24 18:24:33,873 - father_agent.py:306 - Step: 490, Training loss: 1.3448750972747803
2025-03-24 18:24:35,098 - father_agent.py:306 - Step: 495, Training loss: 1.5136895179748535
2025-03-24 18:24:36,423 - father_agent.py:306 - Step: 500, Training loss: 1.3662314414978027
2025-03-24 18:24:36,565 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:24:45,352 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:24:45,353 - father_agent.py:564 - Average Virtual Goal Value = 4.02195930480957
2025-03-24 18:24:45,353 - father_agent.py:566 - Goal Reach Probability = 0.0804391839277371
2025-03-24 18:24:45,353 - father_agent.py:568 - Trap Reach Probability = 0.9195608160722629
2025-03-24 18:24:45,353 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:24:45,353 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:24:45,353 - father_agent.py:574 - Current Best Reach Probability = 0.22607715430861725
2025-03-24 18:24:45,406 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:24:47,365 - father_agent.py:306 - Step: 505, Training loss: 1.1938905715942383
2025-03-24 18:24:48,733 - father_agent.py:306 - Step: 510, Training loss: 1.6264716386795044
2025-03-24 18:24:50,404 - father_agent.py:306 - Step: 515, Training loss: 1.1048105955123901
2025-03-24 18:24:51,854 - father_agent.py:306 - Step: 520, Training loss: 1.020507574081421
2025-03-24 18:24:53,193 - father_agent.py:306 - Step: 525, Training loss: 0.9438459277153015
2025-03-24 18:24:54,433 - father_agent.py:306 - Step: 530, Training loss: 1.308967113494873
2025-03-24 18:24:55,647 - father_agent.py:306 - Step: 535, Training loss: 1.08610200881958
2025-03-24 18:24:56,977 - father_agent.py:306 - Step: 540, Training loss: 1.4133609533309937
2025-03-24 18:24:58,219 - father_agent.py:306 - Step: 545, Training loss: 1.1945953369140625
2025-03-24 18:24:59,450 - father_agent.py:306 - Step: 550, Training loss: 1.0928232669830322
2025-03-24 18:25:00,786 - father_agent.py:306 - Step: 555, Training loss: 1.516808271408081
2025-03-24 18:25:02,028 - father_agent.py:306 - Step: 560, Training loss: 1.2606767416000366
2025-03-24 18:25:03,246 - father_agent.py:306 - Step: 565, Training loss: 1.167691707611084
2025-03-24 18:25:04,569 - father_agent.py:306 - Step: 570, Training loss: 1.1588250398635864
2025-03-24 18:25:05,745 - father_agent.py:306 - Step: 575, Training loss: 1.1431275606155396
2025-03-24 18:25:07,127 - father_agent.py:306 - Step: 580, Training loss: 1.0448508262634277
2025-03-24 18:25:08,302 - father_agent.py:306 - Step: 585, Training loss: 0.914028525352478
2025-03-24 18:25:09,505 - father_agent.py:306 - Step: 590, Training loss: 1.490148663520813
2025-03-24 18:25:10,877 - father_agent.py:306 - Step: 595, Training loss: 0.9835020303726196
2025-03-24 18:25:12,067 - father_agent.py:306 - Step: 600, Training loss: 0.9701499342918396
2025-03-24 18:25:12,114 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:25:20,683 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:25:20,683 - father_agent.py:564 - Average Virtual Goal Value = 4.727989673614502
2025-03-24 18:25:20,684 - father_agent.py:566 - Goal Reach Probability = 0.09455979398036375
2025-03-24 18:25:20,684 - father_agent.py:568 - Trap Reach Probability = 0.9054402060196363
2025-03-24 18:25:20,684 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:25:20,684 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:25:20,684 - father_agent.py:574 - Current Best Reach Probability = 0.22607715430861725
2025-03-24 18:25:20,733 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:25:23,444 - father_agent.py:306 - Step: 605, Training loss: 1.1989823579788208
2025-03-24 18:25:25,246 - father_agent.py:306 - Step: 610, Training loss: 1.2206690311431885
2025-03-24 18:25:26,578 - father_agent.py:306 - Step: 615, Training loss: 1.0385679006576538
2025-03-24 18:25:27,755 - father_agent.py:306 - Step: 620, Training loss: 1.156759262084961
2025-03-24 18:25:29,099 - father_agent.py:306 - Step: 625, Training loss: 1.0583046674728394
2025-03-24 18:25:30,388 - father_agent.py:306 - Step: 630, Training loss: 1.1595088243484497
2025-03-24 18:25:31,562 - father_agent.py:306 - Step: 635, Training loss: 1.1491754055023193
2025-03-24 18:25:32,925 - father_agent.py:306 - Step: 640, Training loss: 0.8399184942245483
2025-03-24 18:25:34,333 - father_agent.py:306 - Step: 645, Training loss: 0.7694630026817322
2025-03-24 18:25:35,570 - father_agent.py:306 - Step: 650, Training loss: 1.0997315645217896
2025-03-24 18:25:36,817 - father_agent.py:306 - Step: 655, Training loss: 0.7598124742507935
2025-03-24 18:25:38,190 - father_agent.py:306 - Step: 660, Training loss: 0.6490117907524109
2025-03-24 18:25:39,480 - father_agent.py:306 - Step: 665, Training loss: 0.9283762574195862
2025-03-24 18:25:40,724 - father_agent.py:306 - Step: 670, Training loss: 0.6837517023086548
2025-03-24 18:25:42,065 - father_agent.py:306 - Step: 675, Training loss: 0.5815048813819885
2025-03-24 18:25:43,400 - father_agent.py:306 - Step: 680, Training loss: 0.8779907822608948
2025-03-24 18:25:44,625 - father_agent.py:306 - Step: 685, Training loss: 0.6674581170082092
2025-03-24 18:25:45,940 - father_agent.py:306 - Step: 690, Training loss: 0.34628021717071533
2025-03-24 18:25:47,315 - father_agent.py:306 - Step: 695, Training loss: 0.5125976800918579
2025-03-24 18:25:48,593 - father_agent.py:306 - Step: 700, Training loss: 0.39958077669143677
2025-03-24 18:25:48,642 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:25:57,702 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:25:57,702 - father_agent.py:564 - Average Virtual Goal Value = 1.8293614387512207
2025-03-24 18:25:57,702 - father_agent.py:566 - Goal Reach Probability = 0.03658722884204094
2025-03-24 18:25:57,702 - father_agent.py:568 - Trap Reach Probability = 0.9634127711579591
2025-03-24 18:25:57,702 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:25:57,702 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:25:57,702 - father_agent.py:574 - Current Best Reach Probability = 0.22607715430861725
2025-03-24 18:25:57,750 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:25:57,879 - father_agent.py:455 - Training finished.
2025-03-24 18:25:57,886 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:25:58,118 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 18:26:07,689 - father_agent.py:562 - Average Return = 0.0
2025-03-24 18:26:07,689 - father_agent.py:564 - Average Virtual Goal Value = 1.631011962890625
2025-03-24 18:26:07,689 - father_agent.py:566 - Goal Reach Probability = 0.03262023972083144
2025-03-24 18:26:07,689 - father_agent.py:568 - Trap Reach Probability = 0.9673797602791686
2025-03-24 18:26:07,689 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 18:26:07,689 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 18:26:07,689 - father_agent.py:574 - Current Best Reach Probability = 0.22607715430861725
2025-03-24 18:26:07,745 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 18:26:26,422 - evaluators.py:128 - Average Return = 0.0
2025-03-24 18:26:26,422 - evaluators.py:130 - Average Virtual Goal Value = 0.6830140352249146
2025-03-24 18:26:26,422 - evaluators.py:132 - Goal Reach Probability = 0.013660280626011873
2025-03-24 18:26:26,422 - evaluators.py:134 - Trap Reach Probability = 0.9863397193739881
2025-03-24 18:26:26,422 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 18:26:26,422 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 18:26:26,422 - evaluators.py:140 - Current Best Reach Probability = 0.013660280626011873
2025-03-24 18:26:26,439 - environment_wrapper_vec.py:365 - Resetting the environment.
