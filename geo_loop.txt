2025-03-24 13:27:32.301376: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-24 13:27:32.320437: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-24 13:27:32.320506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-24 13:27:32.321661: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-24 13:27:32.326001: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-24 13:27:32.908279: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
0it [00:00, ?it/s]0it [00:00, ?it/s]0it [43:59, ?it/s]
 - PRISM model type: POMDP
2025-03-24 13:27:31,458 - prism_parser.py:203 - loading properties from rl_src/models_paynt_experiments/geo-2-8/sketch.props ...
2025-03-24 13:27:31,459 - prism_parser.py:219 - found the following specification: optimality: Pmax=? ["notbad" U "goal"] 
2025-03-24 13:27:31,704 - sketch.py:135 - sketch parsing OK
2025-03-24 13:27:31,732 - sketch.py:148 - constructed explicit quotient having 45391 states and 177676 choices
2025-03-24 13:27:31,733 - property.py:332 - converting until formula to eventually...
2025-03-24 13:27:31,733 - sketch.py:154 - found the following specification optimality: Pmax=? [F "goal"] 
2025-03-24 13:27:31,774 - pomdp.py:68 - constructed  POMDP having 7 observations.
2025-03-24 13:27:31,784 - pomdp.py:349 - unfolding 1-FSC template into POMDP...
2025-03-24 13:27:31,906 - pomdp.py:355 - constructed quotient MDP having 45391 states and 177676 actions.
2025-03-24 13:27:32,824 - tpu_cluster_resolver.py:34 - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2025-03-24 13:27:32,960 - __init__.py:47 - Creating converter from 7 to 5
2025-03-24 13:27:32,960 - __init__.py:47 - Creating converter from 5 to 7
2025-03-24 13:27:32,960 - __init__.py:47 - Creating converter from 7 to 5
2025-03-24 13:27:32,960 - __init__.py:47 - Creating converter from 5 to 7
2025-03-24 13:27:33,123 - path.py:45 - etils.epath was not found. Using pathlib for file I/O.
2025-03-24 13:27:33,894 - __init__.py:341 - matplotlib data path: /home/davidh/Plocha/synthesis/prerequisites/venv/lib/python3.10/site-packages/matplotlib/mpl-data
2025-03-24 13:27:33,897 - __init__.py:341 - CONFIGDIR=/home/davidh/.config/matplotlib
2025-03-24 13:27:33,897 - __init__.py:1509 - interactive is False
2025-03-24 13:27:33,898 - __init__.py:1510 - platform is linux
2025-03-24 13:27:33,924 - __init__.py:341 - CACHEDIR=/home/davidh/.cache/matplotlib
2025-03-24 13:27:33,926 - font_manager.py:1580 - Using fontManager instance from /home/davidh/.cache/matplotlib/fontlist-v390.json
2025-03-24 13:27:34,295 - synthesizer_pomdp.py:413 - Storm POMDP option enabled
2025-03-24 13:27:34,295 - synthesizer_pomdp.py:414 - Storm settings: iterative - (1200, 20, 20), get_storm_result - None, storm_options - cutoff, prune_storm - False, unfold_strategy - (True, False), use_storm_cutoffs - False
2025-03-24 13:27:34,295 - experimental_interface.py:93 - Model initialized
2025-03-24 13:27:34,301 - vectorized_sim_initializer.py:85 - Model geo-2-8 found in compiled_models_vec_storm. The model will be loaded
2025-03-24 13:27:34,318 - environment_wrapper_vec.py:105 - Grid-like renderer not possible to initialize.
Simulator initialized.
<class 'environment.environment_wrapper_vec.EnvironmentWrapperVec'>
2025-03-24 13:27:34,435 - experimental_interface.py:108 - Environment initialized
2025-03-24 13:27:34,435 - agents_wrapper.py:53 - RL Environment initialized
2025-03-24 13:27:34,556 - recurrent_ppo_agent.py:86 - Agent initialized
2025-03-24 13:27:34,570 - recurrent_ppo_agent.py:88 - Replay buffer initialized
2025-03-24 13:27:34,574 - statistic.py:67 - synthesis initiated, design space: 256
2025-03-24 13:27:34,574 - synthesizer_pomdp.py:240 - Timeout for PAYNT started
2025-03-24 13:27:34,574 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
-----------PAYNT-----------                     
Value = 0.5320137256190993 | Time elapsed = 0.5s | FSC size = 14

-----------PAYNT-----------                     
Value = 0.5696736180875472 | Time elapsed = 1.2s | FSC size = 14

2025-03-24 13:27:36,066 - synthesizer.py:192 - printing synthesized assignment below:
2025-03-24 13:27:36,066 - synthesizer.py:193 - A([done=0	& obsReady=1	& see=1],0)=r, A([done=0	& obsReady=1	& see=0],0)=d, A([done=1	& obsReady=1	& see=0],0)=l, A([done=1	& obsReady=1	& see=1],0)=l
2025-03-24 13:27:36,120 - synthesizer.py:198 - double-checking specification satisfiability:  : 0.5696736180875472
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: AR, synthesis time: 1.49 s
number of holes: 4, family size: 256, quotient: 45391 states / 177676 actions
explored: 100 %

optimum: 0.569674
--------------------
2025-03-24 13:27:53,576 - synthesizer_pomdp.py:111 - Increased memory in all imperfect observation
2025-03-24 13:27:53,584 - pomdp.py:349 - unfolding 2-FSC template into POMDP...
2025-03-24 13:27:54,292 - pomdp.py:355 - constructed quotient MDP having 90782 states and 710704 actions.
2025-03-24 13:27:55,436 - statistic.py:67 - synthesis initiated, design space: 1e9
2025-03-24 13:27:55,436 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-03-24 13:27:55,436 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 13:27:55,536 - storm_pomdp_control.py:231 - Interactive Storm started
2025-03-24 13:27:55,537 - storm_pomdp_control.py:258 - starting Storm POMDP analysis
2025-03-24 13:28:16,557 - storm_pomdp_control.py:305 - Pausing Storm
-----------Storm-----------               
Value = 0.6167931846288172 | Time elapsed = 69.3s | FSC size = 10826

2025-03-24 13:28:46,956 - storm_pomdp_control.py:912 - constructed FSC with 2441 nodes
2025-03-24 13:28:47,618 - synthesizer_pomdp.py:240 - Timeout for PAYNT started
2025-03-24 13:28:48,510 - synthesizer_ar_storm.py:136 - Resuming synthesis
2025-03-24 13:28:48,510 - synthesizer_ar_storm.py:140 - Additional memory needed
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: AR, synthesis time: 0.0 s
number of holes: 22, family size: 1e9, quotient: 90782 states / 710704 actions
explored: 0 %

optimum: 0.569674
--------------------
2025-03-24 13:28:48,510 - synthesizer_pomdp.py:90 - Added memory nodes to match Storm data
2025-03-24 13:28:48,545 - pomdp.py:349 - unfolding 4-FSC template into POMDP...
2025-03-24 13:28:50,884 - pomdp.py:355 - constructed quotient MDP having 166762 states and 2608771 actions.
2025-03-24 13:28:54,342 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e12 to 1e10
2025-03-24 13:28:54,342 - statistic.py:67 - synthesis initiated, design space: 1e12
2025-03-24 13:28:54,342 - synthesizer.py:186 - Synthesizer does not support timeout limiter. Running without timeout.
2025-03-24 13:29:09,236 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 13:29:09,268 - agents_wrapper.py:252 - Creating pretrainer
2025-03-24 13:29:09,580 - recurrent_ppo_agent.py:86 - Agent initialized
2025-03-24 13:29:09,586 - recurrent_ppo_agent.py:88 - Replay buffer initialized
2025-03-24 13:29:09,590 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:29:17,642 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:29:17,642 - father_agent.py:564 - Average Virtual Goal Value = 5.003742218017578
2025-03-24 13:29:17,642 - father_agent.py:566 - Goal Reach Probability = 0.10007484229658933
2025-03-24 13:29:17,642 - father_agent.py:568 - Trap Reach Probability = 0.8999251577034106
2025-03-24 13:29:17,642 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:29:17,642 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:29:17,642 - father_agent.py:574 - Current Best Reach Probability = 0.10007484229658933
2025-03-24 13:29:20,725 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:29:24,035 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:29:24,035 - evaluators.py:130 - Average Virtual Goal Value = 2.8839471340179443
2025-03-24 13:29:24,035 - evaluators.py:132 - Goal Reach Probability = 0.057678943710910355
2025-03-24 13:29:24,035 - evaluators.py:134 - Trap Reach Probability = 0.9423210562890897
2025-03-24 13:29:24,035 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:29:24,035 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:29:24,035 - evaluators.py:140 - Current Best Reach Probability = 0.057678943710910355
2025-03-24 13:29:24,035 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 1.3982458114624023
Epoch: 0, Critic Loss: 1.6233404874801636
Epoch: 10, Actor Loss: 0.1256343424320221
Epoch: 10, Critic Loss: 0.0003171965654473752
Epoch: 20, Actor Loss: 0.2119593769311905
Epoch: 20, Critic Loss: 1.607457160949707
Epoch: 30, Actor Loss: 0.3291924297809601
Epoch: 30, Critic Loss: 2.9859323501586914
Epoch: 40, Actor Loss: 0.2581588923931122
Epoch: 40, Critic Loss: 3.8494205474853516
2025-03-24 13:29:55,362 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:29:58,591 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:29:58,592 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 13:29:58,592 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 13:29:58,592 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 13:29:58,592 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:29:58,592 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:29:58,592 - evaluators.py:140 - Current Best Reach Probability = 0.057678943710910355
2025-03-24 13:29:58,592 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.09539034217596054
Epoch: 50, Critic Loss: 1.2929742336273193
Epoch: 60, Actor Loss: 0.0516299344599247
Epoch: 60, Critic Loss: 0.03128297999501228
Epoch: 70, Actor Loss: 0.09457925707101822
Epoch: 70, Critic Loss: 1.5168638229370117
Epoch: 80, Actor Loss: 0.05553112179040909
Epoch: 80, Critic Loss: 1.333232045173645
Epoch: 90, Actor Loss: 0.10350346565246582
Epoch: 90, Critic Loss: 0.030228018760681152
2025-03-24 13:30:30,678 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:30:33,734 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:30:33,734 - evaluators.py:130 - Average Virtual Goal Value = 13.682219505310059
2025-03-24 13:30:33,734 - evaluators.py:132 - Goal Reach Probability = 0.27364438839848676
2025-03-24 13:30:33,734 - evaluators.py:134 - Trap Reach Probability = 0.7263556116015133
2025-03-24 13:30:33,735 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:30:33,735 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:30:33,735 - evaluators.py:140 - Current Best Reach Probability = 0.27364438839848676
2025-03-24 13:30:33,735 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.03427610918879509
Epoch: 100, Critic Loss: 0.019437087699770927
Epoch: 110, Actor Loss: 0.08829929679632187
Epoch: 110, Critic Loss: 1.5553538799285889
Epoch: 120, Actor Loss: 0.055214930325746536
Epoch: 120, Critic Loss: 0.06973724067211151
Epoch: 130, Actor Loss: 0.0186532661318779
Epoch: 130, Critic Loss: 0.024004725739359856
Epoch: 140, Actor Loss: 0.15141615271568298
Epoch: 140, Critic Loss: 4.528965473175049
2025-03-24 13:31:05,281 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:31:08,377 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:31:08,377 - evaluators.py:130 - Average Virtual Goal Value = 10.315186500549316
2025-03-24 13:31:08,377 - evaluators.py:132 - Goal Reach Probability = 0.20630372492836677
2025-03-24 13:31:08,377 - evaluators.py:134 - Trap Reach Probability = 0.7936962750716332
2025-03-24 13:31:08,377 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:31:08,377 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:31:08,377 - evaluators.py:140 - Current Best Reach Probability = 0.27364438839848676
2025-03-24 13:31:08,377 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.09436594694852829
Epoch: 150, Critic Loss: 0.032234564423561096
Epoch: 160, Actor Loss: 0.032965775579214096
Epoch: 160, Critic Loss: 1.5738506317138672
Epoch: 170, Actor Loss: 0.04948697239160538
Epoch: 170, Critic Loss: 1.6777666807174683
Epoch: 180, Actor Loss: 0.013403487391769886
Epoch: 180, Critic Loss: 0.02132698893547058
Epoch: 190, Actor Loss: 0.19059105217456818
Epoch: 190, Critic Loss: 4.366369247436523
2025-03-24 13:31:39,761 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:31:42,850 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:31:42,850 - evaluators.py:130 - Average Virtual Goal Value = 10.90629768371582
2025-03-24 13:31:42,850 - evaluators.py:132 - Goal Reach Probability = 0.21812596006144394
2025-03-24 13:31:42,850 - evaluators.py:134 - Trap Reach Probability = 0.7818740399385561
2025-03-24 13:31:42,850 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:31:42,850 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:31:42,850 - evaluators.py:140 - Current Best Reach Probability = 0.27364438839848676
2025-03-24 13:31:42,850 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.03058084473013878
Epoch: 200, Critic Loss: 0.06869388371706009
Epoch: 210, Actor Loss: 0.09094230830669403
Epoch: 210, Critic Loss: 1.1403943300247192
Epoch: 220, Actor Loss: 0.013854851946234703
Epoch: 220, Critic Loss: 0.0990716964006424
Epoch: 230, Actor Loss: 0.06736084818840027
Epoch: 230, Critic Loss: 0.1401880383491516
Epoch: 240, Actor Loss: 0.004485192708671093
Epoch: 240, Critic Loss: 0.026905808597803116
2025-03-24 13:32:15,900 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:32:19,388 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:32:19,388 - evaluators.py:130 - Average Virtual Goal Value = 12.944785118103027
2025-03-24 13:32:19,388 - evaluators.py:132 - Goal Reach Probability = 0.2588957055214724
2025-03-24 13:32:19,388 - evaluators.py:134 - Trap Reach Probability = 0.7411042944785277
2025-03-24 13:32:19,388 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:32:19,388 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:32:19,388 - evaluators.py:140 - Current Best Reach Probability = 0.27364438839848676
2025-03-24 13:32:19,388 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.012454795651137829
Epoch: 250, Critic Loss: 0.012504026293754578
Epoch: 260, Actor Loss: 0.05919410660862923
Epoch: 260, Critic Loss: 0.056971434503793716
Epoch: 270, Actor Loss: 0.024256553500890732
Epoch: 270, Critic Loss: 0.1179354265332222
Epoch: 280, Actor Loss: 0.07412730157375336
Epoch: 280, Critic Loss: 0.3648906648159027
Epoch: 290, Actor Loss: 0.046750325709581375
Epoch: 290, Critic Loss: 1.2171630859375
2025-03-24 13:32:54,118 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:32:56,974 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:32:56,974 - evaluators.py:130 - Average Virtual Goal Value = 16.785715103149414
2025-03-24 13:32:56,974 - evaluators.py:132 - Goal Reach Probability = 0.3357142857142857
2025-03-24 13:32:56,974 - evaluators.py:134 - Trap Reach Probability = 0.6642857142857143
2025-03-24 13:32:56,975 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:32:56,975 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:32:56,975 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 13:32:56,975 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.04607323184609413
Epoch: 300, Critic Loss: 2.103970766067505
Epoch: 310, Actor Loss: 0.06944771111011505
Epoch: 310, Critic Loss: 2.014742612838745
Epoch: 320, Actor Loss: 0.021197590976953506
Epoch: 320, Critic Loss: 2.099865674972534
Epoch: 330, Actor Loss: 0.01047375425696373
Epoch: 330, Critic Loss: 0.0013752668164670467
Epoch: 340, Actor Loss: 0.031643737107515335
Epoch: 340, Critic Loss: 1.9700161218643188
2025-03-24 13:33:28,832 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:33:32,049 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:33:32,050 - evaluators.py:130 - Average Virtual Goal Value = 1.836734652519226
2025-03-24 13:33:32,050 - evaluators.py:132 - Goal Reach Probability = 0.036734693877551024
2025-03-24 13:33:32,050 - evaluators.py:134 - Trap Reach Probability = 0.963265306122449
2025-03-24 13:33:32,050 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:33:32,050 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:33:32,050 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 13:33:32,050 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.06801609694957733
Epoch: 350, Critic Loss: 2.262617588043213
Epoch: 360, Actor Loss: 0.020938973873853683
Epoch: 360, Critic Loss: 0.03878936544060707
Epoch: 370, Actor Loss: 0.022147202864289284
Epoch: 370, Critic Loss: 1.7959544658660889
Epoch: 380, Actor Loss: 0.02967802993953228
Epoch: 380, Critic Loss: 1.7778843641281128
Epoch: 390, Actor Loss: 0.0906321257352829
Epoch: 390, Critic Loss: 2.368809461593628
2025-03-24 13:34:03,523 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:34:06,755 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:34:06,755 - evaluators.py:130 - Average Virtual Goal Value = 0.625
2025-03-24 13:34:06,755 - evaluators.py:132 - Goal Reach Probability = 0.0125
2025-03-24 13:34:06,755 - evaluators.py:134 - Trap Reach Probability = 0.9875
2025-03-24 13:34:06,755 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:34:06,755 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:34:06,755 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 13:34:06,755 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.09049152582883835
Epoch: 400, Critic Loss: 5.477519989013672
Epoch: 410, Actor Loss: 0.04068504646420479
Epoch: 410, Critic Loss: 2.1881911754608154
Epoch: 420, Actor Loss: 0.041987668722867966
Epoch: 420, Critic Loss: 0.2271616905927658
Epoch: 430, Actor Loss: 0.08743724972009659
Epoch: 430, Critic Loss: 0.18616169691085815
Epoch: 440, Actor Loss: 0.04940526932477951
Epoch: 440, Critic Loss: 0.23402804136276245
2025-03-24 13:34:39,505 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:34:42,644 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:34:42,644 - evaluators.py:130 - Average Virtual Goal Value = 5.622009754180908
2025-03-24 13:34:42,644 - evaluators.py:132 - Goal Reach Probability = 0.11244019138755981
2025-03-24 13:34:42,644 - evaluators.py:134 - Trap Reach Probability = 0.8875598086124402
2025-03-24 13:34:42,644 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:34:42,644 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:34:42,644 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 13:34:42,644 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.008282635360956192
Epoch: 450, Critic Loss: 0.16695058345794678
Epoch: 460, Actor Loss: 0.01790514588356018
Epoch: 460, Critic Loss: 0.432822585105896
Epoch: 470, Actor Loss: 0.02858600579202175
Epoch: 470, Critic Loss: 0.2944270372390747
Epoch: 480, Actor Loss: 0.038223929703235626
Epoch: 480, Critic Loss: 0.08355844765901566
Epoch: 490, Actor Loss: 0.03169142082333565
Epoch: 490, Critic Loss: 0.21098241209983826
2025-03-24 13:35:14,381 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:35:17,626 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:35:17,626 - evaluators.py:130 - Average Virtual Goal Value = 2.046783685684204
2025-03-24 13:35:17,626 - evaluators.py:132 - Goal Reach Probability = 0.04093567251461988
2025-03-24 13:35:17,626 - evaluators.py:134 - Trap Reach Probability = 0.9590643274853801
2025-03-24 13:35:17,626 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:35:17,626 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:35:17,626 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 13:35:17,626 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.06832396239042282
Epoch: 500, Critic Loss: 2.0573437213897705
Epoch: 510, Actor Loss: 0.01591995730996132
Epoch: 510, Critic Loss: 1.4666849374771118
Epoch: 520, Actor Loss: 0.03465162590146065
Epoch: 520, Critic Loss: 2.1003167629241943
Epoch: 530, Actor Loss: 0.005490196403115988
Epoch: 530, Critic Loss: 0.1351194679737091
Epoch: 540, Actor Loss: 0.04835304245352745
Epoch: 540, Critic Loss: 0.31217604875564575
2025-03-24 13:35:51,299 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:35:54,641 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:35:54,641 - evaluators.py:130 - Average Virtual Goal Value = 0.7936508059501648
2025-03-24 13:35:54,641 - evaluators.py:132 - Goal Reach Probability = 0.015873015873015872
2025-03-24 13:35:54,641 - evaluators.py:134 - Trap Reach Probability = 0.9841269841269841
2025-03-24 13:35:54,641 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:35:54,641 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:35:54,641 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 13:35:54,641 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.02279273420572281
Epoch: 550, Critic Loss: 2.9488346576690674
Epoch: 560, Actor Loss: 0.04098132252693176
Epoch: 560, Critic Loss: 1.3817083835601807
Epoch: 570, Actor Loss: 0.0688692182302475
Epoch: 570, Critic Loss: 0.22511005401611328
Epoch: 580, Actor Loss: 0.0022620174568146467
Epoch: 580, Critic Loss: 0.12892411649227142
Epoch: 590, Actor Loss: 0.04545808956027031
Epoch: 590, Critic Loss: 3.900883436203003
2025-03-24 13:36:27,482 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:36:30,808 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:36:30,808 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 13:36:30,808 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 13:36:30,808 - evaluators.py:134 - Trap Reach Probability = 1.0
2025-03-24 13:36:30,808 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:36:30,808 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:36:30,808 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 13:36:30,809 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.08398734778165817
Epoch: 600, Critic Loss: 2.9973549842834473
Epoch: 610, Actor Loss: 0.03320671245455742
Epoch: 610, Critic Loss: 2.2101972103118896
Epoch: 620, Actor Loss: 0.09116456657648087
Epoch: 620, Critic Loss: 2.0900795459747314
Epoch: 630, Actor Loss: 0.003676900640130043
Epoch: 630, Critic Loss: 0.0008824149845167994
Epoch: 640, Actor Loss: 0.03892851248383522
Epoch: 640, Critic Loss: 0.12198644876480103
2025-03-24 13:37:02,474 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:37:05,687 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:37:05,687 - evaluators.py:130 - Average Virtual Goal Value = 3.2921810150146484
2025-03-24 13:37:05,687 - evaluators.py:132 - Goal Reach Probability = 0.06584362139917696
2025-03-24 13:37:05,687 - evaluators.py:134 - Trap Reach Probability = 0.934156378600823
2025-03-24 13:37:05,687 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:37:05,687 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:37:05,687 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 13:37:05,687 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.07974879443645477
Epoch: 650, Critic Loss: 0.18251313269138336
Epoch: 660, Actor Loss: 0.017001770436763763
Epoch: 660, Critic Loss: 1.8797348737716675
Epoch: 670, Actor Loss: 0.08234527707099915
Epoch: 670, Critic Loss: 1.8824963569641113
Epoch: 680, Actor Loss: 0.004597132094204426
Epoch: 680, Critic Loss: 0.05750788003206253
Epoch: 690, Actor Loss: 0.0027709919959306717
Epoch: 690, Critic Loss: 0.05244992673397064
2025-03-24 13:37:37,497 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:37:40,939 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:37:40,939 - evaluators.py:130 - Average Virtual Goal Value = 2.669902801513672
2025-03-24 13:37:40,939 - evaluators.py:132 - Goal Reach Probability = 0.05339805825242718
2025-03-24 13:37:40,939 - evaluators.py:134 - Trap Reach Probability = 0.9466019417475728
2025-03-24 13:37:40,939 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:37:40,939 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:37:40,939 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 13:37:40,939 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.06928633898496628
Epoch: 700, Critic Loss: 2.5315136909484863
2025-03-24 13:37:40,941 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 13:37:40,942 - father_agent.py:449 - Before training evaluation.
2025-03-24 13:37:40,942 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:37:48,763 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:37:48,764 - father_agent.py:564 - Average Virtual Goal Value = 1.0144927501678467
2025-03-24 13:37:48,764 - father_agent.py:566 - Goal Reach Probability = 0.020289855072463767
2025-03-24 13:37:48,764 - father_agent.py:568 - Trap Reach Probability = 0.5652173913043478
2025-03-24 13:37:48,764 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:37:48,764 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:37:48,764 - father_agent.py:574 - Current Best Reach Probability = 0.10007484229658933
2025-03-24 13:37:48,786 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:37:48,867 - father_agent.py:359 - Training agent on-policy
2025-03-24 13:37:55,288 - father_agent.py:306 - Step: 0, Training loss: 9.613133430480957
2025-03-24 13:37:55,312 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:38:02,710 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:38:02,710 - father_agent.py:564 - Average Virtual Goal Value = 9.01084041595459
2025-03-24 13:38:02,710 - father_agent.py:566 - Goal Reach Probability = 0.1802168021680217
2025-03-24 13:38:02,710 - father_agent.py:568 - Trap Reach Probability = 0.8197831978319783
2025-03-24 13:38:02,710 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:38:02,710 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:38:02,710 - father_agent.py:574 - Current Best Reach Probability = 0.1802168021680217
2025-03-24 13:38:02,737 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:38:04,477 - father_agent.py:306 - Step: 5, Training loss: 10.831265449523926
2025-03-24 13:38:05,528 - father_agent.py:306 - Step: 10, Training loss: 6.026795387268066
2025-03-24 13:38:06,577 - father_agent.py:306 - Step: 15, Training loss: 3.382929801940918
2025-03-24 13:38:07,647 - father_agent.py:306 - Step: 20, Training loss: 2.627521276473999
2025-03-24 13:38:08,653 - father_agent.py:306 - Step: 25, Training loss: 2.420542001724243
2025-03-24 13:38:09,715 - father_agent.py:306 - Step: 30, Training loss: 2.3016204833984375
2025-03-24 13:38:15,148 - father_agent.py:306 - Step: 35, Training loss: 2.639272451400757
2025-03-24 13:38:16,241 - father_agent.py:306 - Step: 40, Training loss: 3.3752553462982178
2025-03-24 13:38:17,246 - father_agent.py:306 - Step: 45, Training loss: 3.3278021812438965
2025-03-24 13:38:18,326 - father_agent.py:306 - Step: 50, Training loss: 3.2569446563720703
2025-03-24 13:38:19,431 - father_agent.py:306 - Step: 55, Training loss: 2.838218927383423
2025-03-24 13:38:20,838 - father_agent.py:306 - Step: 60, Training loss: 2.1471893787384033
2025-03-24 13:38:21,929 - father_agent.py:306 - Step: 65, Training loss: 1.9669886827468872
2025-03-24 13:38:22,918 - father_agent.py:306 - Step: 70, Training loss: 2.8031246662139893
2025-03-24 13:38:23,916 - father_agent.py:306 - Step: 75, Training loss: 2.3801567554473877
2025-03-24 13:38:24,922 - father_agent.py:306 - Step: 80, Training loss: 2.0903520584106445
2025-03-24 13:38:25,928 - father_agent.py:306 - Step: 85, Training loss: 1.8072731494903564
2025-03-24 13:38:26,939 - father_agent.py:306 - Step: 90, Training loss: 1.7416882514953613
2025-03-24 13:38:27,957 - father_agent.py:306 - Step: 95, Training loss: 1.6561391353607178
2025-03-24 13:38:28,948 - father_agent.py:306 - Step: 100, Training loss: 1.5867689847946167
2025-03-24 13:38:28,976 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:38:35,506 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:38:35,506 - father_agent.py:564 - Average Virtual Goal Value = 28.69719886779785
2025-03-24 13:38:35,506 - father_agent.py:566 - Goal Reach Probability = 0.5739439751000445
2025-03-24 13:38:35,506 - father_agent.py:568 - Trap Reach Probability = 0.42605602489995553
2025-03-24 13:38:35,506 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:38:35,506 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:38:35,506 - father_agent.py:574 - Current Best Reach Probability = 0.5739439751000445
2025-03-24 13:38:35,539 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:38:37,188 - father_agent.py:306 - Step: 105, Training loss: 1.4009051322937012
2025-03-24 13:38:38,156 - father_agent.py:306 - Step: 110, Training loss: 1.4628043174743652
2025-03-24 13:38:39,101 - father_agent.py:306 - Step: 115, Training loss: 1.3013473749160767
2025-03-24 13:38:40,054 - father_agent.py:306 - Step: 120, Training loss: 1.6177575588226318
2025-03-24 13:38:41,033 - father_agent.py:306 - Step: 125, Training loss: 1.4496744871139526
2025-03-24 13:38:42,009 - father_agent.py:306 - Step: 130, Training loss: 1.4351081848144531
2025-03-24 13:38:42,985 - father_agent.py:306 - Step: 135, Training loss: 1.2166062593460083
2025-03-24 13:38:43,957 - father_agent.py:306 - Step: 140, Training loss: 1.5553175210952759
2025-03-24 13:38:44,912 - father_agent.py:306 - Step: 145, Training loss: 1.171341896057129
2025-03-24 13:38:45,887 - father_agent.py:306 - Step: 150, Training loss: 1.2920572757720947
2025-03-24 13:38:46,834 - father_agent.py:306 - Step: 155, Training loss: 1.2064297199249268
2025-03-24 13:38:47,827 - father_agent.py:306 - Step: 160, Training loss: 1.4453998804092407
2025-03-24 13:38:48,819 - father_agent.py:306 - Step: 165, Training loss: 1.2933640480041504
2025-03-24 13:38:49,813 - father_agent.py:306 - Step: 170, Training loss: 1.3513500690460205
2025-03-24 13:38:50,791 - father_agent.py:306 - Step: 175, Training loss: 1.3883482217788696
2025-03-24 13:38:51,773 - father_agent.py:306 - Step: 180, Training loss: 1.395667314529419
2025-03-24 13:38:52,766 - father_agent.py:306 - Step: 185, Training loss: 1.3734337091445923
2025-03-24 13:38:53,733 - father_agent.py:306 - Step: 190, Training loss: 1.2846060991287231
2025-03-24 13:38:54,698 - father_agent.py:306 - Step: 195, Training loss: 1.3560738563537598
2025-03-24 13:38:55,674 - father_agent.py:306 - Step: 200, Training loss: 1.12826406955719
2025-03-24 13:38:55,706 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:39:02,239 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:39:02,239 - father_agent.py:564 - Average Virtual Goal Value = 28.233810424804688
2025-03-24 13:39:02,239 - father_agent.py:566 - Goal Reach Probability = 0.5646762121705174
2025-03-24 13:39:02,239 - father_agent.py:568 - Trap Reach Probability = 0.4353237878294826
2025-03-24 13:39:02,239 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:39:02,239 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:39:02,239 - father_agent.py:574 - Current Best Reach Probability = 0.5739439751000445
2025-03-24 13:39:02,269 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:39:03,902 - father_agent.py:306 - Step: 205, Training loss: 1.227641224861145
2025-03-24 13:39:04,877 - father_agent.py:306 - Step: 210, Training loss: 1.218730092048645
2025-03-24 13:39:06,046 - father_agent.py:306 - Step: 215, Training loss: 1.2596690654754639
2025-03-24 13:39:07,152 - father_agent.py:306 - Step: 220, Training loss: 1.5379488468170166
2025-03-24 13:39:08,124 - father_agent.py:306 - Step: 225, Training loss: 1.3337053060531616
2025-03-24 13:39:09,116 - father_agent.py:306 - Step: 230, Training loss: 1.4223101139068604
2025-03-24 13:39:10,103 - father_agent.py:306 - Step: 235, Training loss: 1.1839160919189453
2025-03-24 13:39:11,218 - father_agent.py:306 - Step: 240, Training loss: 1.1147606372833252
2025-03-24 13:39:12,219 - father_agent.py:306 - Step: 245, Training loss: 1.0745048522949219
2025-03-24 13:39:13,214 - father_agent.py:306 - Step: 250, Training loss: 1.2017011642456055
2025-03-24 13:39:14,181 - father_agent.py:306 - Step: 255, Training loss: 1.2454164028167725
2025-03-24 13:39:15,160 - father_agent.py:306 - Step: 260, Training loss: 1.219331979751587
2025-03-24 13:39:16,125 - father_agent.py:306 - Step: 265, Training loss: 1.0220179557800293
2025-03-24 13:39:17,096 - father_agent.py:306 - Step: 270, Training loss: 1.3047605752944946
2025-03-24 13:39:18,186 - father_agent.py:306 - Step: 275, Training loss: 1.2463692426681519
2025-03-24 13:39:19,161 - father_agent.py:306 - Step: 280, Training loss: 1.1420265436172485
2025-03-24 13:39:20,132 - father_agent.py:306 - Step: 285, Training loss: 1.086218237876892
2025-03-24 13:39:21,094 - father_agent.py:306 - Step: 290, Training loss: 1.0843610763549805
2025-03-24 13:39:22,094 - father_agent.py:306 - Step: 295, Training loss: 1.265803337097168
2025-03-24 13:39:23,074 - father_agent.py:306 - Step: 300, Training loss: 1.1833699941635132
2025-03-24 13:39:23,105 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:39:29,604 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:39:29,604 - father_agent.py:564 - Average Virtual Goal Value = 28.273212432861328
2025-03-24 13:39:29,604 - father_agent.py:566 - Goal Reach Probability = 0.5654642618570475
2025-03-24 13:39:29,604 - father_agent.py:568 - Trap Reach Probability = 0.4345357381429526
2025-03-24 13:39:29,604 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:39:29,604 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:39:29,604 - father_agent.py:574 - Current Best Reach Probability = 0.5739439751000445
2025-03-24 13:39:29,633 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:39:31,300 - father_agent.py:306 - Step: 305, Training loss: 1.3294285535812378
2025-03-24 13:39:32,282 - father_agent.py:306 - Step: 310, Training loss: 1.035438060760498
2025-03-24 13:39:33,250 - father_agent.py:306 - Step: 315, Training loss: 1.3242138624191284
2025-03-24 13:39:34,231 - father_agent.py:306 - Step: 320, Training loss: 1.1709367036819458
2025-03-24 13:39:35,205 - father_agent.py:306 - Step: 325, Training loss: 1.0971770286560059
2025-03-24 13:39:36,186 - father_agent.py:306 - Step: 330, Training loss: 0.983250617980957
2025-03-24 13:39:37,178 - father_agent.py:306 - Step: 335, Training loss: 1.1489453315734863
2025-03-24 13:39:38,167 - father_agent.py:306 - Step: 340, Training loss: 1.2460296154022217
2025-03-24 13:39:39,148 - father_agent.py:306 - Step: 345, Training loss: 1.2176101207733154
2025-03-24 13:39:40,131 - father_agent.py:306 - Step: 350, Training loss: 1.1381720304489136
2025-03-24 13:39:41,102 - father_agent.py:306 - Step: 355, Training loss: 1.154175043106079
2025-03-24 13:39:42,077 - father_agent.py:306 - Step: 360, Training loss: 1.1331936120986938
2025-03-24 13:39:43,053 - father_agent.py:306 - Step: 365, Training loss: 1.2847542762756348
2025-03-24 13:39:44,024 - father_agent.py:306 - Step: 370, Training loss: 1.1162148714065552
2025-03-24 13:39:44,998 - father_agent.py:306 - Step: 375, Training loss: 1.2649600505828857
2025-03-24 13:39:45,982 - father_agent.py:306 - Step: 380, Training loss: 1.1717662811279297
2025-03-24 13:39:46,957 - father_agent.py:306 - Step: 385, Training loss: 1.2870874404907227
2025-03-24 13:39:47,949 - father_agent.py:306 - Step: 390, Training loss: 1.213770866394043
2025-03-24 13:39:48,922 - father_agent.py:306 - Step: 395, Training loss: 1.164075493812561
2025-03-24 13:39:49,902 - father_agent.py:306 - Step: 400, Training loss: 1.1033313274383545
2025-03-24 13:39:49,939 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:39:57,278 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:39:57,278 - father_agent.py:564 - Average Virtual Goal Value = 29.210886001586914
2025-03-24 13:39:57,278 - father_agent.py:566 - Goal Reach Probability = 0.5842177077016374
2025-03-24 13:39:57,278 - father_agent.py:568 - Trap Reach Probability = 0.41578229229836267
2025-03-24 13:39:57,278 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:39:57,278 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:39:57,278 - father_agent.py:574 - Current Best Reach Probability = 0.5842177077016374
2025-03-24 13:39:57,312 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:39:59,076 - father_agent.py:306 - Step: 405, Training loss: 1.2461636066436768
2025-03-24 13:40:00,054 - father_agent.py:306 - Step: 410, Training loss: 1.1091514825820923
2025-03-24 13:40:01,094 - father_agent.py:306 - Step: 415, Training loss: 0.9694027304649353
2025-03-24 13:40:02,077 - father_agent.py:306 - Step: 420, Training loss: 1.2269415855407715
2025-03-24 13:40:03,068 - father_agent.py:306 - Step: 425, Training loss: 1.0308754444122314
2025-03-24 13:40:04,052 - father_agent.py:306 - Step: 430, Training loss: 1.0731370449066162
2025-03-24 13:40:05,037 - father_agent.py:306 - Step: 435, Training loss: 0.8814578056335449
2025-03-24 13:40:06,025 - father_agent.py:306 - Step: 440, Training loss: 1.1387203931808472
2025-03-24 13:40:07,008 - father_agent.py:306 - Step: 445, Training loss: 1.024070382118225
2025-03-24 13:40:07,999 - father_agent.py:306 - Step: 450, Training loss: 0.922451376914978
2025-03-24 13:40:08,973 - father_agent.py:306 - Step: 455, Training loss: 1.0671255588531494
2025-03-24 13:40:09,959 - father_agent.py:306 - Step: 460, Training loss: 1.0570706129074097
2025-03-24 13:40:10,940 - father_agent.py:306 - Step: 465, Training loss: 0.8886843919754028
2025-03-24 13:40:11,937 - father_agent.py:306 - Step: 470, Training loss: 1.2335703372955322
2025-03-24 13:40:12,930 - father_agent.py:306 - Step: 475, Training loss: 1.006424069404602
2025-03-24 13:40:13,910 - father_agent.py:306 - Step: 480, Training loss: 1.008821964263916
2025-03-24 13:40:14,894 - father_agent.py:306 - Step: 485, Training loss: 1.0055631399154663
2025-03-24 13:40:15,878 - father_agent.py:306 - Step: 490, Training loss: 1.0504506826400757
2025-03-24 13:40:16,848 - father_agent.py:306 - Step: 495, Training loss: 1.1454867124557495
2025-03-24 13:40:17,813 - father_agent.py:306 - Step: 500, Training loss: 0.7740384340286255
2025-03-24 13:40:17,844 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:40:25,445 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:40:25,445 - father_agent.py:564 - Average Virtual Goal Value = 29.123004913330078
2025-03-24 13:40:25,445 - father_agent.py:566 - Goal Reach Probability = 0.5824600999516363
2025-03-24 13:40:25,445 - father_agent.py:568 - Trap Reach Probability = 0.4175399000483637
2025-03-24 13:40:25,445 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:40:25,445 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:40:25,445 - father_agent.py:574 - Current Best Reach Probability = 0.5842177077016374
2025-03-24 13:40:25,477 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:40:27,161 - father_agent.py:306 - Step: 505, Training loss: 0.8471617698669434
2025-03-24 13:40:28,138 - father_agent.py:306 - Step: 510, Training loss: 0.7259196043014526
2025-03-24 13:40:29,207 - father_agent.py:306 - Step: 515, Training loss: 0.45369386672973633
2025-03-24 13:40:30,289 - father_agent.py:306 - Step: 520, Training loss: 0.5664582252502441
2025-03-24 13:40:31,378 - father_agent.py:306 - Step: 525, Training loss: 0.37617573142051697
2025-03-24 13:40:32,564 - father_agent.py:306 - Step: 530, Training loss: 0.21457241475582123
2025-03-24 13:40:33,715 - father_agent.py:306 - Step: 535, Training loss: 0.2732754945755005
2025-03-24 13:40:34,861 - father_agent.py:306 - Step: 540, Training loss: 0.3080219626426697
2025-03-24 13:40:36,065 - father_agent.py:306 - Step: 545, Training loss: 0.2821519672870636
2025-03-24 13:40:37,220 - father_agent.py:306 - Step: 550, Training loss: 0.41677892208099365
2025-03-24 13:40:38,360 - father_agent.py:306 - Step: 555, Training loss: 0.39609289169311523
2025-03-24 13:40:39,498 - father_agent.py:306 - Step: 560, Training loss: 0.5577195286750793
2025-03-24 13:40:40,602 - father_agent.py:306 - Step: 565, Training loss: 0.3671207129955292
2025-03-24 13:40:41,788 - father_agent.py:306 - Step: 570, Training loss: 0.10753680765628815
2025-03-24 13:40:42,989 - father_agent.py:306 - Step: 575, Training loss: 0.09547501802444458
2025-03-24 13:40:44,185 - father_agent.py:306 - Step: 580, Training loss: 0.1001819372177124
2025-03-24 13:40:45,371 - father_agent.py:306 - Step: 585, Training loss: 0.27849191427230835
2025-03-24 13:40:46,472 - father_agent.py:306 - Step: 590, Training loss: 1.0618518590927124
2025-03-24 13:40:47,466 - father_agent.py:306 - Step: 595, Training loss: 1.7784159183502197
2025-03-24 13:40:48,454 - father_agent.py:306 - Step: 600, Training loss: 1.3424752950668335
2025-03-24 13:40:48,487 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:40:55,012 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:40:55,012 - father_agent.py:564 - Average Virtual Goal Value = 27.807024002075195
2025-03-24 13:40:55,012 - father_agent.py:566 - Goal Reach Probability = 0.5561404723973125
2025-03-24 13:40:55,013 - father_agent.py:568 - Trap Reach Probability = 0.44385952760268754
2025-03-24 13:40:55,013 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:40:55,013 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:40:55,013 - father_agent.py:574 - Current Best Reach Probability = 0.5842177077016374
2025-03-24 13:40:55,043 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:40:56,682 - father_agent.py:306 - Step: 605, Training loss: 1.4132282733917236
2025-03-24 13:40:57,692 - father_agent.py:306 - Step: 610, Training loss: 1.4017109870910645
2025-03-24 13:40:58,685 - father_agent.py:306 - Step: 615, Training loss: 1.2629969120025635
2025-03-24 13:40:59,674 - father_agent.py:306 - Step: 620, Training loss: 1.2523529529571533
2025-03-24 13:41:00,669 - father_agent.py:306 - Step: 625, Training loss: 1.2025285959243774
2025-03-24 13:41:01,645 - father_agent.py:306 - Step: 630, Training loss: 1.126321792602539
2025-03-24 13:41:02,625 - father_agent.py:306 - Step: 635, Training loss: 1.3528072834014893
2025-03-24 13:41:03,607 - father_agent.py:306 - Step: 640, Training loss: 1.1049469709396362
2025-03-24 13:41:04,583 - father_agent.py:306 - Step: 645, Training loss: 1.114542841911316
2025-03-24 13:41:05,557 - father_agent.py:306 - Step: 650, Training loss: 1.3164926767349243
2025-03-24 13:41:06,527 - father_agent.py:306 - Step: 655, Training loss: 1.3132368326187134
2025-03-24 13:41:07,521 - father_agent.py:306 - Step: 660, Training loss: 1.2889769077301025
2025-03-24 13:41:08,492 - father_agent.py:306 - Step: 665, Training loss: 1.1322920322418213
2025-03-24 13:41:09,569 - father_agent.py:306 - Step: 670, Training loss: 1.2961506843566895
2025-03-24 13:41:10,638 - father_agent.py:306 - Step: 675, Training loss: 1.0774568319320679
2025-03-24 13:41:11,612 - father_agent.py:306 - Step: 680, Training loss: 1.2109696865081787
2025-03-24 13:41:12,619 - father_agent.py:306 - Step: 685, Training loss: 1.0730576515197754
2025-03-24 13:41:13,642 - father_agent.py:306 - Step: 690, Training loss: 1.0393301248550415
2025-03-24 13:41:14,662 - father_agent.py:306 - Step: 695, Training loss: 1.0086588859558105
2025-03-24 13:41:15,626 - father_agent.py:306 - Step: 700, Training loss: 1.094222903251648
2025-03-24 13:41:15,661 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:41:22,396 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:41:22,396 - father_agent.py:564 - Average Virtual Goal Value = 29.024045944213867
2025-03-24 13:41:22,397 - father_agent.py:566 - Goal Reach Probability = 0.5804809052333805
2025-03-24 13:41:22,397 - father_agent.py:568 - Trap Reach Probability = 0.4195190947666195
2025-03-24 13:41:22,397 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:41:22,397 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:41:22,397 - father_agent.py:574 - Current Best Reach Probability = 0.5842177077016374
2025-03-24 13:41:22,429 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:41:22,519 - father_agent.py:455 - Training finished.
2025-03-24 13:41:22,524 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:41:22,608 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 13:41:29,518 - father_agent.py:562 - Average Return = 0.0
2025-03-24 13:41:29,519 - father_agent.py:564 - Average Virtual Goal Value = 29.47249412536621
2025-03-24 13:41:29,519 - father_agent.py:566 - Goal Reach Probability = 0.5894498656484232
2025-03-24 13:41:29,519 - father_agent.py:568 - Trap Reach Probability = 0.4105501343515769
2025-03-24 13:41:29,519 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 13:41:29,519 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 13:41:29,519 - father_agent.py:574 - Current Best Reach Probability = 0.5894498656484232
2025-03-24 13:41:29,561 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:41:42,515 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:41:42,515 - evaluators.py:130 - Average Virtual Goal Value = 33.607460021972656
2025-03-24 13:41:42,515 - evaluators.py:132 - Goal Reach Probability = 0.6721492327750477
2025-03-24 13:41:42,515 - evaluators.py:134 - Trap Reach Probability = 0.32303680673954466
2025-03-24 13:41:42,515 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:41:42,515 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:41:42,515 - evaluators.py:140 - Current Best Reach Probability = 0.6721492327750477
2025-03-24 13:41:42,526 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:42:19,186 - cloned_fsc_actor_policy.py:189 - Epoch 0, Loss: 1.6613
2025-03-24 13:42:19,186 - cloned_fsc_actor_policy.py:190 - Epoch 0, Accuracy: 0.0506
2025-03-24 13:42:19,225 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:42:37,883 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:42:37,883 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 13:42:37,883 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 13:42:37,883 - evaluators.py:134 - Trap Reach Probability = 0.21746880570409982
2025-03-24 13:42:37,883 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:42:37,883 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:42:37,883 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 13:42:37,896 - attrs.py:205 - Creating converter from 5 to 3
2025-03-24 13:42:41,375 - cloned_fsc_actor_policy.py:189 - Epoch 1000, Loss: 0.9664
2025-03-24 13:42:41,375 - cloned_fsc_actor_policy.py:190 - Epoch 1000, Accuracy: 0.6661
2025-03-24 13:42:44,464 - cloned_fsc_actor_policy.py:189 - Epoch 2000, Loss: 0.3743
2025-03-24 13:42:44,464 - cloned_fsc_actor_policy.py:190 - Epoch 2000, Accuracy: 0.8798
2025-03-24 13:42:47,951 - cloned_fsc_actor_policy.py:189 - Epoch 3000, Loss: 0.3212
2025-03-24 13:42:47,951 - cloned_fsc_actor_policy.py:190 - Epoch 3000, Accuracy: 0.8855
2025-03-24 13:42:51,173 - cloned_fsc_actor_policy.py:189 - Epoch 4000, Loss: 0.3093
2025-03-24 13:42:51,173 - cloned_fsc_actor_policy.py:190 - Epoch 4000, Accuracy: 0.8851
2025-03-24 13:42:54,643 - cloned_fsc_actor_policy.py:189 - Epoch 5000, Loss: 0.3058
2025-03-24 13:42:54,643 - cloned_fsc_actor_policy.py:190 - Epoch 5000, Accuracy: 0.8838
2025-03-24 13:42:54,677 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:43:10,224 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:43:10,224 - evaluators.py:130 - Average Virtual Goal Value = 24.13793182373047
2025-03-24 13:43:10,224 - evaluators.py:132 - Goal Reach Probability = 0.4827586206896552
2025-03-24 13:43:10,224 - evaluators.py:134 - Trap Reach Probability = 0.1557285873192436
2025-03-24 13:43:10,224 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:43:10,224 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:43:10,224 - evaluators.py:140 - Current Best Reach Probability = 0.4827586206896552
2025-03-24 13:43:13,496 - cloned_fsc_actor_policy.py:189 - Epoch 6000, Loss: 0.3012
2025-03-24 13:43:13,496 - cloned_fsc_actor_policy.py:190 - Epoch 6000, Accuracy: 0.8824
2025-03-24 13:43:16,579 - cloned_fsc_actor_policy.py:189 - Epoch 7000, Loss: 0.2997
2025-03-24 13:43:16,579 - cloned_fsc_actor_policy.py:190 - Epoch 7000, Accuracy: 0.8820
2025-03-24 13:43:19,731 - cloned_fsc_actor_policy.py:189 - Epoch 8000, Loss: 0.2974
2025-03-24 13:43:19,731 - cloned_fsc_actor_policy.py:190 - Epoch 8000, Accuracy: 0.8834
2025-03-24 13:43:22,973 - cloned_fsc_actor_policy.py:189 - Epoch 9000, Loss: 0.2920
2025-03-24 13:43:22,974 - cloned_fsc_actor_policy.py:190 - Epoch 9000, Accuracy: 0.8846
2025-03-24 13:43:26,193 - cloned_fsc_actor_policy.py:189 - Epoch 10000, Loss: 0.2884
2025-03-24 13:43:26,193 - cloned_fsc_actor_policy.py:190 - Epoch 10000, Accuracy: 0.8850
2025-03-24 13:43:26,221 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:43:41,838 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:43:41,838 - evaluators.py:130 - Average Virtual Goal Value = 13.777090072631836
2025-03-24 13:43:41,838 - evaluators.py:132 - Goal Reach Probability = 0.2755417956656347
2025-03-24 13:43:41,838 - evaluators.py:134 - Trap Reach Probability = 0.14086687306501547
2025-03-24 13:43:41,838 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:43:41,838 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:43:41,838 - evaluators.py:140 - Current Best Reach Probability = 0.4827586206896552
2025-03-24 13:43:44,882 - cloned_fsc_actor_policy.py:189 - Epoch 11000, Loss: 0.2892
2025-03-24 13:43:44,882 - cloned_fsc_actor_policy.py:190 - Epoch 11000, Accuracy: 0.8845
2025-03-24 13:43:47,981 - cloned_fsc_actor_policy.py:189 - Epoch 12000, Loss: 0.2866
2025-03-24 13:43:47,981 - cloned_fsc_actor_policy.py:190 - Epoch 12000, Accuracy: 0.8852
2025-03-24 13:43:51,065 - cloned_fsc_actor_policy.py:189 - Epoch 13000, Loss: 0.2819
2025-03-24 13:43:51,066 - cloned_fsc_actor_policy.py:190 - Epoch 13000, Accuracy: 0.8864
2025-03-24 13:43:54,261 - cloned_fsc_actor_policy.py:189 - Epoch 14000, Loss: 0.2807
2025-03-24 13:43:54,261 - cloned_fsc_actor_policy.py:190 - Epoch 14000, Accuracy: 0.8854
2025-03-24 13:43:57,345 - cloned_fsc_actor_policy.py:189 - Epoch 15000, Loss: 0.2779
2025-03-24 13:43:57,345 - cloned_fsc_actor_policy.py:190 - Epoch 15000, Accuracy: 0.8860
2025-03-24 13:43:57,375 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:44:13,459 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:44:13,459 - evaluators.py:130 - Average Virtual Goal Value = 16.320884704589844
2025-03-24 13:44:13,459 - evaluators.py:132 - Goal Reach Probability = 0.326417704011065
2025-03-24 13:44:13,459 - evaluators.py:134 - Trap Reach Probability = 0.17289073305670816
2025-03-24 13:44:13,459 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:44:13,459 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:44:13,459 - evaluators.py:140 - Current Best Reach Probability = 0.4827586206896552
2025-03-24 13:44:16,761 - cloned_fsc_actor_policy.py:189 - Epoch 16000, Loss: 0.2730
2025-03-24 13:44:16,761 - cloned_fsc_actor_policy.py:190 - Epoch 16000, Accuracy: 0.8877
2025-03-24 13:44:20,046 - cloned_fsc_actor_policy.py:189 - Epoch 17000, Loss: 0.2699
2025-03-24 13:44:20,047 - cloned_fsc_actor_policy.py:190 - Epoch 17000, Accuracy: 0.8896
2025-03-24 13:44:23,273 - cloned_fsc_actor_policy.py:189 - Epoch 18000, Loss: 0.2694
2025-03-24 13:44:23,273 - cloned_fsc_actor_policy.py:190 - Epoch 18000, Accuracy: 0.8895
2025-03-24 13:44:26,513 - cloned_fsc_actor_policy.py:189 - Epoch 19000, Loss: 0.2692
2025-03-24 13:44:26,513 - cloned_fsc_actor_policy.py:190 - Epoch 19000, Accuracy: 0.8898
2025-03-24 13:44:30,045 - cloned_fsc_actor_policy.py:189 - Epoch 20000, Loss: 0.2647
2025-03-24 13:44:30,045 - cloned_fsc_actor_policy.py:190 - Epoch 20000, Accuracy: 0.8915
2025-03-24 13:44:30,083 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:44:46,482 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:44:46,483 - evaluators.py:130 - Average Virtual Goal Value = 15.104894638061523
2025-03-24 13:44:46,483 - evaluators.py:132 - Goal Reach Probability = 0.3020979020979021
2025-03-24 13:44:46,483 - evaluators.py:134 - Trap Reach Probability = 0.16923076923076924
2025-03-24 13:44:46,483 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:44:46,483 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:44:46,483 - evaluators.py:140 - Current Best Reach Probability = 0.4827586206896552
2025-03-24 13:44:49,633 - cloned_fsc_actor_policy.py:189 - Epoch 21000, Loss: 0.2664
2025-03-24 13:44:49,633 - cloned_fsc_actor_policy.py:190 - Epoch 21000, Accuracy: 0.8906
2025-03-24 13:44:53,046 - cloned_fsc_actor_policy.py:189 - Epoch 22000, Loss: 0.2639
2025-03-24 13:44:53,046 - cloned_fsc_actor_policy.py:190 - Epoch 22000, Accuracy: 0.8916
2025-03-24 13:44:56,204 - cloned_fsc_actor_policy.py:189 - Epoch 23000, Loss: 0.2603
2025-03-24 13:44:56,204 - cloned_fsc_actor_policy.py:190 - Epoch 23000, Accuracy: 0.8928
2025-03-24 13:44:59,653 - cloned_fsc_actor_policy.py:189 - Epoch 24000, Loss: 0.2596
2025-03-24 13:44:59,654 - cloned_fsc_actor_policy.py:190 - Epoch 24000, Accuracy: 0.8922
2025-03-24 13:45:03,152 - cloned_fsc_actor_policy.py:189 - Epoch 25000, Loss: 0.2581
2025-03-24 13:45:03,152 - cloned_fsc_actor_policy.py:190 - Epoch 25000, Accuracy: 0.8924
2025-03-24 13:45:03,180 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:45:18,655 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:45:18,655 - evaluators.py:130 - Average Virtual Goal Value = 5.745062828063965
2025-03-24 13:45:18,655 - evaluators.py:132 - Goal Reach Probability = 0.11490125673249552
2025-03-24 13:45:18,655 - evaluators.py:134 - Trap Reach Probability = 0.11490125673249552
2025-03-24 13:45:18,655 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:45:18,656 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:45:18,656 - evaluators.py:140 - Current Best Reach Probability = 0.4827586206896552
2025-03-24 13:45:21,709 - cloned_fsc_actor_policy.py:189 - Epoch 26000, Loss: 0.2579
2025-03-24 13:45:21,709 - cloned_fsc_actor_policy.py:190 - Epoch 26000, Accuracy: 0.8919
2025-03-24 13:45:24,796 - cloned_fsc_actor_policy.py:189 - Epoch 27000, Loss: 0.2561
2025-03-24 13:45:24,796 - cloned_fsc_actor_policy.py:190 - Epoch 27000, Accuracy: 0.8922
2025-03-24 13:45:27,892 - cloned_fsc_actor_policy.py:189 - Epoch 28000, Loss: 0.2515
2025-03-24 13:45:27,892 - cloned_fsc_actor_policy.py:190 - Epoch 28000, Accuracy: 0.8937
2025-03-24 13:45:31,202 - cloned_fsc_actor_policy.py:189 - Epoch 29000, Loss: 0.2514
2025-03-24 13:45:31,202 - cloned_fsc_actor_policy.py:190 - Epoch 29000, Accuracy: 0.8931
2025-03-24 13:45:34,761 - cloned_fsc_actor_policy.py:189 - Epoch 30000, Loss: 0.2511
2025-03-24 13:45:34,762 - cloned_fsc_actor_policy.py:190 - Epoch 30000, Accuracy: 0.8936
2025-03-24 13:45:34,789 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:45:50,024 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:45:50,025 - evaluators.py:130 - Average Virtual Goal Value = 4.204753398895264
2025-03-24 13:45:50,025 - evaluators.py:132 - Goal Reach Probability = 0.08409506398537477
2025-03-24 13:45:50,025 - evaluators.py:134 - Trap Reach Probability = 0.113345521023766
2025-03-24 13:45:50,025 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:45:50,025 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:45:50,025 - evaluators.py:140 - Current Best Reach Probability = 0.4827586206896552
2025-03-24 13:45:53,099 - cloned_fsc_actor_policy.py:189 - Epoch 31000, Loss: 0.2505
2025-03-24 13:45:53,099 - cloned_fsc_actor_policy.py:190 - Epoch 31000, Accuracy: 0.8932
2025-03-24 13:45:56,218 - cloned_fsc_actor_policy.py:189 - Epoch 32000, Loss: 0.2494
2025-03-24 13:45:56,218 - cloned_fsc_actor_policy.py:190 - Epoch 32000, Accuracy: 0.8936
2025-03-24 13:45:59,408 - cloned_fsc_actor_policy.py:189 - Epoch 33000, Loss: 0.2475
2025-03-24 13:45:59,409 - cloned_fsc_actor_policy.py:190 - Epoch 33000, Accuracy: 0.8945
2025-03-24 13:46:02,503 - cloned_fsc_actor_policy.py:189 - Epoch 34000, Loss: 0.2486
2025-03-24 13:46:02,503 - cloned_fsc_actor_policy.py:190 - Epoch 34000, Accuracy: 0.8935
2025-03-24 13:46:05,596 - cloned_fsc_actor_policy.py:189 - Epoch 35000, Loss: 0.2466
2025-03-24 13:46:05,596 - cloned_fsc_actor_policy.py:190 - Epoch 35000, Accuracy: 0.8947
2025-03-24 13:46:05,623 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:46:21,845 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:46:21,846 - evaluators.py:130 - Average Virtual Goal Value = 4.667863368988037
2025-03-24 13:46:21,846 - evaluators.py:132 - Goal Reach Probability = 0.0933572710951526
2025-03-24 13:46:21,846 - evaluators.py:134 - Trap Reach Probability = 0.12567324955116696
2025-03-24 13:46:21,846 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:46:21,846 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:46:21,846 - evaluators.py:140 - Current Best Reach Probability = 0.4827586206896552
2025-03-24 13:46:25,088 - cloned_fsc_actor_policy.py:189 - Epoch 36000, Loss: 0.2458
2025-03-24 13:46:25,088 - cloned_fsc_actor_policy.py:190 - Epoch 36000, Accuracy: 0.8949
2025-03-24 13:46:28,592 - cloned_fsc_actor_policy.py:189 - Epoch 37000, Loss: 0.2430
2025-03-24 13:46:28,592 - cloned_fsc_actor_policy.py:190 - Epoch 37000, Accuracy: 0.8961
2025-03-24 13:46:31,730 - cloned_fsc_actor_policy.py:189 - Epoch 38000, Loss: 0.2455
2025-03-24 13:46:31,730 - cloned_fsc_actor_policy.py:190 - Epoch 38000, Accuracy: 0.8947
2025-03-24 13:46:34,943 - cloned_fsc_actor_policy.py:189 - Epoch 39000, Loss: 0.2442
2025-03-24 13:46:34,943 - cloned_fsc_actor_policy.py:190 - Epoch 39000, Accuracy: 0.8952
2025-03-24 13:46:38,671 - cloned_fsc_actor_policy.py:189 - Epoch 40000, Loss: 0.2414
2025-03-24 13:46:38,671 - cloned_fsc_actor_policy.py:190 - Epoch 40000, Accuracy: 0.8966
2025-03-24 13:46:38,698 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:46:54,264 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:46:54,265 - evaluators.py:130 - Average Virtual Goal Value = 4.293381214141846
2025-03-24 13:46:54,265 - evaluators.py:132 - Goal Reach Probability = 0.08586762075134168
2025-03-24 13:46:54,265 - evaluators.py:134 - Trap Reach Probability = 0.12880143112701253
2025-03-24 13:46:54,265 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:46:54,265 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:46:54,265 - evaluators.py:140 - Current Best Reach Probability = 0.4827586206896552
2025-03-24 13:46:57,494 - cloned_fsc_actor_policy.py:189 - Epoch 41000, Loss: 0.2426
2025-03-24 13:46:57,494 - cloned_fsc_actor_policy.py:190 - Epoch 41000, Accuracy: 0.8954
2025-03-24 13:47:00,653 - cloned_fsc_actor_policy.py:189 - Epoch 42000, Loss: 0.2453
2025-03-24 13:47:00,653 - cloned_fsc_actor_policy.py:190 - Epoch 42000, Accuracy: 0.8942
2025-03-24 13:47:03,826 - cloned_fsc_actor_policy.py:189 - Epoch 43000, Loss: 0.2422
2025-03-24 13:47:03,826 - cloned_fsc_actor_policy.py:190 - Epoch 43000, Accuracy: 0.8959
2025-03-24 13:47:06,969 - cloned_fsc_actor_policy.py:189 - Epoch 44000, Loss: 0.2413
2025-03-24 13:47:06,969 - cloned_fsc_actor_policy.py:190 - Epoch 44000, Accuracy: 0.8965
2025-03-24 13:47:10,126 - cloned_fsc_actor_policy.py:189 - Epoch 45000, Loss: 0.2401
2025-03-24 13:47:10,126 - cloned_fsc_actor_policy.py:190 - Epoch 45000, Accuracy: 0.8972
2025-03-24 13:47:10,152 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:47:25,692 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:47:25,692 - evaluators.py:130 - Average Virtual Goal Value = 5.200729846954346
2025-03-24 13:47:25,692 - evaluators.py:132 - Goal Reach Probability = 0.10401459854014598
2025-03-24 13:47:25,692 - evaluators.py:134 - Trap Reach Probability = 0.10766423357664233
2025-03-24 13:47:25,692 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:47:25,692 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:47:25,692 - evaluators.py:140 - Current Best Reach Probability = 0.4827586206896552
2025-03-24 13:47:28,847 - cloned_fsc_actor_policy.py:189 - Epoch 46000, Loss: 0.2395
2025-03-24 13:47:28,847 - cloned_fsc_actor_policy.py:190 - Epoch 46000, Accuracy: 0.8969
2025-03-24 13:47:32,108 - cloned_fsc_actor_policy.py:189 - Epoch 47000, Loss: 0.2405
2025-03-24 13:47:32,108 - cloned_fsc_actor_policy.py:190 - Epoch 47000, Accuracy: 0.8970
2025-03-24 13:47:35,309 - cloned_fsc_actor_policy.py:189 - Epoch 48000, Loss: 0.2382
2025-03-24 13:47:35,309 - cloned_fsc_actor_policy.py:190 - Epoch 48000, Accuracy: 0.8989
2025-03-24 13:47:38,527 - cloned_fsc_actor_policy.py:189 - Epoch 49000, Loss: 0.2374
2025-03-24 13:47:38,527 - cloned_fsc_actor_policy.py:190 - Epoch 49000, Accuracy: 0.8993
2025-03-24 13:47:41,621 - cloned_fsc_actor_policy.py:189 - Epoch 50000, Loss: 0.2372
2025-03-24 13:47:41,621 - cloned_fsc_actor_policy.py:190 - Epoch 50000, Accuracy: 0.8999
2025-03-24 13:47:41,652 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:47:56,702 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:47:56,703 - evaluators.py:130 - Average Virtual Goal Value = 24.949186325073242
2025-03-24 13:47:56,703 - evaluators.py:132 - Goal Reach Probability = 0.49898373983739835
2025-03-24 13:47:56,703 - evaluators.py:134 - Trap Reach Probability = 0.1758130081300813
2025-03-24 13:47:56,703 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:47:56,703 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:47:56,703 - evaluators.py:140 - Current Best Reach Probability = 0.49898373983739835
2025-03-24 13:47:59,792 - cloned_fsc_actor_policy.py:189 - Epoch 51000, Loss: 0.2342
2025-03-24 13:47:59,792 - cloned_fsc_actor_policy.py:190 - Epoch 51000, Accuracy: 0.9020
2025-03-24 13:48:02,931 - cloned_fsc_actor_policy.py:189 - Epoch 52000, Loss: 0.2343
2025-03-24 13:48:02,931 - cloned_fsc_actor_policy.py:190 - Epoch 52000, Accuracy: 0.9018
2025-03-24 13:48:06,028 - cloned_fsc_actor_policy.py:189 - Epoch 53000, Loss: 0.2345
2025-03-24 13:48:06,028 - cloned_fsc_actor_policy.py:190 - Epoch 53000, Accuracy: 0.9022
2025-03-24 13:48:09,164 - cloned_fsc_actor_policy.py:189 - Epoch 54000, Loss: 0.2335
2025-03-24 13:48:09,164 - cloned_fsc_actor_policy.py:190 - Epoch 54000, Accuracy: 0.9043
2025-03-24 13:48:12,342 - cloned_fsc_actor_policy.py:189 - Epoch 55000, Loss: 0.2319
2025-03-24 13:48:12,342 - cloned_fsc_actor_policy.py:190 - Epoch 55000, Accuracy: 0.9056
2025-03-24 13:48:12,373 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:48:28,117 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:48:28,117 - evaluators.py:130 - Average Virtual Goal Value = 4.1894354820251465
2025-03-24 13:48:28,117 - evaluators.py:132 - Goal Reach Probability = 0.08378870673952642
2025-03-24 13:48:28,117 - evaluators.py:134 - Trap Reach Probability = 0.13296903460837886
2025-03-24 13:48:28,118 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:48:28,118 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:48:28,118 - evaluators.py:140 - Current Best Reach Probability = 0.49898373983739835
2025-03-24 13:48:31,195 - cloned_fsc_actor_policy.py:189 - Epoch 56000, Loss: 0.2324
2025-03-24 13:48:31,195 - cloned_fsc_actor_policy.py:190 - Epoch 56000, Accuracy: 0.9053
2025-03-24 13:48:34,350 - cloned_fsc_actor_policy.py:189 - Epoch 57000, Loss: 0.2304
2025-03-24 13:48:34,351 - cloned_fsc_actor_policy.py:190 - Epoch 57000, Accuracy: 0.9063
2025-03-24 13:48:37,477 - cloned_fsc_actor_policy.py:189 - Epoch 58000, Loss: 0.2299
2025-03-24 13:48:37,478 - cloned_fsc_actor_policy.py:190 - Epoch 58000, Accuracy: 0.9057
2025-03-24 13:48:41,194 - cloned_fsc_actor_policy.py:189 - Epoch 59000, Loss: 0.2295
2025-03-24 13:48:41,194 - cloned_fsc_actor_policy.py:190 - Epoch 59000, Accuracy: 0.9057
2025-03-24 13:48:44,308 - attrs.py:78 - Creating converter from 3 to 5
2025-03-24 13:48:44,553 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 13:48:59,840 - evaluators.py:128 - Average Return = 0.0
2025-03-24 13:48:59,840 - evaluators.py:130 - Average Virtual Goal Value = 25.121240615844727
2025-03-24 13:48:59,840 - evaluators.py:132 - Goal Reach Probability = 0.5024248302618817
2025-03-24 13:48:59,840 - evaluators.py:134 - Trap Reach Probability = 0.18040737148399613
2025-03-24 13:48:59,840 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 13:48:59,840 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 13:48:59,840 - evaluators.py:140 - Current Best Reach Probability = 0.5024248302618817
2025-03-24 13:48:59,849 - pomdp.py:349 - unfolding 9-FSC template into POMDP...
2025-03-24 13:49:10,970 - pomdp.py:355 - constructed quotient MDP having 408519 states and 14391756 actions.
2025-03-24 13:49:28,339 - rl_family_extractor.py:329 - Building family from FFNN...
2025-03-24 13:49:28,350 - rl_family_extractor.py:357 - Number of misses: 8 out of 99
2025-03-24 13:49:28,350 - rl_family_extractor.py:359 - Number of complete misses: 8 out of 99
2025-03-24 13:49:28,350 - statistic.py:67 - synthesis initiated, design space: 65536
> progress 0.015%, elapsed 3 s, estimated 21274 s (5 hours), iters = {DTMC: 11}, opt = 0.5362
> progress 0.032%, elapsed 6 s, estimated 20386 s (5 hours), iters = {DTMC: 22}, opt = 0.5362
> progress 0.048%, elapsed 9 s, estimated 20084 s (5 hours), iters = {DTMC: 33}, opt = 0.5362
> progress 0.064%, elapsed 12 s, estimated 20250 s (5 hours), iters = {DTMC: 43}, opt = 0.5362
> progress 0.079%, elapsed 15 s, estimated 20148 s (5 hours), iters = {DTMC: 53}, opt = 0.5362
> progress 0.096%, elapsed 19 s, estimated 20054 s (5 hours), iters = {DTMC: 64}, opt = 0.5362
> progress 0.111%, elapsed 22 s, estimated 20012 s (5 hours), iters = {DTMC: 74}, opt = 0.5362
> progress 0.126%, elapsed 25 s, estimated 19983 s (5 hours), iters = {DTMC: 84}, opt = 0.5362
> progress 0.141%, elapsed 28 s, estimated 19952 s (5 hours), iters = {DTMC: 94}, opt = 0.5362
> progress 0.158%, elapsed 31 s, estimated 19916 s (5 hours), iters = {DTMC: 105}, opt = 0.5362
> progress 0.173%, elapsed 34 s, estimated 19894 s (5 hours), iters = {DTMC: 115}, opt = 0.5362
> progress 0.189%, elapsed 37 s, estimated 19888 s (5 hours), iters = {DTMC: 125}, opt = 0.5362
> progress 0.204%, elapsed 40 s, estimated 19874 s (5 hours), iters = {DTMC: 135}, opt = 0.5362
> progress 0.219%, elapsed 43 s, estimated 19861 s (5 hours), iters = {DTMC: 145}, opt = 0.5362
> progress 0.236%, elapsed 46 s, estimated 19845 s (5 hours), iters = {DTMC: 156}, opt = 0.5362
> progress 0.253%, elapsed 50 s, estimated 19830 s (5 hours), iters = {DTMC: 167}, opt = 0.5362
> progress 0.27%, elapsed 53 s, estimated 19808 s (5 hours), iters = {DTMC: 178}, opt = 0.5362
> progress 0.286%, elapsed 56 s, estimated 19795 s (5 hours), iters = {DTMC: 189}, opt = 0.5362
> progress 0.303%, elapsed 60 s, estimated 19779 s (5 hours), iters = {DTMC: 200}, opt = 0.5362
2025-03-24 13:50:28,495 - synthesizer_onebyone.py:19 - Time limit reached
2025-03-24 13:50:28,496 - synthesizer.py:192 - printing synthesized assignment below:
2025-03-24 13:50:28,496 - synthesizer.py:193 - A([done=0	& obsReady=1	& see=1],0)=u, A([done=0	& obsReady=1	& see=1],1)=r, A([done=0	& obsReady=1	& see=1],2)=u, A([done=0	& obsReady=1	& see=1],3)=u, A([done=0	& obsReady=1	& see=1],4)=r, A([done=0	& obsReady=1	& see=1],5)=u, A([done=0	& obsReady=1	& see=1],6)=u, A([done=0	& obsReady=1	& see=1],7)=d, A([done=0	& obsReady=1	& see=1],8)=u, M([done=0	& obsReady=1	& see=1],0)=6, M([done=0	& obsReady=1	& see=1],1)=6, M([done=0	& obsReady=1	& see=1],2)=6, M([done=0	& obsReady=1	& see=1],3)=0, M([done=0	& obsReady=1	& see=1],4)=0, M([done=0	& obsReady=1	& see=1],5)=2, M([done=0	& obsReady=1	& see=1],6)=6, M([done=0	& obsReady=1	& see=1],7)=6, M([done=0	& obsReady=1	& see=1],8)=6, A([done=0	& obsReady=1	& see=0],0)=d, A([done=0	& obsReady=1	& see=0],1)=r, A([done=0	& obsReady=1	& see=0],2)=r, A([done=0	& obsReady=1	& see=0],3)=d, A([done=0	& obsReady=1	& see=0],4)=d, A([done=0	& obsReady=1	& see=0],5)=d, A([done=0	& obsReady=1	& see=0],6)=r, A([done=0	& obsReady=1	& see=0],7)=d, A([done=0	& obsReady=1	& see=0],8)=r, M([done=0	& obsReady=1	& see=0],0)=0, M([done=0	& obsReady=1	& see=0],1)=0, M([done=0	& obsReady=1	& see=0],2)=0, M([done=0	& obsReady=1	& see=0],3)=3, M([done=0	& obsReady=1	& see=0],4)=3, M([done=0	& obsReady=1	& see=0],5)=3, M([done=0	& obsReady=1	& see=0],6)=0, M([done=0	& obsReady=1	& see=0],7)=1, M([done=0	& obsReady=1	& see=0],8)=0, A([done=1	& obsReady=1	& see=0],0)=d, A([done=1	& obsReady=1	& see=0],1)=d, A([done=1	& obsReady=1	& see=0],2)=d, A([done=1	& obsReady=1	& see=0],3)=d, A([done=1	& obsReady=1	& see=0],4)=d, A([done=1	& obsReady=1	& see=0],5)=d, A([done=1	& obsReady=1	& see=0],6)=d, A([done=1	& obsReady=1	& see=0],7)=d, A([done=1	& obsReady=1	& see=0],8)=d, M([done=1	& obsReady=1	& see=0],0)=3, M([done=1	& obsReady=1	& see=0],1)=3, M([done=1	& obsReady=1	& see=0],2)=3, M([done=1	& obsReady=1	& see=0],3)=3, M([done=1	& obsReady=1	& see=0],4)=3, M([done=1	& obsReady=1	& see=0],5)=3, M([done=1	& obsReady=1	& see=0],6)=3, M([done=1	& obsReady=1	& see=0],7)=4, M([done=1	& obsReady=1	& see=0],8)=3, A([done=1	& obsReady=1	& see=1],0)=l, A([done=1	& obsReady=1	& see=1],1)=l, A([done=1	& obsReady=1	& see=1],2)=l, A([done=1	& obsReady=1	& see=1],3)=l, A([done=1	& obsReady=1	& see=1],4)=l, A([done=1	& obsReady=1	& see=1],5)=l, A([done=1	& obsReady=1	& see=1],6)=l, A([done=1	& obsReady=1	& see=1],7)=d, A([done=1	& obsReady=1	& see=1],8)=l, M([done=1	& obsReady=1	& see=1],0)=5, M([done=1	& obsReady=1	& see=1],1)=5, M([done=1	& obsReady=1	& see=1],2)=5, M([done=1	& obsReady=1	& see=1],3)=5, M([done=1	& obsReady=1	& see=1],4)=5, M([done=1	& obsReady=1	& see=1],5)=5, M([done=1	& obsReady=1	& see=1],6)=2, M([done=1	& obsReady=1	& see=1],7)=2, M([done=1	& obsReady=1	& see=1],8)=2, M([done=0	& obsReady=0	& see=0],0)=5, M([done=0	& obsReady=0	& see=0],1)=3, M([done=0	& obsReady=0	& see=0],2)=5, M([done=0	& obsReady=0	& see=0],3)=5, M([done=0	& obsReady=0	& see=0],4)=5, M([done=0	& obsReady=0	& see=0],5)=5, M([done=0	& obsReady=0	& see=0],6)=2, M([done=0	& obsReady=0	& see=0],7)=0, M([done=0	& obsReady=0	& see=0],8)=2, M([done=0	& obsReady=0	& see=1],0)=8, M([done=0	& obsReady=0	& see=1],1)=8, M([done=0	& obsReady=0	& see=1],2)=8, M([done=0	& obsReady=0	& see=1],3)=8, M([done=0	& obsReady=0	& see=1],4)=8, M([done=0	& obsReady=0	& see=1],5)=8, M([done=0	& obsReady=0	& see=1],6)=8, M([done=0	& obsReady=0	& see=1],7)=8, M([done=0	& obsReady=0	& see=1],8)=8, M([done=1	& obsReady=0	& see=1],0)=5, M([done=1	& obsReady=0	& see=1],1)=5, M([done=1	& obsReady=0	& see=1],2)=5, M([done=1	& obsReady=0	& see=1],3)=5, M([done=1	& obsReady=0	& see=1],4)=5, M([done=1	& obsReady=0	& see=1],5)=5, M([done=1	& obsReady=0	& see=1],6)=2, M([done=1	& obsReady=0	& see=1],7)=2, M([done=1	& obsReady=0	& see=1],8)=2
2025-03-24 13:50:28,807 - synthesizer.py:198 - double-checking specification satisfiability:  : 0.5362178297694254
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.15 s
number of holes: 99, family size: 1e81, quotient: 408519 states / 14391756 actions
explored: 0 %
DTMC stats: avg DTMC size: 63990, iterations: 200

optimum: 0.536218
--------------------
2025-03-24 13:50:28,808 - statistic.py:67 - synthesis initiated, design space: 1e81
> progress 0.0%, elapsed 3 s, estimated 880860480378280984009756397081426597068722833228181762731127171902511481756844032 s (27931902599514238509585549931275341835395161604901368669776028264290058240 years), iters = {DTMC: 23}, opt = 0.5362
> progress 0.0%, elapsed 6 s, estimated 863325945711678035928447233158445260026218255134227075867075751432384582894223360 s (27375886152704152763343570060852322329585948314888830805455242553764872192 years), iters = {DTMC: 45}, opt = 0.5362
> progress 0.0%, elapsed 9 s, estimated 859337911777330290741379243184938980497665770502272822329898991582787751265697792 s (27249426426221786535777581756986922873487757595367702829494178955053236224 years), iters = {DTMC: 67}, opt = 0.5362
> progress 0.0%, elapsed 12 s, estimated 856662756830111553309075625223087240715675454502816750316563772076669118418255872 s (27164597819321142836596297077569349485364169232921442736720732164030726144 years), iters = {DTMC: 89}, opt = 0.5362
> progress 0.0%, elapsed 15 s, estimated 865085121370191373251604433185128216735226672062002377057357624432467942980976640 s (27431669246898509891150391797907617487517160456902108264629169093192187904 years), iters = {DTMC: 110}, opt = 0.5362
> progress 0.0%, elapsed 18 s, estimated 869806789958613463659273168514890039530583901669329302927302355720128577937604608 s (27581392375653645997464156128960299091306911811400782196361078559897288704 years), iters = {DTMC: 131}, opt = 0.5362
> progress 0.0%, elapsed 21 s, estimated 866649436519403700345477960784882848260632689201931437190242297940364329373138944 s (27481273354877079511135269117556907123530213163206791343445317728352600064 years), iters = {DTMC: 153}, opt = 0.5362
> progress 0.0%, elapsed 24 s, estimated 865031125612512328427840017712587536899099624288294974364444383659917710710538240 s (27429957052654499812830644982832125837110251731114096875484558324003766272 years), iters = {DTMC: 175}, opt = 0.5362
> progress 0.0%, elapsed 27 s, estimated 863618735890225020576290165702900470790084375257113925761558037009891650810413056 s (27385170468360765416403878499808414148638521045918581484749671805078208512 years), iters = {DTMC: 197}, opt = 0.5362
> progress 0.0%, elapsed 30 s, estimated 861857018099225421001636488474047624046963798166879633961797605313486804760920064 s (27329306763674067823153103425003966662731196118868191284703569799932805120 years), iters = {DTMC: 219}, opt = 0.5362
> progress 0.0%, elapsed 33 s, estimated 860211377857564795093944152515335166539508500917273119931524345740357800067858432 s (27277123853930894031604974672429647974452747455373657865601889271023665152 years), iters = {DTMC: 241}, opt = 0.5362
> progress 0.0%, elapsed 36 s, estimated 859234883531210951484572733918210769816775037787022671895103175109794458903773184 s (27246159421968886730646601850470271885587945470490137563938503433016836096 years), iters = {DTMC: 263}, opt = 0.5362
> progress 0.0%, elapsed 39 s, estimated 858179403614849294514311683927244086160660785312278057918238171577493991402242048 s (27212690373378022378248198532175794629572289224183583309982507396925227008 years), iters = {DTMC: 285}, opt = 0.5362
> progress 0.0%, elapsed 42 s, estimated 858136133985300132281499356669931835706725516074332186437365350163075095016243200 s (27211318302425803309402437880909959641514607650366933594132959054528512000 years), iters = {DTMC: 307}, opt = 0.5362
> progress 0.0%, elapsed 45 s, estimated 857832865294344931600293075757268914322263234221890010749385503273055641153830912 s (27201701715320424264426139601479406003637558711461829950763555652907302912 years), iters = {DTMC: 329}, opt = 0.5362
> progress 0.0%, elapsed 48 s, estimated 857289365657597824210060098629746325776853238588448991316995199862266601253371904 s (27184467454895924950673823180978620219292271617441675481769170511341813760 years), iters = {DTMC: 351}, opt = 0.5362
> progress 0.0%, elapsed 51 s, estimated 858382615451953364366924440068071489467877652476054531952550112191742754873671680 s (27219134178461228593425571975436761843031613126128772138189873943486136320 years), iters = {DTMC: 373}, opt = 0.5362
> progress 0.0%, elapsed 54 s, estimated 857609433152602233039980857827468375648233037300648951363981348603382136500649984 s (27194616728583277008034135092599925274900106831719302569889386578558582784 years), iters = {DTMC: 395}, opt = 0.5362
> progress 0.0%, elapsed 57 s, estimated 856776825575289604872783138109473319082388612892552705587689534687409067266146304 s (27168214915502585884155990198143116058253323097225906079298202166221602816 years), iters = {DTMC: 418}, opt = 0.5362
2025-03-24 13:51:28,891 - synthesizer_onebyone.py:19 - Time limit reached
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.08 s
number of holes: 99, family size: 1e81, quotient: 408519 states / 14391756 actions
explored: 0 %
DTMC stats: avg DTMC size: 8047, iterations: 435

optimum: 0.536218
--------------------
2025-03-24 13:51:28,891 - synthesizer_rl_storm_paynt.py:274 - No improving assignment found.
2025-03-24 13:59:50,426 - storm_pomdp_control.py:246 - Interactive Storm resumed
2025-03-24 13:59:50,426 - storm_pomdp_control.py:291 - Updating FSC values in Storm
2025-03-24 14:00:11,453 - storm_pomdp_control.py:305 - Pausing Storm
-----------Storm-----------               
Value = 0.6167936701851813 | Time elapsed = 2032.3s | FSC size = 10817

2025-03-24 14:01:31,164 - storm_pomdp_control.py:912 - constructed FSC with 2438 nodes
2025-03-24 14:01:31,872 - synthesizer_pomdp.py:240 - Timeout for PAYNT started
2025-03-24 14:01:32,144 - synthesizer_ar_storm.py:136 - Resuming synthesis
2025-03-24 14:01:32,149 - synthesizer_ar_storm.py:143 - Applying family split according to Storm results
2025-03-24 14:01:32,153 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e10 to 1e10
2025-03-24 14:01:32,153 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e10 to 1e10
2025-03-24 14:01:32,154 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e10 to 1e10
2025-03-24 14:01:32,154 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e9 to 1e9
2025-03-24 14:01:32,155 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e9 to 1e9
2025-03-24 14:01:32,156 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e9 to 1e9
2025-03-24 14:01:32,156 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e9 to 1e9
2025-03-24 14:01:32,157 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e9 to 1e9
2025-03-24 14:01:32,157 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e9 to 1e9
2025-03-24 14:01:32,158 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e8 to 1e8
2025-03-24 14:01:32,159 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e8 to 1e8
2025-03-24 14:01:32,159 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e7 to 1e7
2025-03-24 14:01:32,159 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e7 to 1e7
2025-03-24 14:01:32,160 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e7 to 1e7
2025-03-24 14:01:32,160 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e7 to 1e7
2025-03-24 14:01:32,161 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e7 to 1e7
2025-03-24 14:01:32,161 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e7 to 1e7
2025-03-24 14:01:32,162 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e6 to 1e6
2025-03-24 14:01:32,162 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e6 to 1e6
2025-03-24 14:01:32,163 - storm_pomdp_control.py:589 - Main family based on data from Storm: reduced design space from 1e6 to 1e6
2025-03-24 14:01:32,163 - synthesizer_ar_storm.py:57 - State after Storm splitting: Main families - 20, Subfamilies - 0
2025-03-24 14:01:52,927 - synthesizer_ar_storm.py:128 - Pausing synthesis
2025-03-24 14:01:52,961 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:02:00,311 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:02:00,311 - father_agent.py:564 - Average Virtual Goal Value = 29.440126419067383
2025-03-24 14:02:00,311 - father_agent.py:566 - Goal Reach Probability = 0.5888025331030512
2025-03-24 14:02:00,311 - father_agent.py:568 - Trap Reach Probability = 0.41119746689694875
2025-03-24 14:02:00,311 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:02:00,311 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:02:00,311 - father_agent.py:574 - Current Best Reach Probability = 0.5894498656484232
2025-03-24 14:02:04,225 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:02:08,223 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:02:08,223 - evaluators.py:130 - Average Virtual Goal Value = 12.6126127243042
2025-03-24 14:02:08,223 - evaluators.py:132 - Goal Reach Probability = 0.25225225225225223
2025-03-24 14:02:08,224 - evaluators.py:134 - Trap Reach Probability = 0.7477477477477478
2025-03-24 14:02:08,224 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:02:08,224 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:02:08,224 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:02:08,224 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 0, Actor Loss: 1.9890742301940918
Epoch: 0, Critic Loss: 0.006070801056921482
Epoch: 10, Actor Loss: 0.04833894595503807
Epoch: 10, Critic Loss: 0.0025235055945813656
Epoch: 20, Actor Loss: 0.02286146581172943
Epoch: 20, Critic Loss: 0.002138851210474968
Epoch: 30, Actor Loss: 0.02892489731311798
Epoch: 30, Critic Loss: 1.617537498474121
Epoch: 40, Actor Loss: 0.035225700587034225
Epoch: 40, Critic Loss: 0.008577569387853146
2025-03-24 14:02:40,419 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:02:43,922 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:02:43,922 - evaluators.py:130 - Average Virtual Goal Value = 5.529411792755127
2025-03-24 14:02:43,922 - evaluators.py:132 - Goal Reach Probability = 0.11058823529411765
2025-03-24 14:02:43,922 - evaluators.py:134 - Trap Reach Probability = 0.8894117647058823
2025-03-24 14:02:43,922 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:02:43,922 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:02:43,922 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:02:43,923 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 50, Actor Loss: 0.05230595916509628
Epoch: 50, Critic Loss: 0.005971296224743128
Epoch: 60, Actor Loss: 0.049824513494968414
Epoch: 60, Critic Loss: 0.013372215442359447
Epoch: 70, Actor Loss: 0.015041017904877663
Epoch: 70, Critic Loss: 1.413888692855835
Epoch: 80, Actor Loss: 0.07093220204114914
Epoch: 80, Critic Loss: 3.044724225997925
Epoch: 90, Actor Loss: 0.056712809950113297
Epoch: 90, Critic Loss: 1.3295009136199951
2025-03-24 14:03:16,943 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:03:20,461 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:03:20,461 - evaluators.py:130 - Average Virtual Goal Value = 10.88888931274414
2025-03-24 14:03:20,461 - evaluators.py:132 - Goal Reach Probability = 0.21777777777777776
2025-03-24 14:03:20,461 - evaluators.py:134 - Trap Reach Probability = 0.7822222222222223
2025-03-24 14:03:20,461 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:03:20,461 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:03:20,461 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:03:20,461 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 100, Actor Loss: 0.05974876508116722
Epoch: 100, Critic Loss: 0.24730193614959717
Epoch: 110, Actor Loss: 0.040994394570589066
Epoch: 110, Critic Loss: 3.500300168991089
Epoch: 120, Actor Loss: 0.06471855193376541
Epoch: 120, Critic Loss: 1.7878247499465942
Epoch: 130, Actor Loss: 0.016187302768230438
Epoch: 130, Critic Loss: 1.127527117729187
Epoch: 140, Actor Loss: 0.09459073841571808
Epoch: 140, Critic Loss: 1.538394570350647
2025-03-24 14:03:53,059 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:03:56,597 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:03:56,597 - evaluators.py:130 - Average Virtual Goal Value = 6.741572856903076
2025-03-24 14:03:56,597 - evaluators.py:132 - Goal Reach Probability = 0.1348314606741573
2025-03-24 14:03:56,597 - evaluators.py:134 - Trap Reach Probability = 0.8651685393258427
2025-03-24 14:03:56,597 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:03:56,597 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:03:56,597 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:03:56,598 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 150, Actor Loss: 0.015473032370209694
Epoch: 150, Critic Loss: 0.1617516428232193
Epoch: 160, Actor Loss: 0.06062047928571701
Epoch: 160, Critic Loss: 0.24073053896427155
Epoch: 170, Actor Loss: 0.015824608504772186
Epoch: 170, Critic Loss: 0.12697568535804749
Epoch: 180, Actor Loss: 0.06726555526256561
Epoch: 180, Critic Loss: 5.408349990844727
Epoch: 190, Actor Loss: 0.054032664746046066
Epoch: 190, Critic Loss: 1.9526052474975586
2025-03-24 14:04:28,929 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:04:32,302 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:04:32,302 - evaluators.py:130 - Average Virtual Goal Value = 7.09779167175293
2025-03-24 14:04:32,302 - evaluators.py:132 - Goal Reach Probability = 0.14195583596214512
2025-03-24 14:04:32,302 - evaluators.py:134 - Trap Reach Probability = 0.8580441640378549
2025-03-24 14:04:32,302 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:04:32,302 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:04:32,302 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:04:32,302 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 200, Actor Loss: 0.0851205438375473
Epoch: 200, Critic Loss: 1.9202020168304443
Epoch: 210, Actor Loss: 0.026417184621095657
Epoch: 210, Critic Loss: 0.07664957642555237
Epoch: 220, Actor Loss: 0.005104792769998312
Epoch: 220, Critic Loss: 0.019754022359848022
Epoch: 230, Actor Loss: 0.0023916412610560656
Epoch: 230, Critic Loss: 0.00022543803788721561
Epoch: 240, Actor Loss: 0.0544934943318367
Epoch: 240, Critic Loss: 4.240952968597412
2025-03-24 14:05:04,608 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:05:07,950 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:05:07,950 - evaluators.py:130 - Average Virtual Goal Value = 12.991453170776367
2025-03-24 14:05:07,950 - evaluators.py:132 - Goal Reach Probability = 0.25982905982905985
2025-03-24 14:05:07,950 - evaluators.py:134 - Trap Reach Probability = 0.7401709401709402
2025-03-24 14:05:07,950 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:05:07,950 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:05:07,950 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:05:07,951 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 250, Actor Loss: 0.025590805336833
Epoch: 250, Critic Loss: 1.8230788707733154
Epoch: 260, Actor Loss: 0.027515077963471413
Epoch: 260, Critic Loss: 1.976584553718567
Epoch: 270, Actor Loss: 0.006831069011241198
Epoch: 270, Critic Loss: 0.005523008294403553
Epoch: 280, Actor Loss: 0.11384473741054535
Epoch: 280, Critic Loss: 0.22681674361228943
Epoch: 290, Actor Loss: 0.041244689375162125
Epoch: 290, Critic Loss: 2.6523261070251465
2025-03-24 14:05:41,277 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:05:44,613 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:05:44,613 - evaluators.py:130 - Average Virtual Goal Value = 13.569321632385254
2025-03-24 14:05:44,613 - evaluators.py:132 - Goal Reach Probability = 0.2713864306784661
2025-03-24 14:05:44,613 - evaluators.py:134 - Trap Reach Probability = 0.7286135693215339
2025-03-24 14:05:44,613 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:05:44,613 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:05:44,613 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:05:44,613 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 300, Actor Loss: 0.0026067993603646755
Epoch: 300, Critic Loss: 0.0024067300837486982
Epoch: 310, Actor Loss: 0.06381871551275253
Epoch: 310, Critic Loss: 1.7926406860351562
Epoch: 320, Actor Loss: 0.020971298217773438
Epoch: 320, Critic Loss: 1.5223859548568726
Epoch: 330, Actor Loss: 0.013136709108948708
Epoch: 330, Critic Loss: 1.3883442878723145
Epoch: 340, Actor Loss: 0.0071115875616669655
Epoch: 340, Critic Loss: 0.03167198598384857
2025-03-24 14:06:16,732 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:06:20,122 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:06:20,123 - evaluators.py:130 - Average Virtual Goal Value = 2.3809523582458496
2025-03-24 14:06:20,123 - evaluators.py:132 - Goal Reach Probability = 0.047619047619047616
2025-03-24 14:06:20,123 - evaluators.py:134 - Trap Reach Probability = 0.9523809523809523
2025-03-24 14:06:20,123 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:06:20,123 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:06:20,123 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:06:20,123 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 350, Actor Loss: 0.014969672076404095
Epoch: 350, Critic Loss: 0.09783954918384552
Epoch: 360, Actor Loss: 0.01955265924334526
Epoch: 360, Critic Loss: 0.05860017240047455
Epoch: 370, Actor Loss: 0.020576925948262215
Epoch: 370, Critic Loss: 2.2237980365753174
Epoch: 380, Actor Loss: 0.12898437678813934
Epoch: 380, Critic Loss: 2.2862348556518555
Epoch: 390, Actor Loss: 0.012102434411644936
Epoch: 390, Critic Loss: 0.06923385709524155
2025-03-24 14:06:52,583 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:06:55,778 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:06:55,778 - evaluators.py:130 - Average Virtual Goal Value = 12.104072570800781
2025-03-24 14:06:55,778 - evaluators.py:132 - Goal Reach Probability = 0.2420814479638009
2025-03-24 14:06:55,778 - evaluators.py:134 - Trap Reach Probability = 0.7579185520361991
2025-03-24 14:06:55,778 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:06:55,778 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:06:55,778 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:06:55,778 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 400, Actor Loss: 0.03815038129687309
Epoch: 400, Critic Loss: 1.3784422874450684
Epoch: 410, Actor Loss: 0.006808297708630562
Epoch: 410, Critic Loss: 0.014468677341938019
Epoch: 420, Actor Loss: 0.03915230929851532
Epoch: 420, Critic Loss: 0.23106186091899872
Epoch: 430, Actor Loss: 0.015860410407185555
Epoch: 430, Critic Loss: 0.05956104397773743
Epoch: 440, Actor Loss: 0.011907348409295082
Epoch: 440, Critic Loss: 0.07773011922836304
2025-03-24 14:07:28,149 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:07:31,417 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:07:31,417 - evaluators.py:130 - Average Virtual Goal Value = 8.625
2025-03-24 14:07:31,417 - evaluators.py:132 - Goal Reach Probability = 0.1725
2025-03-24 14:07:31,417 - evaluators.py:134 - Trap Reach Probability = 0.8275
2025-03-24 14:07:31,417 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:07:31,417 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:07:31,417 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:07:31,417 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 450, Actor Loss: 0.04976838082075119
Epoch: 450, Critic Loss: 1.2098311185836792
Epoch: 460, Actor Loss: 0.01693839766085148
Epoch: 460, Critic Loss: 0.03970944881439209
Epoch: 470, Actor Loss: 0.1124410405755043
Epoch: 470, Critic Loss: 5.31393575668335
Epoch: 480, Actor Loss: 0.06191166862845421
Epoch: 480, Critic Loss: 2.230653762817383
Epoch: 490, Actor Loss: 0.023467086255550385
Epoch: 490, Critic Loss: 2.069145441055298
2025-03-24 14:08:03,538 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:08:06,824 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:08:06,824 - evaluators.py:130 - Average Virtual Goal Value = 9.191176414489746
2025-03-24 14:08:06,824 - evaluators.py:132 - Goal Reach Probability = 0.18382352941176472
2025-03-24 14:08:06,824 - evaluators.py:134 - Trap Reach Probability = 0.8161764705882353
2025-03-24 14:08:06,824 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:08:06,824 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:08:06,824 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:08:06,824 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 500, Actor Loss: 0.01963532529771328
Epoch: 500, Critic Loss: 0.23448118567466736
Epoch: 510, Actor Loss: 0.037296708673238754
Epoch: 510, Critic Loss: 2.0864133834838867
Epoch: 520, Actor Loss: 0.0401960089802742
Epoch: 520, Critic Loss: 1.5303809642791748
Epoch: 530, Actor Loss: 0.03971796855330467
Epoch: 530, Critic Loss: 0.2545740008354187
Epoch: 540, Actor Loss: 0.028760172426700592
Epoch: 540, Critic Loss: 3.78005313873291
2025-03-24 14:08:38,590 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:08:41,815 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:08:41,815 - evaluators.py:130 - Average Virtual Goal Value = 11.985018730163574
2025-03-24 14:08:41,815 - evaluators.py:132 - Goal Reach Probability = 0.2397003745318352
2025-03-24 14:08:41,815 - evaluators.py:134 - Trap Reach Probability = 0.7602996254681648
2025-03-24 14:08:41,815 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:08:41,815 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:08:41,815 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:08:41,815 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 550, Actor Loss: 0.02381727658212185
Epoch: 550, Critic Loss: 1.3350651264190674
Epoch: 560, Actor Loss: 0.077308289706707
Epoch: 560, Critic Loss: 3.015836715698242
Epoch: 570, Actor Loss: 0.040026720613241196
Epoch: 570, Critic Loss: 2.8650546073913574
Epoch: 580, Actor Loss: 0.02806505747139454
Epoch: 580, Critic Loss: 1.584409475326538
Epoch: 590, Actor Loss: 0.1044134721159935
Epoch: 590, Critic Loss: 4.460859298706055
2025-03-24 14:09:13,928 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:09:17,275 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:09:17,275 - evaluators.py:130 - Average Virtual Goal Value = 11.795774459838867
2025-03-24 14:09:17,275 - evaluators.py:132 - Goal Reach Probability = 0.23591549295774647
2025-03-24 14:09:17,275 - evaluators.py:134 - Trap Reach Probability = 0.7640845070422535
2025-03-24 14:09:17,275 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:09:17,275 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:09:17,275 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:09:17,275 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 600, Actor Loss: 0.040069472044706345
Epoch: 600, Critic Loss: 2.175525426864624
Epoch: 610, Actor Loss: 0.04253791272640228
Epoch: 610, Critic Loss: 2.2195234298706055
Epoch: 620, Actor Loss: 0.01037533674389124
Epoch: 620, Critic Loss: 0.11359133571386337
Epoch: 630, Actor Loss: 0.11014293879270554
Epoch: 630, Critic Loss: 2.978205919265747
Epoch: 640, Actor Loss: 0.01464485377073288
Epoch: 640, Critic Loss: 1.9589507579803467
2025-03-24 14:09:49,598 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:09:52,949 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:09:52,949 - evaluators.py:130 - Average Virtual Goal Value = 9.348442077636719
2025-03-24 14:09:52,949 - evaluators.py:132 - Goal Reach Probability = 0.18696883852691218
2025-03-24 14:09:52,949 - evaluators.py:134 - Trap Reach Probability = 0.8130311614730878
2025-03-24 14:09:52,949 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:09:52,950 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:09:52,950 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:09:52,950 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 650, Actor Loss: 0.07522653788328171
Epoch: 650, Critic Loss: 5.756641387939453
Epoch: 660, Actor Loss: 0.02967289835214615
Epoch: 660, Critic Loss: 1.5284535884857178
Epoch: 670, Actor Loss: 0.018595129251480103
Epoch: 670, Critic Loss: 1.3091037273406982
Epoch: 680, Actor Loss: 0.07881000638008118
Epoch: 680, Critic Loss: 0.9702168703079224
Epoch: 690, Actor Loss: 0.04555286467075348
Epoch: 690, Critic Loss: 0.29467540979385376
2025-03-24 14:10:24,734 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:10:27,970 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:10:27,970 - evaluators.py:130 - Average Virtual Goal Value = 8.7890625
2025-03-24 14:10:27,971 - evaluators.py:132 - Goal Reach Probability = 0.17578125
2025-03-24 14:10:27,971 - evaluators.py:134 - Trap Reach Probability = 0.82421875
2025-03-24 14:10:27,971 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:10:27,971 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:10:27,971 - evaluators.py:140 - Current Best Reach Probability = 0.3357142857142857
2025-03-24 14:10:27,971 - environment_wrapper_vec.py:365 - Resetting the environment.
Epoch: 700, Actor Loss: 0.0038263187743723392
Epoch: 700, Critic Loss: 0.06551328301429749
2025-03-24 14:10:27,982 - father_agent.py:447 - Training agent with replay buffer option: 1
2025-03-24 14:10:27,982 - father_agent.py:449 - Before training evaluation.
2025-03-24 14:10:27,982 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:10:35,927 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:10:35,927 - father_agent.py:564 - Average Virtual Goal Value = 4.488778114318848
2025-03-24 14:10:35,927 - father_agent.py:566 - Goal Reach Probability = 0.08977556109725686
2025-03-24 14:10:35,927 - father_agent.py:568 - Trap Reach Probability = 0.6134663341645885
2025-03-24 14:10:35,927 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:10:35,927 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:10:35,927 - father_agent.py:574 - Current Best Reach Probability = 0.5894498656484232
2025-03-24 14:10:35,963 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:10:36,053 - father_agent.py:359 - Training agent on-policy
2025-03-24 14:10:39,382 - father_agent.py:306 - Step: 0, Training loss: 27.580228805541992
2025-03-24 14:10:39,417 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:10:48,035 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:10:48,035 - father_agent.py:564 - Average Virtual Goal Value = 8.22510814666748
2025-03-24 14:10:48,035 - father_agent.py:566 - Goal Reach Probability = 0.1645021645021645
2025-03-24 14:10:48,035 - father_agent.py:568 - Trap Reach Probability = 0.5844155844155844
2025-03-24 14:10:48,035 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:10:48,035 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:10:48,035 - father_agent.py:574 - Current Best Reach Probability = 0.5894498656484232
2025-03-24 14:10:48,066 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:10:50,054 - father_agent.py:306 - Step: 5, Training loss: 0.8249697685241699
2025-03-24 14:10:51,182 - father_agent.py:306 - Step: 10, Training loss: 2.145606517791748
2025-03-24 14:10:52,229 - father_agent.py:306 - Step: 15, Training loss: 1.6887292861938477
2025-03-24 14:10:53,242 - father_agent.py:306 - Step: 20, Training loss: 1.3294984102249146
2025-03-24 14:10:54,272 - father_agent.py:306 - Step: 25, Training loss: 1.001147985458374
2025-03-24 14:10:55,371 - father_agent.py:306 - Step: 30, Training loss: 0.7639284133911133
2025-03-24 14:10:56,467 - father_agent.py:306 - Step: 35, Training loss: 0.8269145488739014
2025-03-24 14:10:57,551 - father_agent.py:306 - Step: 40, Training loss: 0.7431432008743286
2025-03-24 14:10:58,675 - father_agent.py:306 - Step: 45, Training loss: 0.7951563000679016
2025-03-24 14:10:59,757 - father_agent.py:306 - Step: 50, Training loss: 0.9927581548690796
2025-03-24 14:11:00,830 - father_agent.py:306 - Step: 55, Training loss: 0.8549548387527466
2025-03-24 14:11:01,951 - father_agent.py:306 - Step: 60, Training loss: 0.611968457698822
2025-03-24 14:11:03,085 - father_agent.py:306 - Step: 65, Training loss: 0.536967396736145
2025-03-24 14:11:04,230 - father_agent.py:306 - Step: 70, Training loss: 0.5476164221763611
2025-03-24 14:11:05,324 - father_agent.py:306 - Step: 75, Training loss: 0.674092710018158
2025-03-24 14:11:06,453 - father_agent.py:306 - Step: 80, Training loss: 0.4585052728652954
2025-03-24 14:11:07,661 - father_agent.py:306 - Step: 85, Training loss: 0.11850651353597641
2025-03-24 14:11:11,702 - father_agent.py:306 - Step: 90, Training loss: 0.125497505068779
2025-03-24 14:11:12,923 - father_agent.py:306 - Step: 95, Training loss: 0.10992337018251419
2025-03-24 14:11:14,155 - father_agent.py:306 - Step: 100, Training loss: 0.09129014611244202
2025-03-24 14:11:14,190 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:11:23,428 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:11:23,428 - father_agent.py:564 - Average Virtual Goal Value = 0.0
2025-03-24 14:11:23,428 - father_agent.py:566 - Goal Reach Probability = 0.0
2025-03-24 14:11:23,428 - father_agent.py:568 - Trap Reach Probability = 0.5929203539823009
2025-03-24 14:11:23,428 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:11:23,428 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:11:23,428 - father_agent.py:574 - Current Best Reach Probability = 0.5894498656484232
2025-03-24 14:11:23,465 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:11:25,618 - father_agent.py:306 - Step: 105, Training loss: 0.07291291654109955
2025-03-24 14:11:26,836 - father_agent.py:306 - Step: 110, Training loss: 0.0610261932015419
2025-03-24 14:11:28,069 - father_agent.py:306 - Step: 115, Training loss: 0.06260091811418533
2025-03-24 14:11:29,300 - father_agent.py:306 - Step: 120, Training loss: 0.06457427144050598
2025-03-24 14:11:30,534 - father_agent.py:306 - Step: 125, Training loss: 0.06057728826999664
2025-03-24 14:11:31,741 - father_agent.py:306 - Step: 130, Training loss: 0.05075180530548096
2025-03-24 14:11:32,980 - father_agent.py:306 - Step: 135, Training loss: 0.09453655779361725
2025-03-24 14:11:34,176 - father_agent.py:306 - Step: 140, Training loss: 0.298930823802948
2025-03-24 14:11:35,375 - father_agent.py:306 - Step: 145, Training loss: 0.6201139092445374
2025-03-24 14:11:36,499 - father_agent.py:306 - Step: 150, Training loss: 0.9203545451164246
2025-03-24 14:11:37,556 - father_agent.py:306 - Step: 155, Training loss: 1.5065943002700806
2025-03-24 14:11:38,579 - father_agent.py:306 - Step: 160, Training loss: 3.01487135887146
2025-03-24 14:11:39,601 - father_agent.py:306 - Step: 165, Training loss: 2.551623582839966
2025-03-24 14:11:40,579 - father_agent.py:306 - Step: 170, Training loss: 1.85212242603302
2025-03-24 14:11:41,571 - father_agent.py:306 - Step: 175, Training loss: 1.832582712173462
2025-03-24 14:11:42,583 - father_agent.py:306 - Step: 180, Training loss: 1.5035176277160645
2025-03-24 14:11:43,529 - father_agent.py:306 - Step: 185, Training loss: 1.3032777309417725
2025-03-24 14:11:44,505 - father_agent.py:306 - Step: 190, Training loss: 1.4025986194610596
2025-03-24 14:11:45,487 - father_agent.py:306 - Step: 195, Training loss: 2.0856494903564453
2025-03-24 14:11:46,459 - father_agent.py:306 - Step: 200, Training loss: 2.058990001678467
2025-03-24 14:11:46,496 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:11:53,026 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:11:53,027 - father_agent.py:564 - Average Virtual Goal Value = 23.939374923706055
2025-03-24 14:11:53,027 - father_agent.py:566 - Goal Reach Probability = 0.4787875061485489
2025-03-24 14:11:53,027 - father_agent.py:568 - Trap Reach Probability = 0.5212124938514511
2025-03-24 14:11:53,027 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:11:53,027 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:11:53,027 - father_agent.py:574 - Current Best Reach Probability = 0.5894498656484232
2025-03-24 14:11:53,063 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:11:54,808 - father_agent.py:306 - Step: 205, Training loss: 1.5713359117507935
2025-03-24 14:11:55,789 - father_agent.py:306 - Step: 210, Training loss: 1.4304434061050415
2025-03-24 14:11:56,748 - father_agent.py:306 - Step: 215, Training loss: 1.5116219520568848
2025-03-24 14:11:57,735 - father_agent.py:306 - Step: 220, Training loss: 2.0226659774780273
2025-03-24 14:11:58,713 - father_agent.py:306 - Step: 225, Training loss: 1.9099757671356201
2025-03-24 14:11:59,674 - father_agent.py:306 - Step: 230, Training loss: 1.8259763717651367
2025-03-24 14:12:00,646 - father_agent.py:306 - Step: 235, Training loss: 1.571944236755371
2025-03-24 14:12:01,630 - father_agent.py:306 - Step: 240, Training loss: 1.7670063972473145
2025-03-24 14:12:02,627 - father_agent.py:306 - Step: 245, Training loss: 1.6331074237823486
2025-03-24 14:12:03,623 - father_agent.py:306 - Step: 250, Training loss: 1.6683282852172852
2025-03-24 14:12:04,612 - father_agent.py:306 - Step: 255, Training loss: 1.62667715549469
2025-03-24 14:12:05,596 - father_agent.py:306 - Step: 260, Training loss: 1.7671968936920166
2025-03-24 14:12:06,571 - father_agent.py:306 - Step: 265, Training loss: 1.582900047302246
2025-03-24 14:12:07,542 - father_agent.py:306 - Step: 270, Training loss: 1.6802693605422974
2025-03-24 14:12:08,516 - father_agent.py:306 - Step: 275, Training loss: 1.5596860647201538
2025-03-24 14:12:09,484 - father_agent.py:306 - Step: 280, Training loss: 1.4743525981903076
2025-03-24 14:12:10,458 - father_agent.py:306 - Step: 285, Training loss: 1.4690922498703003
2025-03-24 14:12:11,422 - father_agent.py:306 - Step: 290, Training loss: 1.4294371604919434
2025-03-24 14:12:12,395 - father_agent.py:306 - Step: 295, Training loss: 1.4954655170440674
2025-03-24 14:12:13,359 - father_agent.py:306 - Step: 300, Training loss: 1.5461583137512207
2025-03-24 14:12:13,396 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:12:19,902 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:12:19,902 - father_agent.py:564 - Average Virtual Goal Value = 29.406240463256836
2025-03-24 14:12:19,902 - father_agent.py:566 - Goal Reach Probability = 0.5881247903388125
2025-03-24 14:12:19,902 - father_agent.py:568 - Trap Reach Probability = 0.4118752096611875
2025-03-24 14:12:19,902 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:12:19,902 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:12:19,902 - father_agent.py:574 - Current Best Reach Probability = 0.5894498656484232
2025-03-24 14:12:19,942 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:12:21,650 - father_agent.py:306 - Step: 305, Training loss: 1.5106121301651
2025-03-24 14:12:22,629 - father_agent.py:306 - Step: 310, Training loss: 1.4499461650848389
2025-03-24 14:12:23,623 - father_agent.py:306 - Step: 315, Training loss: 1.551344394683838
2025-03-24 14:12:24,620 - father_agent.py:306 - Step: 320, Training loss: 1.2257320880889893
2025-03-24 14:12:25,597 - father_agent.py:306 - Step: 325, Training loss: 1.6870579719543457
2025-03-24 14:12:26,557 - father_agent.py:306 - Step: 330, Training loss: 1.3043928146362305
2025-03-24 14:12:27,553 - father_agent.py:306 - Step: 335, Training loss: 1.4232077598571777
2025-03-24 14:12:28,579 - father_agent.py:306 - Step: 340, Training loss: 1.4448251724243164
2025-03-24 14:12:29,566 - father_agent.py:306 - Step: 345, Training loss: 1.3090814352035522
2025-03-24 14:12:30,553 - father_agent.py:306 - Step: 350, Training loss: 1.4425361156463623
2025-03-24 14:12:31,541 - father_agent.py:306 - Step: 355, Training loss: 1.2768170833587646
2025-03-24 14:12:32,522 - father_agent.py:306 - Step: 360, Training loss: 1.440037488937378
2025-03-24 14:12:33,542 - father_agent.py:306 - Step: 365, Training loss: 1.310987114906311
2025-03-24 14:12:34,501 - father_agent.py:306 - Step: 370, Training loss: 1.4385051727294922
2025-03-24 14:12:35,484 - father_agent.py:306 - Step: 375, Training loss: 1.2867963314056396
2025-03-24 14:12:36,563 - father_agent.py:306 - Step: 380, Training loss: 1.343157410621643
2025-03-24 14:12:37,712 - father_agent.py:306 - Step: 385, Training loss: 1.2607085704803467
2025-03-24 14:12:38,777 - father_agent.py:306 - Step: 390, Training loss: 1.2113862037658691
2025-03-24 14:12:39,823 - father_agent.py:306 - Step: 395, Training loss: 1.1405338048934937
2025-03-24 14:12:40,802 - father_agent.py:306 - Step: 400, Training loss: 1.1222625970840454
2025-03-24 14:12:40,838 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:12:47,655 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:12:47,655 - father_agent.py:564 - Average Virtual Goal Value = 29.701305389404297
2025-03-24 14:12:47,655 - father_agent.py:566 - Goal Reach Probability = 0.5940261161936039
2025-03-24 14:12:47,655 - father_agent.py:568 - Trap Reach Probability = 0.4059738838063961
2025-03-24 14:12:47,655 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:12:47,655 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:12:47,655 - father_agent.py:574 - Current Best Reach Probability = 0.5940261161936039
2025-03-24 14:12:47,694 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:12:49,529 - father_agent.py:306 - Step: 405, Training loss: 1.2330046892166138
2025-03-24 14:12:50,595 - father_agent.py:306 - Step: 410, Training loss: 1.2630281448364258
2025-03-24 14:12:51,599 - father_agent.py:306 - Step: 415, Training loss: 1.1106396913528442
2025-03-24 14:12:52,588 - father_agent.py:306 - Step: 420, Training loss: 1.224289059638977
2025-03-24 14:12:53,616 - father_agent.py:306 - Step: 425, Training loss: 1.2220182418823242
2025-03-24 14:12:54,613 - father_agent.py:306 - Step: 430, Training loss: 1.2690304517745972
2025-03-24 14:12:55,607 - father_agent.py:306 - Step: 435, Training loss: 1.2352792024612427
2025-03-24 14:12:56,609 - father_agent.py:306 - Step: 440, Training loss: 1.0183134078979492
2025-03-24 14:12:57,603 - father_agent.py:306 - Step: 445, Training loss: 1.2749509811401367
2025-03-24 14:12:58,569 - father_agent.py:306 - Step: 450, Training loss: 1.2137479782104492
2025-03-24 14:12:59,532 - father_agent.py:306 - Step: 455, Training loss: 1.2353770732879639
2025-03-24 14:13:00,508 - father_agent.py:306 - Step: 460, Training loss: 1.1664986610412598
2025-03-24 14:13:01,533 - father_agent.py:306 - Step: 465, Training loss: 1.0902167558670044
2025-03-24 14:13:02,522 - father_agent.py:306 - Step: 470, Training loss: 1.2966986894607544
2025-03-24 14:13:03,505 - father_agent.py:306 - Step: 475, Training loss: 1.1189833879470825
2025-03-24 14:13:04,496 - father_agent.py:306 - Step: 480, Training loss: 1.3744693994522095
2025-03-24 14:13:05,494 - father_agent.py:306 - Step: 485, Training loss: 1.2754651308059692
2025-03-24 14:13:06,484 - father_agent.py:306 - Step: 490, Training loss: 1.0559240579605103
2025-03-24 14:13:07,443 - father_agent.py:306 - Step: 495, Training loss: 1.1391085386276245
2025-03-24 14:13:08,440 - father_agent.py:306 - Step: 500, Training loss: 1.196333646774292
2025-03-24 14:13:08,477 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:13:24,052 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:13:24,052 - father_agent.py:564 - Average Virtual Goal Value = 29.975465774536133
2025-03-24 14:13:24,053 - father_agent.py:566 - Goal Reach Probability = 0.5995093087025545
2025-03-24 14:13:24,053 - father_agent.py:568 - Trap Reach Probability = 0.4004906912974455
2025-03-24 14:13:24,053 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:13:24,053 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:13:24,053 - father_agent.py:574 - Current Best Reach Probability = 0.5995093087025545
2025-03-24 14:13:24,090 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:13:26,074 - father_agent.py:306 - Step: 505, Training loss: 1.2571007013320923
2025-03-24 14:13:27,512 - father_agent.py:306 - Step: 510, Training loss: 1.2557220458984375
2025-03-24 14:13:28,748 - father_agent.py:306 - Step: 515, Training loss: 1.0662251710891724
2025-03-24 14:13:29,730 - father_agent.py:306 - Step: 520, Training loss: 1.1682673692703247
2025-03-24 14:13:30,736 - father_agent.py:306 - Step: 525, Training loss: 1.1699784994125366
2025-03-24 14:13:31,722 - father_agent.py:306 - Step: 530, Training loss: 1.467442274093628
2025-03-24 14:13:32,676 - father_agent.py:306 - Step: 535, Training loss: 1.1852836608886719
2025-03-24 14:13:33,651 - father_agent.py:306 - Step: 540, Training loss: 1.091861605644226
2025-03-24 14:13:34,634 - father_agent.py:306 - Step: 545, Training loss: 1.1387600898742676
2025-03-24 14:13:35,631 - father_agent.py:306 - Step: 550, Training loss: 1.2568010091781616
2025-03-24 14:13:36,600 - father_agent.py:306 - Step: 555, Training loss: 1.1392899751663208
2025-03-24 14:13:37,625 - father_agent.py:306 - Step: 560, Training loss: 1.2540531158447266
2025-03-24 14:13:38,627 - father_agent.py:306 - Step: 565, Training loss: 1.2289634943008423
2025-03-24 14:13:39,661 - father_agent.py:306 - Step: 570, Training loss: 1.1808109283447266
2025-03-24 14:13:40,686 - father_agent.py:306 - Step: 575, Training loss: 1.2742435932159424
2025-03-24 14:13:41,727 - father_agent.py:306 - Step: 580, Training loss: 1.0711885690689087
2025-03-24 14:13:42,702 - father_agent.py:306 - Step: 585, Training loss: 1.2019617557525635
2025-03-24 14:13:43,678 - father_agent.py:306 - Step: 590, Training loss: 1.0802650451660156
2025-03-24 14:13:44,659 - father_agent.py:306 - Step: 595, Training loss: 1.1479414701461792
2025-03-24 14:13:45,643 - father_agent.py:306 - Step: 600, Training loss: 1.15066659450531
2025-03-24 14:13:45,680 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:13:52,224 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:13:52,225 - father_agent.py:564 - Average Virtual Goal Value = 29.884424209594727
2025-03-24 14:13:52,225 - father_agent.py:566 - Goal Reach Probability = 0.5976884975233901
2025-03-24 14:13:52,225 - father_agent.py:568 - Trap Reach Probability = 0.4023115024766098
2025-03-24 14:13:52,225 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:13:52,225 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:13:52,225 - father_agent.py:574 - Current Best Reach Probability = 0.5995093087025545
2025-03-24 14:13:52,259 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:13:53,956 - father_agent.py:306 - Step: 605, Training loss: 1.0401312112808228
2025-03-24 14:13:54,932 - father_agent.py:306 - Step: 610, Training loss: 1.25623619556427
2025-03-24 14:13:55,937 - father_agent.py:306 - Step: 615, Training loss: 1.1558979749679565
2025-03-24 14:13:56,928 - father_agent.py:306 - Step: 620, Training loss: 1.1558008193969727
2025-03-24 14:13:57,907 - father_agent.py:306 - Step: 625, Training loss: 1.1808764934539795
2025-03-24 14:13:58,879 - father_agent.py:306 - Step: 630, Training loss: 1.1598405838012695
2025-03-24 14:13:59,874 - father_agent.py:306 - Step: 635, Training loss: 1.2010551691055298
2025-03-24 14:14:00,834 - father_agent.py:306 - Step: 640, Training loss: 1.1988085508346558
2025-03-24 14:14:01,814 - father_agent.py:306 - Step: 645, Training loss: 0.8633747696876526
2025-03-24 14:14:02,803 - father_agent.py:306 - Step: 650, Training loss: 1.1193805932998657
2025-03-24 14:14:03,790 - father_agent.py:306 - Step: 655, Training loss: 1.2284613847732544
2025-03-24 14:14:04,755 - father_agent.py:306 - Step: 660, Training loss: 1.0471832752227783
2025-03-24 14:14:05,731 - father_agent.py:306 - Step: 665, Training loss: 0.9536858797073364
2025-03-24 14:14:06,696 - father_agent.py:306 - Step: 670, Training loss: 1.18247389793396
2025-03-24 14:14:07,674 - father_agent.py:306 - Step: 675, Training loss: 0.9013226628303528
2025-03-24 14:14:08,688 - father_agent.py:306 - Step: 680, Training loss: 1.0994150638580322
2025-03-24 14:14:09,670 - father_agent.py:306 - Step: 685, Training loss: 1.2120561599731445
2025-03-24 14:14:10,644 - father_agent.py:306 - Step: 690, Training loss: 0.9895404577255249
2025-03-24 14:14:11,638 - father_agent.py:306 - Step: 695, Training loss: 1.1196402311325073
2025-03-24 14:14:12,626 - father_agent.py:306 - Step: 700, Training loss: 1.005324125289917
2025-03-24 14:14:12,663 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:14:19,192 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:14:19,192 - father_agent.py:564 - Average Virtual Goal Value = 30.225988388061523
2025-03-24 14:14:19,193 - father_agent.py:566 - Goal Reach Probability = 0.6045197740112994
2025-03-24 14:14:19,193 - father_agent.py:568 - Trap Reach Probability = 0.3954802259887006
2025-03-24 14:14:19,193 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:14:19,193 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:14:19,193 - father_agent.py:574 - Current Best Reach Probability = 0.6045197740112994
2025-03-24 14:14:19,232 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:14:19,350 - father_agent.py:455 - Training finished.
2025-03-24 14:14:19,356 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:14:19,448 - father_agent.py:545 - Evaluating agent with greedy policy.
2025-03-24 14:14:25,960 - father_agent.py:562 - Average Return = 0.0
2025-03-24 14:14:25,960 - father_agent.py:564 - Average Virtual Goal Value = 30.058469772338867
2025-03-24 14:14:25,961 - father_agent.py:566 - Goal Reach Probability = 0.6011693980070822
2025-03-24 14:14:25,961 - father_agent.py:568 - Trap Reach Probability = 0.39883060199291775
2025-03-24 14:14:25,961 - father_agent.py:570 - Variance of Return = 0.0
2025-03-24 14:14:25,961 - father_agent.py:572 - Current Best Return = 0.0
2025-03-24 14:14:25,961 - father_agent.py:574 - Current Best Reach Probability = 0.6045197740112994
2025-03-24 14:14:26,006 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:14:38,406 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:14:38,406 - evaluators.py:130 - Average Virtual Goal Value = 32.390865325927734
2025-03-24 14:14:38,406 - evaluators.py:132 - Goal Reach Probability = 0.6478173223967747
2025-03-24 14:14:38,406 - evaluators.py:134 - Trap Reach Probability = 0.35218267760322536
2025-03-24 14:14:38,406 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:14:38,406 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:14:38,406 - evaluators.py:140 - Current Best Reach Probability = 0.6478173223967747
2025-03-24 14:14:38,422 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:15:07,150 - cloned_fsc_actor_policy.py:189 - Epoch 0, Loss: 1.5945
2025-03-24 14:15:07,150 - cloned_fsc_actor_policy.py:190 - Epoch 0, Accuracy: 0.4531
2025-03-24 14:15:07,192 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:15:22,370 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:15:22,370 - evaluators.py:130 - Average Virtual Goal Value = 0.0
2025-03-24 14:15:22,370 - evaluators.py:132 - Goal Reach Probability = 0.0
2025-03-24 14:15:22,370 - evaluators.py:134 - Trap Reach Probability = 0.23297491039426524
2025-03-24 14:15:22,370 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:15:22,370 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:15:22,370 - evaluators.py:140 - Current Best Reach Probability = 0.0
2025-03-24 14:15:25,445 - cloned_fsc_actor_policy.py:189 - Epoch 1000, Loss: 0.7656
2025-03-24 14:15:25,445 - cloned_fsc_actor_policy.py:190 - Epoch 1000, Accuracy: 0.7935
2025-03-24 14:15:28,489 - cloned_fsc_actor_policy.py:189 - Epoch 2000, Loss: 0.3136
2025-03-24 14:15:28,489 - cloned_fsc_actor_policy.py:190 - Epoch 2000, Accuracy: 0.9165
2025-03-24 14:15:32,129 - cloned_fsc_actor_policy.py:189 - Epoch 3000, Loss: 0.2443
2025-03-24 14:15:32,129 - cloned_fsc_actor_policy.py:190 - Epoch 3000, Accuracy: 0.9426
2025-03-24 14:15:35,294 - cloned_fsc_actor_policy.py:189 - Epoch 4000, Loss: 0.2137
2025-03-24 14:15:35,294 - cloned_fsc_actor_policy.py:190 - Epoch 4000, Accuracy: 0.9445
2025-03-24 14:15:38,336 - cloned_fsc_actor_policy.py:189 - Epoch 5000, Loss: 0.1990
2025-03-24 14:15:38,336 - cloned_fsc_actor_policy.py:190 - Epoch 5000, Accuracy: 0.9443
2025-03-24 14:15:38,374 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:15:53,196 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:15:53,196 - evaluators.py:130 - Average Virtual Goal Value = 28.081287384033203
2025-03-24 14:15:53,196 - evaluators.py:132 - Goal Reach Probability = 0.5616257587754024
2025-03-24 14:15:53,197 - evaluators.py:134 - Trap Reach Probability = 0.3647400369490631
2025-03-24 14:15:53,197 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:15:53,197 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:15:53,197 - evaluators.py:140 - Current Best Reach Probability = 0.5616257587754024
2025-03-24 14:15:56,282 - cloned_fsc_actor_policy.py:189 - Epoch 6000, Loss: 0.1898
2025-03-24 14:15:56,282 - cloned_fsc_actor_policy.py:190 - Epoch 6000, Accuracy: 0.9443
2025-03-24 14:15:59,359 - cloned_fsc_actor_policy.py:189 - Epoch 7000, Loss: 0.1821
2025-03-24 14:15:59,359 - cloned_fsc_actor_policy.py:190 - Epoch 7000, Accuracy: 0.9445
2025-03-24 14:16:02,450 - cloned_fsc_actor_policy.py:189 - Epoch 8000, Loss: 0.1762
2025-03-24 14:16:02,451 - cloned_fsc_actor_policy.py:190 - Epoch 8000, Accuracy: 0.9443
2025-03-24 14:16:05,532 - cloned_fsc_actor_policy.py:189 - Epoch 9000, Loss: 0.1706
2025-03-24 14:16:05,532 - cloned_fsc_actor_policy.py:190 - Epoch 9000, Accuracy: 0.9444
2025-03-24 14:16:08,615 - cloned_fsc_actor_policy.py:189 - Epoch 10000, Loss: 0.1663
2025-03-24 14:16:08,615 - cloned_fsc_actor_policy.py:190 - Epoch 10000, Accuracy: 0.9442
2025-03-24 14:16:08,649 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:16:22,984 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:16:22,984 - evaluators.py:130 - Average Virtual Goal Value = 27.974802017211914
2025-03-24 14:16:22,984 - evaluators.py:132 - Goal Reach Probability = 0.5594960335977601
2025-03-24 14:16:22,984 - evaluators.py:134 - Trap Reach Probability = 0.37727484834344377
2025-03-24 14:16:22,984 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:16:22,984 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:16:22,984 - evaluators.py:140 - Current Best Reach Probability = 0.5616257587754024
2025-03-24 14:16:26,044 - cloned_fsc_actor_policy.py:189 - Epoch 11000, Loss: 0.1621
2025-03-24 14:16:26,044 - cloned_fsc_actor_policy.py:190 - Epoch 11000, Accuracy: 0.9442
2025-03-24 14:16:29,130 - cloned_fsc_actor_policy.py:189 - Epoch 12000, Loss: 0.1589
2025-03-24 14:16:29,130 - cloned_fsc_actor_policy.py:190 - Epoch 12000, Accuracy: 0.9442
2025-03-24 14:16:32,373 - cloned_fsc_actor_policy.py:189 - Epoch 13000, Loss: 0.1554
2025-03-24 14:16:32,373 - cloned_fsc_actor_policy.py:190 - Epoch 13000, Accuracy: 0.9446
2025-03-24 14:16:35,412 - cloned_fsc_actor_policy.py:189 - Epoch 14000, Loss: 0.1530
2025-03-24 14:16:35,412 - cloned_fsc_actor_policy.py:190 - Epoch 14000, Accuracy: 0.9447
2025-03-24 14:16:38,459 - cloned_fsc_actor_policy.py:189 - Epoch 15000, Loss: 0.1502
2025-03-24 14:16:38,460 - cloned_fsc_actor_policy.py:190 - Epoch 15000, Accuracy: 0.9478
2025-03-24 14:16:38,494 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:16:52,726 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:16:52,726 - evaluators.py:130 - Average Virtual Goal Value = 28.201908111572266
2025-03-24 14:16:52,726 - evaluators.py:132 - Goal Reach Probability = 0.5640381717729784
2025-03-24 14:16:52,726 - evaluators.py:134 - Trap Reach Probability = 0.3661476644902059
2025-03-24 14:16:52,726 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:16:52,726 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:16:52,726 - evaluators.py:140 - Current Best Reach Probability = 0.5640381717729784
2025-03-24 14:16:55,725 - cloned_fsc_actor_policy.py:189 - Epoch 16000, Loss: 0.1453
2025-03-24 14:16:55,725 - cloned_fsc_actor_policy.py:190 - Epoch 16000, Accuracy: 0.9497
2025-03-24 14:16:58,785 - cloned_fsc_actor_policy.py:189 - Epoch 17000, Loss: 0.1421
2025-03-24 14:16:58,785 - cloned_fsc_actor_policy.py:190 - Epoch 17000, Accuracy: 0.9497
2025-03-24 14:17:01,847 - cloned_fsc_actor_policy.py:189 - Epoch 18000, Loss: 0.1390
2025-03-24 14:17:01,847 - cloned_fsc_actor_policy.py:190 - Epoch 18000, Accuracy: 0.9489
2025-03-24 14:17:04,859 - cloned_fsc_actor_policy.py:189 - Epoch 19000, Loss: 0.1376
2025-03-24 14:17:04,859 - cloned_fsc_actor_policy.py:190 - Epoch 19000, Accuracy: 0.9490
2025-03-24 14:17:08,076 - cloned_fsc_actor_policy.py:189 - Epoch 20000, Loss: 0.1344
2025-03-24 14:17:08,076 - cloned_fsc_actor_policy.py:190 - Epoch 20000, Accuracy: 0.9507
2025-03-24 14:17:08,114 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:17:22,548 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:17:22,548 - evaluators.py:130 - Average Virtual Goal Value = 28.41798973083496
2025-03-24 14:17:22,548 - evaluators.py:132 - Goal Reach Probability = 0.5683598120384874
2025-03-24 14:17:22,548 - evaluators.py:134 - Trap Reach Probability = 0.3710002237637055
2025-03-24 14:17:22,548 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:17:22,548 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:17:22,548 - evaluators.py:140 - Current Best Reach Probability = 0.5683598120384874
2025-03-24 14:17:25,638 - cloned_fsc_actor_policy.py:189 - Epoch 21000, Loss: 0.1324
2025-03-24 14:17:25,638 - cloned_fsc_actor_policy.py:190 - Epoch 21000, Accuracy: 0.9509
2025-03-24 14:17:28,833 - cloned_fsc_actor_policy.py:189 - Epoch 22000, Loss: 0.1307
2025-03-24 14:17:28,833 - cloned_fsc_actor_policy.py:190 - Epoch 22000, Accuracy: 0.9509
2025-03-24 14:17:31,866 - cloned_fsc_actor_policy.py:189 - Epoch 23000, Loss: 0.1301
2025-03-24 14:17:31,866 - cloned_fsc_actor_policy.py:190 - Epoch 23000, Accuracy: 0.9504
2025-03-24 14:17:35,300 - cloned_fsc_actor_policy.py:189 - Epoch 24000, Loss: 0.1274
2025-03-24 14:17:35,300 - cloned_fsc_actor_policy.py:190 - Epoch 24000, Accuracy: 0.9509
2025-03-24 14:17:38,641 - cloned_fsc_actor_policy.py:189 - Epoch 25000, Loss: 0.1270
2025-03-24 14:17:38,641 - cloned_fsc_actor_policy.py:190 - Epoch 25000, Accuracy: 0.9506
2025-03-24 14:17:38,670 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:17:52,816 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:17:52,816 - evaluators.py:130 - Average Virtual Goal Value = 28.73976707458496
2025-03-24 14:17:52,816 - evaluators.py:132 - Goal Reach Probability = 0.5747953361448772
2025-03-24 14:17:52,816 - evaluators.py:134 - Trap Reach Probability = 0.35822376581493426
2025-03-24 14:17:52,816 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:17:52,816 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:17:52,816 - evaluators.py:140 - Current Best Reach Probability = 0.5747953361448772
2025-03-24 14:17:55,832 - cloned_fsc_actor_policy.py:189 - Epoch 26000, Loss: 0.1253
2025-03-24 14:17:55,832 - cloned_fsc_actor_policy.py:190 - Epoch 26000, Accuracy: 0.9508
2025-03-24 14:17:58,895 - cloned_fsc_actor_policy.py:189 - Epoch 27000, Loss: 0.1236
2025-03-24 14:17:58,895 - cloned_fsc_actor_policy.py:190 - Epoch 27000, Accuracy: 0.9512
2025-03-24 14:18:01,969 - cloned_fsc_actor_policy.py:189 - Epoch 28000, Loss: 0.1233
2025-03-24 14:18:01,970 - cloned_fsc_actor_policy.py:190 - Epoch 28000, Accuracy: 0.9509
2025-03-24 14:18:05,076 - cloned_fsc_actor_policy.py:189 - Epoch 29000, Loss: 0.1229
2025-03-24 14:18:05,076 - cloned_fsc_actor_policy.py:190 - Epoch 29000, Accuracy: 0.9511
2025-03-24 14:18:08,209 - cloned_fsc_actor_policy.py:189 - Epoch 30000, Loss: 0.1214
2025-03-24 14:18:08,210 - cloned_fsc_actor_policy.py:190 - Epoch 30000, Accuracy: 0.9516
2025-03-24 14:18:08,239 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:18:22,503 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:18:22,503 - evaluators.py:130 - Average Virtual Goal Value = 28.18453025817871
2025-03-24 14:18:22,503 - evaluators.py:132 - Goal Reach Probability = 0.5636906128069773
2025-03-24 14:18:22,503 - evaluators.py:134 - Trap Reach Probability = 0.37181546935965115
2025-03-24 14:18:22,503 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:18:22,503 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:18:22,503 - evaluators.py:140 - Current Best Reach Probability = 0.5747953361448772
2025-03-24 14:18:25,585 - cloned_fsc_actor_policy.py:189 - Epoch 31000, Loss: 0.1209
2025-03-24 14:18:25,585 - cloned_fsc_actor_policy.py:190 - Epoch 31000, Accuracy: 0.9517
2025-03-24 14:18:29,067 - cloned_fsc_actor_policy.py:189 - Epoch 32000, Loss: 0.1203
2025-03-24 14:18:29,067 - cloned_fsc_actor_policy.py:190 - Epoch 32000, Accuracy: 0.9517
2025-03-24 14:18:32,235 - cloned_fsc_actor_policy.py:189 - Epoch 33000, Loss: 0.1191
2025-03-24 14:18:32,235 - cloned_fsc_actor_policy.py:190 - Epoch 33000, Accuracy: 0.9523
2025-03-24 14:18:35,280 - cloned_fsc_actor_policy.py:189 - Epoch 34000, Loss: 0.1184
2025-03-24 14:18:35,280 - cloned_fsc_actor_policy.py:190 - Epoch 34000, Accuracy: 0.9525
2025-03-24 14:18:38,341 - cloned_fsc_actor_policy.py:189 - Epoch 35000, Loss: 0.1174
2025-03-24 14:18:38,341 - cloned_fsc_actor_policy.py:190 - Epoch 35000, Accuracy: 0.9531
2025-03-24 14:18:38,370 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:18:53,339 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:18:53,340 - evaluators.py:130 - Average Virtual Goal Value = 28.228870391845703
2025-03-24 14:18:53,340 - evaluators.py:132 - Goal Reach Probability = 0.5645773979107312
2025-03-24 14:18:53,340 - evaluators.py:134 - Trap Reach Probability = 0.3694207027540361
2025-03-24 14:18:53,340 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:18:53,340 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:18:53,340 - evaluators.py:140 - Current Best Reach Probability = 0.5747953361448772
2025-03-24 14:18:56,350 - cloned_fsc_actor_policy.py:189 - Epoch 36000, Loss: 0.1164
2025-03-24 14:18:56,350 - cloned_fsc_actor_policy.py:190 - Epoch 36000, Accuracy: 0.9533
2025-03-24 14:18:59,417 - cloned_fsc_actor_policy.py:189 - Epoch 37000, Loss: 0.1163
2025-03-24 14:18:59,417 - cloned_fsc_actor_policy.py:190 - Epoch 37000, Accuracy: 0.9535
2025-03-24 14:19:02,487 - cloned_fsc_actor_policy.py:189 - Epoch 38000, Loss: 0.1153
2025-03-24 14:19:02,487 - cloned_fsc_actor_policy.py:190 - Epoch 38000, Accuracy: 0.9539
2025-03-24 14:19:05,541 - cloned_fsc_actor_policy.py:189 - Epoch 39000, Loss: 0.1148
2025-03-24 14:19:05,541 - cloned_fsc_actor_policy.py:190 - Epoch 39000, Accuracy: 0.9541
2025-03-24 14:19:08,605 - cloned_fsc_actor_policy.py:189 - Epoch 40000, Loss: 0.1137
2025-03-24 14:19:08,605 - cloned_fsc_actor_policy.py:190 - Epoch 40000, Accuracy: 0.9545
2025-03-24 14:19:08,636 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:19:23,440 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:19:23,440 - evaluators.py:130 - Average Virtual Goal Value = 27.925796508789062
2025-03-24 14:19:23,440 - evaluators.py:132 - Goal Reach Probability = 0.5585159362549801
2025-03-24 14:19:23,440 - evaluators.py:134 - Trap Reach Probability = 0.3727589641434263
2025-03-24 14:19:23,440 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:19:23,440 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:19:23,440 - evaluators.py:140 - Current Best Reach Probability = 0.5747953361448772
2025-03-24 14:19:26,549 - cloned_fsc_actor_policy.py:189 - Epoch 41000, Loss: 0.1138
2025-03-24 14:19:26,550 - cloned_fsc_actor_policy.py:190 - Epoch 41000, Accuracy: 0.9540
2025-03-24 14:19:29,655 - cloned_fsc_actor_policy.py:189 - Epoch 42000, Loss: 0.1135
2025-03-24 14:19:29,655 - cloned_fsc_actor_policy.py:190 - Epoch 42000, Accuracy: 0.9544
2025-03-24 14:19:32,743 - cloned_fsc_actor_policy.py:189 - Epoch 43000, Loss: 0.1135
2025-03-24 14:19:32,743 - cloned_fsc_actor_policy.py:190 - Epoch 43000, Accuracy: 0.9546
2025-03-24 14:19:35,852 - cloned_fsc_actor_policy.py:189 - Epoch 44000, Loss: 0.1130
2025-03-24 14:19:35,852 - cloned_fsc_actor_policy.py:190 - Epoch 44000, Accuracy: 0.9546
2025-03-24 14:19:39,420 - cloned_fsc_actor_policy.py:189 - Epoch 45000, Loss: 0.1119
2025-03-24 14:19:39,420 - cloned_fsc_actor_policy.py:190 - Epoch 45000, Accuracy: 0.9551
2025-03-24 14:19:39,455 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:19:54,192 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:19:54,192 - evaluators.py:130 - Average Virtual Goal Value = 28.07396697998047
2025-03-24 14:19:54,192 - evaluators.py:132 - Goal Reach Probability = 0.5614793467819404
2025-03-24 14:19:54,192 - evaluators.py:134 - Trap Reach Probability = 0.37175792507204614
2025-03-24 14:19:54,192 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:19:54,192 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:19:54,192 - evaluators.py:140 - Current Best Reach Probability = 0.5747953361448772
2025-03-24 14:19:57,193 - cloned_fsc_actor_policy.py:189 - Epoch 46000, Loss: 0.1109
2025-03-24 14:19:57,193 - cloned_fsc_actor_policy.py:190 - Epoch 46000, Accuracy: 0.9556
2025-03-24 14:20:00,280 - cloned_fsc_actor_policy.py:189 - Epoch 47000, Loss: 0.1111
2025-03-24 14:20:00,280 - cloned_fsc_actor_policy.py:190 - Epoch 47000, Accuracy: 0.9553
2025-03-24 14:20:03,345 - cloned_fsc_actor_policy.py:189 - Epoch 48000, Loss: 0.1111
2025-03-24 14:20:03,346 - cloned_fsc_actor_policy.py:190 - Epoch 48000, Accuracy: 0.9555
2025-03-24 14:20:06,430 - cloned_fsc_actor_policy.py:189 - Epoch 49000, Loss: 0.1102
2025-03-24 14:20:06,430 - cloned_fsc_actor_policy.py:190 - Epoch 49000, Accuracy: 0.9560
2025-03-24 14:20:09,523 - cloned_fsc_actor_policy.py:189 - Epoch 50000, Loss: 0.1092
2025-03-24 14:20:09,524 - cloned_fsc_actor_policy.py:190 - Epoch 50000, Accuracy: 0.9564
2025-03-24 14:20:09,552 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:20:24,119 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:20:24,119 - evaluators.py:130 - Average Virtual Goal Value = 27.6934757232666
2025-03-24 14:20:24,119 - evaluators.py:132 - Goal Reach Probability = 0.5538694992412747
2025-03-24 14:20:24,119 - evaluators.py:134 - Trap Reach Probability = 0.3735457764289327
2025-03-24 14:20:24,119 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:20:24,119 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:20:24,119 - evaluators.py:140 - Current Best Reach Probability = 0.5747953361448772
2025-03-24 14:20:27,188 - cloned_fsc_actor_policy.py:189 - Epoch 51000, Loss: 0.1090
2025-03-24 14:20:27,188 - cloned_fsc_actor_policy.py:190 - Epoch 51000, Accuracy: 0.9567
2025-03-24 14:20:30,275 - cloned_fsc_actor_policy.py:189 - Epoch 52000, Loss: 0.1084
2025-03-24 14:20:30,275 - cloned_fsc_actor_policy.py:190 - Epoch 52000, Accuracy: 0.9570
2025-03-24 14:20:33,484 - cloned_fsc_actor_policy.py:189 - Epoch 53000, Loss: 0.1077
2025-03-24 14:20:33,484 - cloned_fsc_actor_policy.py:190 - Epoch 53000, Accuracy: 0.9579
2025-03-24 14:20:36,596 - cloned_fsc_actor_policy.py:189 - Epoch 54000, Loss: 0.1081
2025-03-24 14:20:36,596 - cloned_fsc_actor_policy.py:190 - Epoch 54000, Accuracy: 0.9582
2025-03-24 14:20:39,743 - cloned_fsc_actor_policy.py:189 - Epoch 55000, Loss: 0.1073
2025-03-24 14:20:39,743 - cloned_fsc_actor_policy.py:190 - Epoch 55000, Accuracy: 0.9588
2025-03-24 14:20:39,771 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:20:54,142 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:20:54,142 - evaluators.py:130 - Average Virtual Goal Value = 28.53253746032715
2025-03-24 14:20:54,142 - evaluators.py:132 - Goal Reach Probability = 0.5706507648431424
2025-03-24 14:20:54,142 - evaluators.py:134 - Trap Reach Probability = 0.35856883588281047
2025-03-24 14:20:54,142 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:20:54,142 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:20:54,142 - evaluators.py:140 - Current Best Reach Probability = 0.5747953361448772
2025-03-24 14:20:57,216 - cloned_fsc_actor_policy.py:189 - Epoch 56000, Loss: 0.1055
2025-03-24 14:20:57,217 - cloned_fsc_actor_policy.py:190 - Epoch 56000, Accuracy: 0.9598
2025-03-24 14:21:00,348 - cloned_fsc_actor_policy.py:189 - Epoch 57000, Loss: 0.1062
2025-03-24 14:21:00,348 - cloned_fsc_actor_policy.py:190 - Epoch 57000, Accuracy: 0.9594
2025-03-24 14:21:03,654 - cloned_fsc_actor_policy.py:189 - Epoch 58000, Loss: 0.1052
2025-03-24 14:21:03,654 - cloned_fsc_actor_policy.py:190 - Epoch 58000, Accuracy: 0.9600
2025-03-24 14:21:06,871 - cloned_fsc_actor_policy.py:189 - Epoch 59000, Loss: 0.1046
2025-03-24 14:21:06,871 - cloned_fsc_actor_policy.py:190 - Epoch 59000, Accuracy: 0.9602
2025-03-24 14:21:10,249 - environment_wrapper_vec.py:365 - Resetting the environment.
2025-03-24 14:21:24,031 - evaluators.py:128 - Average Return = 0.0
2025-03-24 14:21:24,031 - evaluators.py:130 - Average Virtual Goal Value = 27.897933959960938
2025-03-24 14:21:24,031 - evaluators.py:132 - Goal Reach Probability = 0.557958687727825
2025-03-24 14:21:24,031 - evaluators.py:134 - Trap Reach Probability = 0.3732685297691373
2025-03-24 14:21:24,031 - evaluators.py:136 - Variance of Return = 0.0
2025-03-24 14:21:24,031 - evaluators.py:138 - Current Best Return = 0.0
2025-03-24 14:21:24,031 - evaluators.py:140 - Current Best Reach Probability = 0.557958687727825
2025-03-24 14:21:25,121 - pomdp.py:349 - unfolding 9-FSC template into POMDP...
2025-03-24 14:21:36,909 - pomdp.py:355 - constructed quotient MDP having 408519 states and 14391756 actions.
2025-03-24 14:21:58,799 - rl_family_extractor.py:329 - Building family from FFNN...
2025-03-24 14:21:58,813 - rl_family_extractor.py:357 - Number of misses: 0 out of 99
2025-03-24 14:21:58,813 - rl_family_extractor.py:359 - Number of complete misses: 0 out of 99
2025-03-24 14:21:58,813 - statistic.py:67 - synthesis initiated, design space: 1
2025-03-24 14:21:59,069 - synthesizer.py:192 - printing synthesized assignment below:
2025-03-24 14:21:59,069 - synthesizer.py:193 - A([done=0	& obsReady=1	& see=1],0)=r, A([done=0	& obsReady=1	& see=1],1)=r, A([done=0	& obsReady=1	& see=1],2)=r, A([done=0	& obsReady=1	& see=1],3)=d, A([done=0	& obsReady=1	& see=1],4)=r, A([done=0	& obsReady=1	& see=1],5)=d, A([done=0	& obsReady=1	& see=1],6)=r, A([done=0	& obsReady=1	& see=1],7)=r, A([done=0	& obsReady=1	& see=1],8)=r, M([done=0	& obsReady=1	& see=1],0)=2, M([done=0	& obsReady=1	& see=1],1)=2, M([done=0	& obsReady=1	& see=1],2)=2, M([done=0	& obsReady=1	& see=1],3)=2, M([done=0	& obsReady=1	& see=1],4)=2, M([done=0	& obsReady=1	& see=1],5)=2, M([done=0	& obsReady=1	& see=1],6)=2, M([done=0	& obsReady=1	& see=1],7)=2, M([done=0	& obsReady=1	& see=1],8)=2, A([done=0	& obsReady=1	& see=0],0)=d, A([done=0	& obsReady=1	& see=0],1)=d, A([done=0	& obsReady=1	& see=0],2)=d, A([done=0	& obsReady=1	& see=0],3)=d, A([done=0	& obsReady=1	& see=0],4)=d, A([done=0	& obsReady=1	& see=0],5)=d, A([done=0	& obsReady=1	& see=0],6)=r, A([done=0	& obsReady=1	& see=0],7)=r, A([done=0	& obsReady=1	& see=0],8)=r, M([done=0	& obsReady=1	& see=0],0)=0, M([done=0	& obsReady=1	& see=0],1)=0, M([done=0	& obsReady=1	& see=0],2)=0, M([done=0	& obsReady=1	& see=0],3)=0, M([done=0	& obsReady=1	& see=0],4)=0, M([done=0	& obsReady=1	& see=0],5)=0, M([done=0	& obsReady=1	& see=0],6)=0, M([done=0	& obsReady=1	& see=0],7)=0, M([done=0	& obsReady=1	& see=0],8)=0, A([done=1	& obsReady=1	& see=0],0)=d, A([done=1	& obsReady=1	& see=0],1)=d, A([done=1	& obsReady=1	& see=0],2)=d, A([done=1	& obsReady=1	& see=0],3)=d, A([done=1	& obsReady=1	& see=0],4)=d, A([done=1	& obsReady=1	& see=0],5)=d, A([done=1	& obsReady=1	& see=0],6)=d, A([done=1	& obsReady=1	& see=0],7)=d, A([done=1	& obsReady=1	& see=0],8)=d, M([done=1	& obsReady=1	& see=0],0)=7, M([done=1	& obsReady=1	& see=0],1)=7, M([done=1	& obsReady=1	& see=0],2)=7, M([done=1	& obsReady=1	& see=0],3)=7, M([done=1	& obsReady=1	& see=0],4)=7, M([done=1	& obsReady=1	& see=0],5)=7, M([done=1	& obsReady=1	& see=0],6)=7, M([done=1	& obsReady=1	& see=0],7)=7, M([done=1	& obsReady=1	& see=0],8)=7, A([done=1	& obsReady=1	& see=1],0)=l, A([done=1	& obsReady=1	& see=1],1)=l, A([done=1	& obsReady=1	& see=1],2)=l, A([done=1	& obsReady=1	& see=1],3)=d, A([done=1	& obsReady=1	& see=1],4)=d, A([done=1	& obsReady=1	& see=1],5)=d, A([done=1	& obsReady=1	& see=1],6)=u, A([done=1	& obsReady=1	& see=1],7)=r, A([done=1	& obsReady=1	& see=1],8)=u, M([done=1	& obsReady=1	& see=1],0)=7, M([done=1	& obsReady=1	& see=1],1)=7, M([done=1	& obsReady=1	& see=1],2)=6, M([done=1	& obsReady=1	& see=1],3)=7, M([done=1	& obsReady=1	& see=1],4)=7, M([done=1	& obsReady=1	& see=1],5)=7, M([done=1	& obsReady=1	& see=1],6)=6, M([done=1	& obsReady=1	& see=1],7)=6, M([done=1	& obsReady=1	& see=1],8)=6, M([done=0	& obsReady=0	& see=0],0)=0, M([done=0	& obsReady=0	& see=0],1)=0, M([done=0	& obsReady=0	& see=0],2)=0, M([done=0	& obsReady=0	& see=0],3)=1, M([done=0	& obsReady=0	& see=0],4)=1, M([done=0	& obsReady=0	& see=0],5)=1, M([done=0	& obsReady=0	& see=0],6)=0, M([done=0	& obsReady=0	& see=0],7)=0, M([done=0	& obsReady=0	& see=0],8)=0, M([done=0	& obsReady=0	& see=1],0)=2, M([done=0	& obsReady=0	& see=1],1)=2, M([done=0	& obsReady=0	& see=1],2)=2, M([done=0	& obsReady=0	& see=1],3)=2, M([done=0	& obsReady=0	& see=1],4)=2, M([done=0	& obsReady=0	& see=1],5)=2, M([done=0	& obsReady=0	& see=1],6)=8, M([done=0	& obsReady=0	& see=1],7)=8, M([done=0	& obsReady=0	& see=1],8)=2, M([done=1	& obsReady=0	& see=1],0)=7, M([done=1	& obsReady=0	& see=1],1)=7, M([done=1	& obsReady=0	& see=1],2)=7, M([done=1	& obsReady=0	& see=1],3)=7, M([done=1	& obsReady=0	& see=1],4)=7, M([done=1	& obsReady=0	& see=1],5)=7, M([done=1	& obsReady=0	& see=1],6)=6, M([done=1	& obsReady=0	& see=1],7)=7, M([done=1	& obsReady=0	& see=1],8)=6
2025-03-24 14:21:59,324 - synthesizer.py:198 - double-checking specification satisfiability:  : 0.5696731573891967
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 0.26 s
number of holes: 99, family size: 1e81, quotient: 408519 states / 14391756 actions
explored: 100 %
DTMC stats: avg DTMC size: 48757, iterations: 1

optimum: 0.569673
--------------------
2025-03-24 14:21:59,326 - statistic.py:67 - synthesis initiated, design space: 1e81
> progress 0.0%, elapsed 3 s, estimated 904859287347754375568792870467841345686570307778694475113169382480666661669044224 s (28692899776374760646222193885979179244391027614223205139146165993747775488 years), iters = {DTMC: 22}, opt = 0.5697
> progress 0.0%, elapsed 6 s, estimated 886138584811243744454240164702188250284417558450240618614748086939262167736647680 s (28099270193152074626325348160222152707297142181280613907586263990386294784 years), iters = {DTMC: 44}, opt = 0.5697
> progress 0.0%, elapsed 9 s, estimated 880608702046743979200671410556181059159751153566359975784019180937710375868039168 s (27923918760995183018374333671532762917284359552978611483973644741129535488 years), iters = {DTMC: 66}, opt = 0.5697
> progress 0.0%, elapsed 12 s, estimated 873590556583938624651776506808440492252392868247772171149694247112501975190601728 s (27701374828257818470064870774950966671263074505817780735024639914371186688 years), iters = {DTMC: 88}, opt = 0.5697
> progress 0.0%, elapsed 15 s, estimated 877788556962007262670940178345860893641793070301764775589393842902147649517912064 s (27834492546994140805857889594284624492522644554635333918520756992466747392 years), iters = {DTMC: 109}, opt = 0.5697
> progress 0.0%, elapsed 18 s, estimated 882058331601964805365146636780917989087498830023736563361358635002258818699624448 s (27969886212644751720728883296144595521463329059090107593465878877378183168 years), iters = {DTMC: 130}, opt = 0.5697
> progress 0.0%, elapsed 21 s, estimated 886905639001478179266017264432405068996562066472748307604748052269478920186757120 s (28123593321964683572027479201421143480243720052080929305938421583250980864 years), iters = {DTMC: 151}, opt = 0.5697
> progress 0.0%, elapsed 24 s, estimated 883066858856602039513501817814659801096800933586741402565605617814345635810246656 s (28001866402099256553179153547025509908339916394953685635395879052861505536 years), iters = {DTMC: 173}, opt = 0.5697
> progress 0.0%, elapsed 27 s, estimated 887540346605352238084347010053851653429279927836364215370695849015029490442567680 s (28143719768054041818971064049586807678637487737008433176955967810719186944 years), iters = {DTMC: 194}, opt = 0.5697
> progress 0.0%, elapsed 30 s, estimated 891626321792711557078962601943147158245673327244235645019074274923578687418793984 s (28273285191296027487849015169379780414963141416180437607971150996585840640 years), iters = {DTMC: 214}, opt = 0.5697
> progress 0.0%, elapsed 33 s, estimated 887740367183848466233261100341951882677472757316159781496495515680603879434616832 s (28150062378990630716596281180433118009263791496047878181131308168147632128 years), iters = {DTMC: 236}, opt = 0.5697
> progress 0.0%, elapsed 36 s, estimated 884694214398681375950002011618588883953337509421998890900195134836188616199241728 s (28053469507822216344885252998929482979570126915675088513007139098105741312 years), iters = {DTMC: 258}, opt = 0.5697
> progress 0.0%, elapsed 39 s, estimated 882204811800073560422946220255529394911476880871543441247597123048056529346363392 s (27974531069256521170712173360093802110105865710565154156156722884416372736 years), iters = {DTMC: 280}, opt = 0.5697
> progress 0.0%, elapsed 42 s, estimated 880059732337776156602853116748003223242338871580553681655590469304792005899976704 s (27906511045718421872059202730387703765712225492095615860537667649819115520 years), iters = {DTMC: 302}, opt = 0.5697
> progress 0.0%, elapsed 45 s, estimated 879504861195684967235931269055927662445236004259586095361254895910925937168875520 s (27888916197224916137189485599873196622962329274829633383458425714467930112 years), iters = {DTMC: 324}, opt = 0.5697
> progress 0.0%, elapsed 48 s, estimated 877577499462603614450608485740477101908165585124143487321462351461601368548048896 s (27827799957591439010773329157617504561354879788400055728121280108462866432 years), iters = {DTMC: 346}, opt = 0.5697
> progress 0.0%, elapsed 51 s, estimated 876337622282083226001605855434077557705748096873796918323539105519080545596735488 s (27788483710111721267421432617412303779462433531547170518155761829323735040 years), iters = {DTMC: 368}, opt = 0.5697
> progress 0.0%, elapsed 55 s, estimated 874711874484210950502457250999355823102577585376218717352960221973539859004915712 s (27736931585623125578527868713921576300464699114645824365316434649237422080 years), iters = {DTMC: 390}, opt = 0.5697
> progress 0.0%, elapsed 58 s, estimated 875338361787715517909604883402657679373215952071467369024975247699420118419243008 s (27756797367697725373498053420109683213421073609761502771849346990823440384 years), iters = {DTMC: 411}, opt = 0.5697
2025-03-24 14:22:59,374 - synthesizer_onebyone.py:19 - Time limit reached
--------------------
Synthesis summary:
optimality objective: Pmax=? [F "goal"] 

method: 1-by-1, synthesis time: 60.05 s
number of holes: 99, family size: 1e81, quotient: 408519 states / 14391756 actions
explored: 0 %
DTMC stats: avg DTMC size: 8047, iterations: 425

optimum: 0.569673
--------------------
2025-03-24 14:22:59,374 - synthesizer_rl_storm_paynt.py:274 - No improving assignment found.
